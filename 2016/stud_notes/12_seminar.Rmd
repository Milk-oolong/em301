# Стохастические бобры и прочие условности {#12_everything_about_beavers}

Дата: **05.12.2016**

Авторы: **Уманец Екатерина, Купцова Анастасия**

## Упражнение № 0
**Дано**

\begin{tabular}{c|rrrr|}
u/x & 2 & 3  \\
\hline
-1 & 0.1 & 0.2\\
\hline
1 & 0.1 & 0.6\\
\hline
\end{tabular}

$u$, $x$ - скалярные случайные величины

**Хотим найти**

- $\E(u|x)$
- $\Var(u|x)$

**Решение**

\[
E(u|x=2)=0,5*(-1)+0,5*1=0 
\]
\[
E(u|x=3)=-1*0,25 +1*0,75
\]

\[
E(u|x) =
\begin{cases}
0, & \text{если } x=2 \\
0,5, & \text{если } x=3 \\
\end{cases}
 = 0.5(x-2)\]


$Var(u|x)=E(u^2|x)-E^2(u|x)$

$E(u^2|x)=1\Rightarrow$

\[
Var(u|x) =
\begin{cases}
1/3, x=2 \\
3/4, x=3 \\
\end{cases}
=1-0,25(x-2)^2\]

## Условные свойства

1. $E(f(x)|x)=f(x)$ 
2. $Var(f(x)|x)=0$
3. $E(f(x)y|x)=f(x)E(y|x)$
4. $E(E(y|x))=E(y)$
5. $Var(y)=Var(E(y|x))+E(Var(y|x))$

## Упражнение № 1

**Дано:**

$x_1,x_2...$

$x_i\sim N(10,9)$ - независимы


$\underbrace{
\overbrace{(x_1,u_1)}^{can \ be \ dependent}
\rm (x_2,u_2)}_{independent}...$ \ - независимы \ и \ одинаково \ распеделены


\[ 
X=\begin{pmatrix} x_1 \\ : \\ x_n\end{pmatrix} ,u=\begin{pmatrix} u_1 \\ : \\ u_n\end{pmatrix}
\]

**Хотим найти**

a) $plim(\frac {1}{n}X'X)^{-1}$

b) $plim(\frac {1}{n}X'u)$

c) $plim(X'X)^{-1}X'u$

**Решение**

ЗБЧ:

$Y_1,..,Y_n$ - независимы и одинаково распределены

$\bar{Y_n}\longrightarrow E(Y_1)$


a) $plim(\frac{1}{n}\sum_{1}^{n} x_i^2)^{-1}= (plim(\frac{1}{n} \sum_{1}^{n}x_i^2))^{-1} =E(x_1^2)^{-1}=(9+100)^{-1}=\frac{1}{109}$

b) $plim(\frac {1}{n}X'u) = [\frac{X'u}{n}=\frac{\sum(x_iu_i)}{n}]=E(x_i,u_i)=E(E(x_i,u_i|x)) = E(x_i\underbrace{E(u_i|x_i)}_{=0})=0$

c) $plim(X'X)^{-1}X'u=plim(\frac{1}{n}X'X)^{-1}\frac{1}{n}X'u=plim(\frac{1}{n}X'X)^{-1}*plim(\frac{1}{n}X'u)=109^{-1}*0=0$

## Случайность? Не думаю! Или история о том, как отличить стохастические \'{и}ксы

Будем исследовать такой воспрос: как количество выпитого кофе влияет на производительность Бориса.

* **Эксперимент №1** (*неслучайные иксы*): пригласим 100 рандомных Борисов, попросим Бориса номер один в первый день выпить одну кружку кофе, Бориса номер два во второй - две, и так далее, скажем, сто дней; соответственно, на сотый день сотый Борис будет пить сто кружек кофе; и посмотрим, сколько брутальных задачек по эконометрике каждый Борис сможет решить в каждый из этих дней.  **Внимание:** В данном экмперименте ни один Борис не пострадал!!!



* **Эксперимент №2** (*стохастические иксы*): поймаем 100 рандомных Борисов на улице и спросим, сколько кружек кофе каждый из них пьет и сколько брутальных задач решает за день.

Главное отличие между этими двумя экспериментами заключается в том, что в первом случае мы сами выбирали количество кружек кофе, а во втором эта величина получалась случайно.

## Теорема (как на все это смотрели Гаусс и Марков)

**Если:**

* $y = X\beta + u$ - наша регрессионная модель со стохастическими иксами

* $(\chi_{i|\dots}, {y}_{i})$ - независимы и одинаково распределены (то есть между парами зависимостей нет, а вот внутри пары - угадайте что:) ), где $\chi_{i|\dots}$ - $i$-ая строка матрицы $X$

* $E({u}_{i}|\chi_{i|\dots}) = 0$

* $Var({u}_{i}|\chi_{i|\dots}) = \sigma^2$ - условие гомоскедастичности

* $P(\text{столбцы матрицы } X \text{ линейно независимы}) = 1$

**Тогда:**

* $E(\hat{\beta}|X) = \beta$ и $E(\hat{\beta}) = \beta$

* $Var(\hat{\beta}|X) = {\sigma^2}(X^{T}X)^{-1}$

* $\hat{\beta}$ линейна по $y$

* $\hat{\beta}$ эффективна среди линейных по $y$ и несмещенных оценок

* $plim(\hat{\beta}) = \beta$

## А что если гетероскедастичность?

Пусть выполняются все предположения предыдущей теоремы, кроме одного - гомоскедастичности. То есть теперь $Var({u}_{i}|\chi_{i|\dots}) = f(\chi_{i|\dots})$ - условие гетероскедастичности.

**Тогда хотим найти:**

1. $\Omega = Var(u|X)$

2. $E(\hat{\beta}|X)$ 

3. $Var(\hat{\beta}|X)$

4. $plim(\hat{\beta})$ 

**Решение:**

1. $\Omega = Var(u|X) = \begin{pmatrix}
f(\chi_{1|\dots}) & \cdots & 0\\
\vdots & 	\ldots & 	\vdots\\
0 & \cdots & f(\chi_{n|\dots})
\end{pmatrix}$

Теперь поймем, откуда это взялось:

* во-первых, $Var({u}_{i}|X) = Var({u}_{i}|\chi_{i|\dots}) =  f(\chi_{i|\dots})$ - элементы на диагонали матрицы $\Omega$

* во-вторых, $Cov({u}_{i}, {u}_{j}|X) = 0$ - элементы вне диагонали матрицы $\Omega$

2. $E(\hat{\beta}|X) = \beta$

Действительно:

$E(\hat{\beta}|X)  = E({(X^{T}X)^{-1}}X^{T}y|X) = {(X^{T}X)^{-1}}X^{T}E(y|X)$

Так как $E(y|X) = E(X\beta + u|X) = X\beta + E(u|X) = X\beta + 0$ (**Note**: $E(u|X) = 0$ следует из предпосылок теоремы, а именно $E({u}_{i}|\chi_{i|\dots}) = 0$),

то $E(\hat{\beta}|X)  = {(X^{T}X)^{-1}}X^{T}E(y|X) = {(X^{T}X)^{-1}}X^{T}X\beta = 1*\beta = \beta$

3. $Var(\hat{\beta}|X) = {(X^{T}X)^{-1}}X^{T}{\Omega}{X}{(X^{T}X)^{-1}}$

Докажем, что на самом деле получается именно такая сэндвич-формула:

$Var(\hat{\beta}|X) = Var({(X^{T}X)^{-1}}X^{T}y|X) = {(X^{T}X)^{-1}}X^{T}Var(y|X)({(X^{T}X)^{-1}}X^{T})^{T} = {(X^{T}X)^{-1}}X^{T}Var(y|X){X}{(X^{T}X)^{-1}}$

Так как $Var(y|X) = Var(X\beta + u|X) = Var(u|X) = \Omega$,

то $Var(\hat{\beta}|X)  = {(X^{T}X)^{-1}}X^{T}Var(y|X){X}{(X^{T}X)^{-1}} = {(X^{T}X)^{-1}}X^{T}{\Omega}{X}{(X^{T}X)^{-1}}$

4. $plim(\hat{\beta}) = \beta$

Докажем это:

\begin{multline}
plim(\hat{\beta}) = plim({(X^{T}X)^{-1}}X^{T}y)  = plim({(X^{T}X)^{-1}}X^{T}(X\beta + u)) = plim({(X^{T}X)^{-1}}X^{T}X\beta)  + plim({(X^{T}X)^{-1}}X^{T}u) = \\
=\beta + plim({(X^{T}X/n)^{-1}}X^{T}u/n) = \beta + plim{(X^{T}X/n)^{-1}} \cdot plim(X^{T}u/n)  = \beta + const \cdot 0 = \beta
\end{multline}

Эти пределы мы уже искали в упражнении 1 :)

**Как Уайт предлагает оценивать дисперсию оценки** $\beta$ :

$\hat{Var}(\hat{\beta}|X) = {(X^{T}X)^{-1}}X^{T}{\hat{\Omega}}{X}{(X^{T}X)^{-1}}$

где ${\hat{\Omega}} = \begin{pmatrix}
{\hat{u}_{1}}^{2} & \cdots & 0\\
\vdots & 	\ldots & 	\vdots\\
0 & \cdots & {\hat{u}_{n}}^{2} \\
\end{pmatrix}$

## В чем прелесть гетероскедастичности?

А вот в чем - зная о гетероскедастичности, мы знаем распределение:

$\frac{\hat{{\beta}_{j}} - {\beta}_{j}}{{SE}_{HC}(\hat{{\beta}_{j}})} \sim N(0,1)$ - асимптотически, конечно!
