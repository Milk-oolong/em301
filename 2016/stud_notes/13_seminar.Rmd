# Упражнения на гетероскедастичость {#hetero_exercises}

## Упражнение 0 {#hetero_ex0}

Даны две случайные величины:
$u, x$ - случайные величины

(матрица)

$E(u|x)=?$

Посчитаем условные математические ожидания для каждой случайной величины:

$E(u|x=2)=-1*0.5+1*0.5=0$

$E(u|x=3)=-1*\frac{0.2}{0.8}+1*\frac{0.6}{0.8}=0.5$

Совместим их:

\[
E(u|x) = \begin{cases} 
0 &\text{если x=2}\\ 
0.5 &\text{если x=3}
\end{cases}
\]

Также можем представить это условное математическое ожидание в виде функции

$E(u|x)=0.5(x-2)$

Значение условной дисперсии $Var(u|x)$:

Способ №1

Считаем отдельные дисперсии: $Var(u|x=2)$ и $Var(u|x=3)$.
Совмещаем их, по аналогии с математическим ожиданием.

Способ №2

$Var(u|x)=E(u^2|x)-[E(u|x)]^2$

$E(u^2|x)=1$

$Var(u|x)=1-0.25(x-2)^2$

Так как x принимает всего два значения ($x=2, x=3$) условную дисперсию можно представить в виде системы:

\[
Var(u|x) = \begin{cases} 
1 &\text{если x=2}\\ 
0.75 &\text{если x=3}\end{cases}
\]

или в качестве функции:

$Var(u|x)=0.25x+1.5$

Свойства условного математического ожидания:

1. $E(f(x)|x)=f(x)$

2. $Var(f(x)|x)=0$

3. $E(f(x)y|x)=f(x)E(y|x)$

4. $E[E(y|x)]=E(y)$

5. $Var(y)=Var(E(y|x))+E(Var(y|x))$ по теореме Пифагора

## Упражнение 1 {#hetero_ex1}

* Даны случайные величины $X=(x_1, x_2, ..., x_n)$ и $u=(u_1, u_2, ..., u_n)$, 
где $x_i$~$N(10;9)$, при этом векторы $(x_1, u_1); (x_2, u_2) ..., (x_n, u_n)$ независимы, то есть может существовать зависимость между $x_i, u_i$, однако нет зависимости между парами.
*$E(u_i|x_i)=0$

a) $plim(\frac{1}{n}X'X)^{-1}= ?$

b) $plim(\frac{1}{n}X'u)= ?$

c) $plim(X'X)^{-1}X'u= ?$

Решение:

a) $X'X = x_1^2 + x_2^2 + ... + x_n^2 = \sum_{i=1}^n x_i^2$

$plim(\frac{1}{n}\sum_{i=1}^n x_i^2)^{-1}=[E(x_i^2)]^{-1}=(109)^{-1}$ (по закону больших чисел)

Напоминание:

* $lim(f(a_n))=f(lim(a_n))$

* ЗБЧ: $Y_1, Y_2, ..., Y_n ~ i.i.d.$, следовательно, $\bar(Y_n) -> E(Y_1)$

* $E(x_i^2)=Var(x_i)+(E(x_i))^2$


b) $\frac{1}{n}X'u = \frac{1}{n}\sum_{i=1}^n x_iu_i^2$

По ЗБЧ $plim(frac{1}{n}\sum_{i=1}^n x_iu_i^2) = E(x_iu_i)$.

Исходя из свойств условного математического ожидания $E[E(y|x)]=E(y)$ представим $E(x_iu_i)$ как $E(E(x_iu_i|x_i))$.

Так как $x_i$ известно, то по свойству $E(f(x)y|x)=f(x)E(y|x)$ можем вынести известную случайную величину за знак математического ожидания: $E(x_iE(u_i|x_i)) = 0$

Следовательно, $plim(\frac{1}{n}X'u) = 0$


c) $X'X$ и $X'u$ случайные величины
домножим $X'X$ и $X'u$ на $\frac{1}{n}$

$plim(\frac{1}{n}X'X)^{-1}\frac{1}{n}X'u$ = $plim(\frac{1}{n}X'X)^{-1}* plim(\frac{1}{n}X'u)=(109)^{-1}*0=0$

Следовательно, $plim(X'X)^{-1}X'u=0$

Напоминание: 

* $lim(a_nb_n)=lim(a_n)lim(b_n)$

## Стохастические регрессоры {#stoch_regressors}

Если:

1. $y = X\beta+u$

2. $\beta$ - константы

3. В матрице Х: $x_{i.}$ - i-ая строка; векторы $(x_{i.},y_i)$ независимы и одинаково распределены. Наблюдения - это случайная выборка

4. $E(u_i|x_i)=0$

5. Гомоскедастичность $E(u_i^2|x_i)=Var(u_i|x_i)=\sigma^2$

6. $P(столбцы X линейнонезависимы)=1$

7. $\hat{\beta}=(X'X)^{-1}X'y$

## Теорема Гаусса Маркова для стохастических регрессоров {#gm_stochastic}

1. Несмещенность: $E(\hat{\beta}|X)=\beta$

2. $Var(\hat{\beta}|X)=\sigma^2(X'X)^{-1}$

3. $\hat{\beta}$ линейна по y

4. $\hat{\beta}$ эффективная и несмещенная оценка среди линейных по y

5. $plim(\hat{\beta})=\beta$

## Упражнение 2 {#hetero_ex2}

Нарушена предпосылка теоремы Гаусса Маркова об отсутствие гетероскедастичности: $Var(u_i|x_i)=f(x_{i.})$

Найти:

1. Какой вид имеет матрица $\Omega=Var(u|X)$

2. $E(\hat{\beta}|X)=?$

3. $Var(\hat{\beta}|X)=?$

4. $plim(\hat{\beta})=?$

Решение:

1. $\Omega=\left( \begin{matrix} f(x_{1.}) & cov(u_1,u_2|X)&\dots & cov(u_1,u_n|X) \ \vdots & \vdots & \vdots & \vdots\ cov(u_1,u_n|X) & \dots &\dots & f(x_{n.}) \end{matrix} \right)$

Так как выборка случайна, то $cov(u_i,u_j|X)=0$, при $i\neq j$  

$\Omega=\left( \begin{matrix} f(x_{1.}) & 0 &\dots & 0 \ \vdots & \vdots & \vdots & \vdots\ 0 & \dots &\dots & f(x_{n.}) \end{matrix} \right)$
 
2. $E(\hat{\beta}|X)=\beta$

3. $Var(\hat{\beta}|X)=Var((X'X)^{-1}X'y|X)=(X'X)^{-1}X'\times Var(y|X)\times ((X'X)^{-1}X')'= (X'X)^{-1}X'\times Var(y|X)\times X(X'X)^{-1}=(X'X)^{-1}X'\times Var(X\beta+u|X)\times X(X'X)^{-1}=(X'X)^{-1}X'\times Var(u|x)\times X(X'X)^{-1}=(X'X)^{-1}X'\times \Omega\times X(X'X)^{-1}$

В действительности возникает проблема при оценке матрицы $\Omega$, так как она имеет размер $n\times n$, то есть нам нужно оценить n чисел на диагонали по n наблюдениям, что является достаточно нетривиальной задачей. Оказывается, что нельзя получить состоятельную оценку матрицы $\Omega$, но можно получить такую оценку этой матрицы, чтобы левая часть, то есть $Var(\hat{\beta}|X)$, была состоятельна. 

Например, состоятельной оценкой бцдет следующая:
$\hat{Var}_{HC}(\hat{\beta}|X)=(X'X)^{-1}X'\times \hat{\Omega}\times X(X'X)^{-1}$
где $\hat{\Omega}$ можно представить в виде:(White)

$\hat{\Omega}_{HC0}=\left( \begin{matrix} \hat{u_1^2} & 0 &\dots & 0 \ \vdots & \vdots & \vdots & \vdots\ 0 & \dots &\dots & \hat{u_n^2} \end{matrix} \right)$

* абревиатура НС - heteroscedasticity-consistent

При гетероскедастичности оценки несмещенные и состоятельные, но проблема будет заключаться в t-статистике, так как мы не знаем ее распределение, следовательно, нам необходимо использовать стандартные отклонения из устойчивой к гетерокедастичности матрицы, чтобы получить надежные оценки:

$t_{HC}=\frac{\hat{\beta_j}-\beta_j}{se_{HC}(\hat{\beta_j})}$ ~ $N(0;1)$
где $se_{HC}(\hat{\beta_j})$ находятся из матрицы $\hat{Var}_{HC}(\hat{\beta}|X)$

и теперь $t_{HC}$~$N(0;1)$ (асимтотически нормальное)

4. $plim(\hat{\beta})=plim(X'X)^{-1}X'y=\beta+plim(X'X)^{-1}X'u=\beta$

