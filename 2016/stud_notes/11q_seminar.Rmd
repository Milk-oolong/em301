---
html_document: default
output:
  html_document: default
  pdf_document: default
  word_document: default
pdf_document: default
---


* Датa __21/11/16__
* Конспект: __Козловский Евгений, Ермакова Мария__

# Фриш и Ко {#frish_and_co}

Вспомним теорему Фриша-Вау-Ловелла. Допустим, имеется следующая модель:
\[ y_{i}=\beta_1{x}_{i}+\beta_2{z}_{i}+u_{i}  \]
Заметим сначала, что вышеупомянутую теорему можно применять вне зависимости от того, есть ли в модели константа или нет. Мы хотим оценить коэффициент $\beta_1$. Стандартный способ — построить регрессию вида:
\[ \hat{y_{i}}=\hat{\beta_1}{x}_{i}+\hat{\beta_2}{z}_{i}+u_{i}  \]
исходя из которой мы легко с помощью МНК можем вычислить $\hat{\beta_1}$.
Теорема имени Фриша-Вау-Ловелла гласит, что есть и другой путь, состоящий из нескольких шагов:

1) строим регрессию $\hat{y_i}=\hat{\lambda}z_i$, вычисляем остатки $\tilde{y_i}=y_i-\hat{y_i}$

2) строим регрессию $\hat{x_i}=\hat{\delta}z_i$, вычисляем остатки $\tilde{x_i}=x_i-\hat{x_i}$

3) теперь нам остается оценить модель $\tilde{y_i}=\alpha\tilde{x_i}+\varepsilon_i$, т.е. нужно построить регрессию остатков $\tilde{y_i}$ на остатки $\tilde{x_i}$: $\hat{\tilde{y_i}}=\hat{\alpha}\tilde{x_i}$. Полученная оценка $\hat{\tilde{\alpha}}$ будет в точности совпадать с $\hat{\beta_1}$ из исходной регрессии!

Проведем аналогию
$$ y_i = \beta x_i + u_i$$
$$ \hat{\beta} = \frac{\sum{x_i y_i}}{\sum x_i^{2}}$$
$$  y_i = \beta_1x_i+\beta_2z_i + u_i $$
$$ \hat{\beta} = \frac{\sum{\tilde{x_i} \tilde{y_i}}}{\sum \tilde{x_i^{2}}}$$
,где  $\tilde{y_i}$ и $\tilde{x_i}$ остатки от регрессий $y_{i}$ на $z_{i}$ и $x_{i}$ на $z_{i}$


## Упражнение 1
В качестве примера рассмотрим весьма популярный массив mtcars, встроенный в R, из которого возьмем следующие характеристики автомобилей:

* $mpg$ — количество миль на галлон бензина

* $wt$ — масса автомобиля

* $hp$ — мощность (в л.с.)
Пусть имеются следующие модели с соответствующими остатками:

\[
mpg_i = \beta_1 + \beta_2 hp_i + u_i; \widetilde{mpg}_i=mpg_i-\widehat{mpg_i}
\]
\[wt_i = \gamma_1 + \gamma_2 hp_i + u_i; \widetilde{wt}_i=wt_i-\widehat{wt_i}\]
Пусть также дана ковариационная матрица остатков:
\[
\begin{aligned}
\widehat{Var}\left( \begin{matrix}
    \widetilde{mpg}_i \\
    \widetilde{wt}_i
  \end{matrix} \right) &= \begin{pmatrix}
14.4 & -2.10\\
-2.10 & 0.542
\end{pmatrix}\\
\end{aligned}
\]
Найдем всевозможные коэффициенты в следующих регрессиях:
$$ mpg_i = \alpha_1+ \alpha_2 wt_i + \alpha_3 hp_i + \upsilon_i$$
$$ wt_i = \delta_1 + \delta_2 mpg_i + \delta_3 hp_i + \varepsilon_i $$
Чтобы отыскать оценку $\alpha_2$, нужно построить регрессию $\widetilde{mpg}_i =  \alpha_2  \widetilde{wt}_i$

Можем воспользоваться ковариационной матрицей для нахождения оценки коэффициента $\alpha_2$:
\[
\widehat{\alpha_2} = \frac{{\sum{\widetilde{mpg}_i \widetilde{wt}_i}/(n-1)}}{{\sum \widetilde{wt_i^{2}}/(n-1)}}=\frac{-2.10}{0.542}=-3.87454
\]
По такой же схеме можем найти $\hat{\delta_2}$:
\[
\widehat{\delta_2} = \frac{{\sum{\widetilde{mpg}_i \widetilde{wt}_i}/(n-1)}}{{\sum \widetilde{mpg_i^{2}}/(n-1)}}=\frac{-2.10}{14.4}=-0,14583
\]

## Упражнение 2
В глубокий тыл противника заброшен майор Пейн. Оказалось, что ему под силу в уме оценивать $\widehat{\beta}$ для регрессий типа $y_i= \beta x_i + u_i$, а также он что-то слышал про теорему Фриша-Вау-Ловелла.

Что мы хотим? Узнать, сколько нужно построить регрессий, чтобы найти все коэффициенты для:

a) $y_i=\gamma_1 z_i + \gamma_2 x_i + u_i$

б) $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \beta_4 w_i + u_i$

Для начала разберемся с пунктом а) $y_i=\gamma_1 z_i + \gamma_2 x_i + u_i$

Оценим $\gamma_2$ (найдем $y_i$ на $x_i$) :

1) Очистим y от z

2) Очистим x от z

3) Строим регрессию очищенного y на очищенный x

Оценим $\gamma_1$

1) Очистим y от x

2) Очистим z от x

3) Строим регрессию очищенного y на очищенный z

Итого необходимо $3+3 = 6$ регрессий.

Ну а теперь пункт б:

Для оценки $\beta_1$: (строим тройные регрессии)

1) $y_i$ на $x_i,z_i,w_i$ (k штук)

2) 1 на $x_i,z_i,w_i$ (k штук)

3) $\tilde{y_i}$ на $\tilde{1}$

Итого : $2k+1$ регрессий для оценивания одного коэффициента, следовательно для оценивания всех коэффициентов понадобится $4(2k+1)$ однофакторных регрессий,  где $k$ — это количество регрессий на одну переменную, необходимых для того, чтобы узнать оценки всех коэффициентов трехфакторной модели. Заметим, что если за $m$ обозначить количество однофакторных регрессий, которые требуется построить, чтобы рассчитать оценки всех коэффиентов трехфакторной модели, то будет выполняться равенство: $k=3(2m+1)=3(2\times6+1)=39$. Следовательно $4(2k+1)=4(2\times39+1)=316$.

На самом деле Фриш и Ко окружают нас повсюду! Попробуем узреть их теорему в стандартной формуле для $\hat{\beta_2}$ в модели $y_i=\beta_1+\beta_2x_i+u_i$:
\[ \hat{\beta_2} = \frac{\sum{(x_i-\overline{x})(y_i-\overline{y})}}{\sum{(x_i-\overline{x})^2}} \]

Но, как известно, $\overline{x}$ и $\overline{y}$ — это $\hat{\alpha_1}$ и $\hat{\alpha_2}$ в регрессиях  $x_i=\alpha_1$ и $y_i=\alpha_2$ соответственно, а $x_i-\overline{x}$ и $y_i-\overline{y}$ — это остатки. Таким образом, оценку $\hat{\beta_2}$ можно также получить в результате регрессии остатков $\tilde{y_i}$ на остатки $\tilde{x_i}$

## Домашка
4.51, 4.49, 4.47, 4.40, 4.45, 4.46, 4.43, 3.61, 3.60, 3.48, 3.49, 3.47
