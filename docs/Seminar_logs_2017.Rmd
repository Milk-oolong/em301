---
title: "Seminar_logs"
author: "Galina"
date: '27 ноября 2017 г '
output: html_document
---
#Привет, дорогой студент!

Я буду помогать ~~себе~~ тебе быть в курсе всех новостей с семинаров Б. Б. Демешева по эконометрике в 2017 году.

Давай же начнем наше увлекательное приключение в мир регрессий, тестов и красивых графиков! :)

Так как записи семинаров ведутся не с первого сентября, а с 27 ноября 2017 года, то переписывать сюда всё с самого начала трудно, поэтому я прикладываю краткий summary всего самого важного, что было пройдено на лекциях и семинарах за пропущенный период. Если ты нашел неточность или <span style="color:red">ошибку</span>, то скорее пиши Борису Борисовичу или мне.

***

##Конвенция об обозначениях
Во всех последующих записях мы используем следующие обозначения.

1. $a , b$ - это векторы $(n \times 1)$  из констант,

2. $A, B$ - это матрицы $(n \times k)$ из констант,

3. $\alpha, \gamma$ - это скаляры,в виды констант,

4. $r, s$ - это тоже векторы $(n \times 1)$, но состоят они из случайных величин,

5. $R , S$ - это матрицы $(n \times k)$, каждый компонент которых теперь случайная величина,

6. $t$ - это скаляр в виде случайной величины.

Чтобы быть успешным эконометристом (эконометрессой), нужно ~~все-таки выучить~~ вспомнить школьную программу, линейную алгебру и курс ТВ и МС.

###Начнем со старой доброй геометрии.

1. [Теорема Пифагора](http://www.webmath.ru/poleznoe/formules_19_1.php)

2. [Теорема Фалеса](http://www.webmath.ru/poleznoe/formules_19_17.php)

3. [Теорема о трех перпендикулярах](http://www.webmath.ru/poleznoe/formules_19_13.php)

4. [Теорема косинусов и синусов](http://www.math.com.ua/mathdir/teorema_sinus_kosinus.html)

5. [Скалярное произведение векторов и кое-что еще](http://www.webmath.ru/poleznoe/formules_4_10.php)

###Линейная алгебра

1. Матрицы
  
  1.1 [Сложение и вычитание](http://www.webmath.ru/poleznoe/formules_6_5.php)
  
  1.2 [Умножение матриц](http://www.webmath.ru/poleznoe/formules_6_6.php)
  
  1.3 [Обращение матриц](http://www.mathprofi.ru/kak_naiti_obratnuyu_matricu.html)
  
  1.4 [Транспонирование](http://www.webmath.ru/poleznoe/formules_6_7.php)
  
  1.5 [Определитель матрицы](http://www.webmath.ru/poleznoe/formules_6_11.php)
  
  1.6 [След матрицы](https://studopedia.ru/7_45971_sled-matritsi.html)
  

2. [Собственные числа и векторы матриц](http://www.mathprofi.ru/sobstvennye_znachenija_i_sobstvennye_vektory.html)

###ТВиМС

1. Вероятности

  1.1 [Безусловные вероятности](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node2.html#SECTION000200)
  
  1.2 [Условные вероятности](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node11.html#SECTION000500)

2. Характеристики распределений и зависимостей

  2.1 [Математическое ожидание](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node42.html#SECTION0001010)
  
  2.2 [Дисперсия](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node44.html#SECTION0001030)
  
  2.3 [Ковариация](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node48.html#SECTION0001110)
  
  2.4 [Корреляция](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node49.html)
  

3. [Проверка гипотез](http://www.nsu.ru/mmf/tvims/chernova/ms/lec/node40.html#SECTION000700)

4. [p-value](http://datascientist.one/p-value/)

***
#Еще немного теории

Уже не терпится строить регрессии? Всё, всё, это последний ~~но это неточно~~ ликбез.
Давай вспомним про нашего старого друга с первого курса, про дифференциал. 

##Правила дифференцирования векторов и матриц
1. $d(A\times R\times B)$ = $A\times d(R) \times B$

2. $d(\alpha \times a)$ = 0<sub>$(n\times 1$)</sub>

3. $d(\alpha \times A)$ = 0<sub>$(n\times k$)</sub>

4. $d(R+S)$ = $dR +dS$

5. $d(R)'$ = $(dR)'$,  где ' означает транспонирование

6. $d$ сам знает, как дифференцировать сложную функцию.

7. $d(\frac{R}{t})$ = $\frac{d(R) \times t - R \times d(t)}{t^2}$

8. $d(r' \times A \times r)$ = $r' \times (A'+A) d(r)$ (настоятельно рекомендую проверить этот результат, а то мало ли что)

9. $d(R^{-1}) = -R^{-1}\times dR \times R^{-1}$, где $R^{-1}$ это обратная матрица для R

***

Вот теперь мы готовы к изучению эконометрики!

Начнем с самой главной лошадки эконометриста -МНК

##Что такое регрессия?

Начать нужно с того, а что такое регрессия и зачем она нужна. Много чего в жизни можно описать уравнением, но чаще это дает нам статический результат вот здесь и сейчас. Примером такого статического и 100% верного описания будет любая теорема, вот у меня любимая теорема - теорема косинусов, которая утверждает, что если ты знаешь длины двух сторон и косинус угла между ними, то ты без особого труда найдешь третью сторону в треугольнике. Такую зависимость очень просто выразить в виде уравнения, которое описвает зависимость, но является ли оно регрессией? Нет, не является. Почему? Потому что регрессия это не просто описание зависимости между зависимой и объясняемыми переменными, это ожидание значения зависимой переменной при заданных объясняющих переменных, то есть регрессия - это условное математическое ожидание от некоторого процесса, содержащего случайню величину. Заметим важную вещь, что пока мы работаем только с <span style="color:red">линейными</span> связями. Вот мы и получили первый важный вывод:

$E(y_i|x_i) = E((\beta_0+\sum\limits_{i=1}^n\beta_i*x_i +\epsilon_i) | x_i)=\hat{\beta_0}+\sum\limits_{i=1}^n\hat{\beta_i}*x_i=\hat{y}$

***

###OLS без матриц

В чем смысл метода МНК? Считая, что вся случайность i-го наблюдения и непредсказуемость находится в $\epsilon_i$, то мы можем минимизировать ошибку нашего прогноза, сложив и возвеля в квадрат разницу между истинным результатом модели и его оценкой. Зачем нужен квадрат? Что бы не "схлопывались" ошибки прогноза.

Формализация МНК:

$\sum\limits_{i=1}^N (y-\hat{y})^2 \rightarrow \underset{\hat{\beta_j},\; j\in Z}{min}$

F.O.C. дают нам оценки для всех $\hat{\beta_j}$

Если регрессия парная и содержит константу, тогда справедливы следующие результаты минимизации

$\hat{\beta_0} = \overline{y}-\hat{\beta_1}*\overline{x}$

$\hat{\beta_1}= \frac{\sum\limits_{i=0}^N (x_i-\overline{x})*(y_i-\overline{y})}{\sum\limits_{i=0}^N(x_i-\overline{x})^2}= \frac{\hat{cov(x_i,y_i)}}{\hat{var(x)}}$ 

У МНК есть красивая геометрическая интерпретация, которая верна как для парного, так и для многомерного случая, которую я нарисую внизу :)

```{r}
#Тут будет график. Скоро.

```

В трехметрном пространстве есть 4 n-мерных вектора:
$y, x, z \; и \; 1$, где $y \notin Lin(x,z,1)$

Если спроектировать $y \; на\; Lin(x,z,1)$, то полyчим $\hat{y}, \;а \;вектор, \; который \; соединяет \;y \; и\; \hat{y} \; является \; вектором \; ошибок\; прогноза\; -\; RSS$. Теперь спроектируем $\hat{y}$ на единичный вектор, тем самым мы получим вектор средних значений - $\overline{y}$ - через вычитание из вектора $\hat{y}$ необъясняемых квадратов отклонений - $ESS$. Получается, что, так как $RSS \perp \hat{y}, \; а \; ESS \perp \overline{y}$, тогда проекция $y \;на\; 1$ дает в рузультате вектор средних значений $y$  и равен $ESS^2+RSS^2$

Вот так мы получили еще три столба для эконометрики

$RSS=\sum\limits_{i=1}^N (y-\hat{y})^2$ -это объясненная сумма квадратов отклонений

$ESS = \sum\limits_{i=1}^N (\hat{y}- \overline{y})^2$ - это необъясненная сумма квадратов отклонений

$TSS = \sum\limits_{i=1}^N (y-\overline{y})^2$ - это общая сумма квадратов отклонений

И соотношения между ними:

$TSS^2= RSS^2+ESS^2$

$R^2 = \frac{ESS}{TSS}= 1- \frac{RSS}{TSS}$, $R^2 \in [0;1]$ - это коэффициент детерминации, то есть показатель качества подгонки или ( культурным языком ) соответствия нашей модели к реальности.

Интересно заметить, что $R^2= \frac{\sum\limits_{i=1}^N (\hat{y}- \overline{y})^2}{\sum\limits_{i=1}^N (y-\overline{y})^2} = \overline{\rho_{y_i, x_i}}$. Это также имеет красивую геометрическую интерпретацию, которую я привожу ниже.
```{r}
#тут будет график. Скоро
```

Тоже самое пространство, но только оно перевернуто с ног на голову. Заметим, что если $R^2 = \frac{ESS}{TSS}$, то $R^2= cos^2(\phi)$, а косинус можно расписать немного иначе, а именно $cos(\phi) =\frac{(\sum\limits_{i=0}^n (y-\overline{y})*(\hat{y}-\overline{y}))^2}{\sum\limits_{i=0}^n (y-\overline{y})^2*\sum\limits_{i=0}^n(\hat{y}-\overline{y})^2}= \frac{\hat{cov(y, \hat{y})}}{\hat{\sigma^2_y}*\hat{\sigma^2_{\hat{y}}}} = \hat{\rho_{y, \hat{y}}}$

Заметим еще, что задачи $RSS \rightarrow min\; и\; R^2\rightarrow max$ эквивалентны.

Еще раз повторяю. Это справедливо только если есть константа в построенном прогнозе!

###Почему пропал $\epsilon$ ?

Действительно, а почему я так смело убрала случайную величину? Магия, жульничество, тайна за семью печатями? Нет, все проще. Пришло время знакомиться с OLS и теоремой Гаусса-Маркова, в рамках которой мы будем работать довольно долго.

***

####Теорема Гаусса-Маркова (ТГМ)

Работает для парной регрессионной модели, которая 

- Правильно специфицирована

- $\exists x_i, что\; x_i\neq x_j, i,j \in Z$

- $\forall \epsilon_i, E(\epsilon_i)=0, Var(\epsilon_i)=\sigma_{\epsilon_i}^2 \; \epsilon_i \sim N(0, \sigma^2_{\epsilon_i})$

- $cov(\epsilon_i, \epsilon_j) = 0$

Для такой модели оценки МНК:

- Best ($\forall \hat{\beta_i},  MSE^2(\hat{\beta_i})\le MSE^2(\tilde{\beta_i}))$)

- Linear (модель линейна по $y$)

- Unbiased ($E(\hat{\beta_i})=\beta_i$)

- Estimator (с английского - оценка параметра)

На веру ничего и никогда принимать нельзя, поэтому докажем каждый из пунтов последовательно, но перед этим сделаем подготовительные упрощения:

$var(\hat{\beta_0}) = var(\overline{y}-\hat{\beta_1}*\overline{x})=var(\hat{\beta_1}*\overline{x})=\overline{x}^2*var(\frac{\sum\limits_{i=0}^N (x_i-\overline{x})*(y_i-\overline{y})}{\sum\limits_{i=0}^N(x_i-\overline{x})^2})=\sigma^2_{\epsilon}(\frac{\sum\limits_{i=1}^n x_i^2 +n*\overline{x}^2}{n*\sum\limits_{i=1}^n x_i^2})=\sigma^2_{\epsilon}(\frac{1}{n}+\frac{\overline{x}^2}{\sum\limits_{i=1}^n x_i^2})$
$var(\beta_1)=\frac{\sigma^2_{\epsilon}}{\sum\limits_{i=1}^n x_i^2}$

Давайте буде считать, что $w_i=\frac{x_i}{\sum\limits_{j=1}^n x_j^2}$, тогда будет справедливы следующие свойства:

1.$\sum\limits_{i=0}^n w_i = \frac{\sum\limits_{i=0}^n (x_i-\overline{x})}{\sum\limits_{i=0}^nx_i^2}= 0$

2.$\sum\limits_{i=0}^n x_i*w_i = 1$

3.$\sum\limits_{i=0}^n w_i^2 = \frac{1}{\sum\limits_{i=0}^nx_i^2}$

Перепишем наши оценки параметров в новом виде

$\hat{\beta_0}= \sum\limits_{i=1}^n (\frac{1}{n}-w_i*\overline{x})*y_i$

$\hat{\beta_1}= \sum\limits_{i=1}^nw_i*y_i$

#####Best

Рассмотрим $\tilde{\beta_1}=\sum\limits_{i=1}^n \tilde{w_i}*y_i$, которая является несмещенной оценкой лля $\beta_1$, ф значит справедливо $\sum\limits_{i=1}^n \tilde{w_i}=0, \; \sum\limits_{i=1}^n \tilde{w_i}*x_i = 1$.

$var(\tilde{\beta_1}) = var(\sum\limits_{i=1}^n\tilde{w_i}*y_i)= \sigma_{\epsilon}^2*\sum\limits_{i=1}^n\tilde{w_i}^2 \rightarrow min$

Рассмотрим поближе $\sum\limits_{i=1}^n\tilde{w_i}^2 = \sum\limits_{i=1}^n(\tilde{w_i} -w_i+w_i)^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}-w_i)w_i+ \sum\limits_{i=1}^nw_i^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}w_i-w_i^2)+ \sum\limits_{i=1}^n w_i^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}\frac{(x_i-\overline{x})}{\sum\limits_{i=1}^nx_i^2})+ \frac{1}{\sum\limits_{i=1}^n x_i^2}=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2(\frac{\sum\limits_{i=1}^n(\tilde w_i*x_i - \overline{x}*\sum\limits_{i=1}^n(\tilde w_i) - \sum\limits_{i=1}^n(\tilde x^2_i)}{\sum\limits_{i=1}^n(\tilde x^2_i)}) + \frac{1}{\sum\limits_{i=1}^n x_i^2}=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ \frac{1}{\sum\limits_{i=1}^n x^2_i}$

В таком случае, только при $\tilde w_i = w_i, var(\hat{\beta_1})= \frac{\sigma^2_{\epsilon}}{\sum\limits_{i=1}^n x_i^2}$

#####Linear

МНК изначально предполагает линейную связь между $y \;и\;x$

#####Unbiased

$E(\hat{\beta_1})=E(\beta_1 + \frac{\hat{cov(\epsilon, x)}}{\hat{var(x)}})= \beta_1+\frac{E(\sum\limits_{i=1}^n(\epsilon- \hat{\epsilon})*(x-\hat{x}))}{(n-1)*\hat{var(x)}}= \beta_1$

$E(\hat{\beta_0})= E(\hat{y}-\hat{\beta_1}*\hat{x})=\beta_0$

####Полезные результаты МНК без матриц

1. $\overline{\hat{\epsilon}} = 0, \; \hat{\epsilon}= y-\hat{y}$

2. $\overline{y}=\overline{\hat{y}}$

3. $\sum\limits_{i=0}^n x_i*\hat{\epsilon} =0$

4. $\sum\limits_{i=0}^n \hat{y}*\hat{\epsilon} =0$

5. Несмещенная оценка дисперсии ошибки прогноза выражается $\hat{\sigma^2_{\epsilon}}=\frac{RSS}{n-2}$

6. $\frac{RSS}{\sigma^2_{\epsilon}}\sim \chi^2_{n-2}$

7. Нормальность случайных возмущений
 
 Последнее неочевидно, поэтому требует доказательства. Из ТГМ нам известно, что $\epsilon_i \sim N(0, \sigma^2_{\epsilon_i}), \; cov(\epsilon_i, \epsilon_j) = 0$, это дает нам право считать, что $\hat{\beta_1} =\beta_1 + \frac{\hat{cov(x_i,\epsilon_i)}}{\hat{var(x)}}$, поэтому $\hat{\beta_1} \sim N(\beta_1, \sigma^2_{\beta_1})$. Аналочичный анализ справедлив и для $\hat{\beta_0}$
 
8.  $\forall i \in [1,2] \:оценки \:\hat{\beta_i} \; и\; \hat{\sigma^2_{\epsilon}}$ являются независимыми

***

#### МНК в матрицах. 

Тут будет линал, поэтому ~~слабонервным не смотреть~~ нужно кое-что дополнительно ввести.

Да, если бы все в мире описывалось простыми парными регрессиями, то жизнь была бы сказкой, но ~~к сожалению~~ реальность многогранная и парные регрессии используются редко, так как плохо отображают действительность, вместо них используют многофакторные модели или множественные регрессии, поэтому визуализация сложной, а взаимосвязи - не очевидными.






