---
title: "Seminar_logs"
author: "Galina"
date: '27 ноября 2017 г '
output: html_document
---
#Привет, дорогой студент!

Я буду помогать ~~себе~~ тебе быть в курсе всех новостей с семинаров Б. Б. Демешева по эконометрике в 2017 году.

Давай же начнем наше увлекательное приключение в мир регрессий, тестов и красивых графиков! :)

Так как записи семинаров ведутся не с первого сентября, а с 27 ноября 2017 года, то переписывать сюда всё с самого начала трудно, поэтому я прикладываю краткий summary всего самого важного, что было пройдено на лекциях и семинарах за пропущенный период. Если ты нашел неточность или <span style="color:red">ошибку</span>, то скорее пиши Борису Борисовичу или мне.

***

##Конвенция об обозначениях
Во всех последующих записях мы используем следующие обозначения.

1. $a , b$ - это векторы $(n \times 1)$  из констант,

2. $A, B$ - это матрицы $(n \times k)$ из констант,

3. $\alpha, \gamma$ - это скаляры,в виды констант,

4. $r, s$ - это тоже векторы $(n \times 1)$, но состоят они из случайных величин,

5. $R , S$ - это матрицы $(n \times k)$, каждый компонент которых теперь случайная величина,

6. $t$ - это скаляр в виде случайной величины.

Чтобы быть успешным эконометристом (эконометрессой), нужно ~~все-таки выучить~~ вспомнить школьную программу, линейную алгебру и курс ТВ и МС.

###Начнем со старой доброй геометрии.

1. [Теорема Пифагора](http://www.webmath.ru/poleznoe/formules_19_1.php)

2. [Теорема Фалеса](http://www.webmath.ru/poleznoe/formules_19_17.php)

3. [Теорема о трех перпендикулярах](http://www.webmath.ru/poleznoe/formules_19_13.php)

4. [Теорема косинусов и синусов](http://www.math.com.ua/mathdir/teorema_sinus_kosinus.html)

5. [Скалярное произведение векторов и кое-что еще](http://www.webmath.ru/poleznoe/formules_4_10.php)

###Линейная алгебра

1. Матрицы
  
  1.1 [Сложение и вычитание](http://www.webmath.ru/poleznoe/formules_6_5.php)
  
  1.2 [Умножение матриц](http://www.webmath.ru/poleznoe/formules_6_6.php)
  
  1.3 [Обращение матриц](http://www.mathprofi.ru/kak_naiti_obratnuyu_matricu.html)
  
  1.4 [Транспонирование](http://www.webmath.ru/poleznoe/formules_6_7.php)
  
  1.5 [Определитель матрицы](http://www.webmath.ru/poleznoe/formules_6_11.php)
  
  1.6 [След матрицы](https://studopedia.ru/7_45971_sled-matritsi.html)
  

2. [Собственные числа и векторы матриц](http://www.mathprofi.ru/sobstvennye_znachenija_i_sobstvennye_vektory.html)

###ТВиМС

1. Вероятности

  1.1 [Безусловные вероятности](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node2.html#SECTION000200)
  
  1.2 [Условные вероятности](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node11.html#SECTION000500)

2. Характеристики распределений и зависимостей

  2.1 [Математическое ожидание](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node42.html#SECTION0001010)
  
  2.2 [Дисперсия](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node44.html#SECTION0001030)
  
  2.3 [Ковариация](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node48.html#SECTION0001110)
  
  2.4 [Корреляция](http://www.nsu.ru/mmf/tvims/chernova/tv/lec/node49.html)
  

3. [Проверка гипотез](http://www.nsu.ru/mmf/tvims/chernova/ms/lec/node40.html#SECTION000700)

4. [p-value](http://datascientist.one/p-value/)

***
#Еще немного теории

Уже не терпится строить регрессии? Всё, всё, это последний ~~но это неточно~~ ликбез.
Давай вспомним про нашего старого друга с первого курса, про дифференциал. 

##Правила дифференцирования векторов и матриц
1. $d(A\times R\times B)$ = $A\times d(R) \times B$

2. $d(\alpha \times a)$ = 0<sub>$(n\times 1$)</sub>

3. $d(\alpha \times A)$ = 0<sub>$(n\times k$)</sub>

4. $d(R+S)$ = $dR +dS$

5. $d(R)'$ = $(dR)'$,  где ' означает транспонирование

6. $d$ сам знает, как дифференцировать сложную функцию.

7. $d(\frac{R}{t})$ = $\frac{d(R) \times t - R \times d(t)}{t^2}$

8. $d(r' \times A \times r)$ = $r' \times (A'+A) d(r)$ (настоятельно рекомендую проверить этот результат, а то мало ли что)

9. $d(R^{-1}) = -R^{-1}\times dR \times R^{-1}$, где $R^{-1}$ это обратная матрица для R

***

Вот теперь мы готовы к изучению эконометрики!

Начнем с самой главной лошадки эконометриста -МНК

***

##Что такое регрессия?

Начать нужно с того, а что такое регрессия и зачем она нужна. Много чего в жизни можно описать уравнением, но чаще это дает нам статический результат вот здесь и сейчас. Примером такого статического и 100% верного описания будет любая теорема, вот у меня любимая теорема - теорема косинусов, которая утверждает, что если ты знаешь длины двух сторон и косинус угла между ними, то ты без особого труда найдешь третью сторону в треугольнике. Такую зависимость очень просто выразить в виде уравнения, которое описвает зависимость, но является ли оно регрессией? Нет, не является. Почему? Потому что регрессия это не просто описание зависимости между зависимой и объясняемыми переменными, это ожидание значения зависимой переменной при заданных объясняющих переменных, то есть регрессия - это условное математическое ожидание от некоторого процесса, содержащего случайню величину. Заметим важную вещь, что пока мы работаем только с <span style="color:red">линейными</span> связями. Вот мы и получили первый важный вывод:

$E(y_i|x_i) = E((\beta_0+\sum\limits_{i=1}^n\beta_i\times x_i +\epsilon_i) | x_i)=\hat{\beta_0}+\sum\limits_{i=1}^n\hat{\beta_i}\times x_i=\hat{y}$

***

###OLS без матриц

В чем смысл метода МНК? Считая, что вся случайность i-го наблюдения и непредсказуемость находится в $\epsilon_i$, то мы можем минимизировать ошибку нашего прогноза, сложив и возвеля в квадрат разницу между истинным результатом модели и его оценкой. Зачем нужен квадрат? Что бы не "схлопывались" ошибки прогноза.

Формализация МНК:

$\sum\limits_{i=1}^N (y-\hat{y})^2 \rightarrow \underset{\hat{\beta_j},\; j\in Z}{min}$

F.O.C. дают нам оценки для всех $\hat{\beta_j}$

Если регрессия парная и содержит константу, тогда справедливы следующие результаты минимизации

$\hat{\beta_0} = \overline{y}-\hat{\beta_1}\times\overline{x}$

$\hat{\beta_1}= \frac{\sum\limits_{i=0}^N (x_i-\overline{x})\times(y_i-\overline{y})}{\sum\limits_{i=0}^N(x_i-\overline{x})^2}= \frac{\hat{cov(x_i,y_i)}}{\hat{var(x)}}$ 

У МНК есть красивая геометрическая интерпретация, которая верна как для парного, так и для многомерного случая, которую я нарисую внизу :)

```{r}
#Тут будет график. Скоро.

```

В трехметрном пространстве есть 4 n-мерных вектора:
$y, x, z \; и \; 1$, где $y \notin Lin(x,z,1)$

Если спроектировать $y \; на\; Lin(x,z,1)$, то полyчим $\hat{y}, \;а \;вектор, \; который \; соединяет \;y \; и\; \hat{y} \; является \; вектором \; ошибок\; прогноза\; -\; RSS$. Теперь спроектируем $\hat{y}$ на единичный вектор, тем самым мы получим вектор средних значений - $\overline{y}$ - через вычитание из вектора $\hat{y}$ необъясняемых квадратов отклонений - $ESS$. Получается, что, так как $RSS \perp \hat{y}, \; а \; ESS \perp \overline{y}$, тогда проекция $y \;на\; 1$ дает в рузультате вектор средних значений $y$  и равен $ESS^2+RSS^2$

Вот так мы получили еще три столба для эконометрики

$RSS=\sum\limits_{i=1}^N (y-\hat{y})^2$ -это объясненная сумма квадратов отклонений

$ESS = \sum\limits_{i=1}^N (\hat{y}- \overline{y})^2$ - это необъясненная сумма квадратов отклонений

$TSS = \sum\limits_{i=1}^N (y-\overline{y})^2$ - это общая сумма квадратов отклонений

И соотношения между ними:

$TSS^2= RSS^2+ESS^2$

$R^2 = \frac{ESS}{TSS}= 1- \frac{RSS}{TSS}$, $R^2 \in [0;1]$ - это коэффициент детерминации, то есть показатель качества подгонки или ( культурным языком ) соответствия нашей модели к реальности. При добавлении в уравнение регрессии любого фактора, $R^2$ не уменьшится.

Интересно заметить, что $R^2= \frac{\sum\limits_{i=1}^N (\hat{y}- \overline{y})^2}{\sum\limits_{i=1}^N (y-\overline{y})^2} = \hat{\rho_{y_i, x_i}^2}$. Это также имеет красивую геометрическую интерпретацию, которую я привожу ниже.
```{r}
#тут будет график. Скоро
```

Тоже самое пространство, но только оно перевернуто с ног на голову. Заметим, что если $R^2 = \frac{ESS}{TSS}$, то $R^2= cos^2(\phi)$, а косинус можно расписать немного иначе, а именно $cos(\phi) =\frac{(\sum\limits_{i=0}^n (y-\overline{y})\times(\hat{y}-\overline{y}))^2}{\sum\limits_{i=0}^n (y-\overline{y})^2\times\sum\limits_{i=0}^n(\hat{y}-\overline{y})^2}= \frac{\hat{cov(y, \hat{y})}}{\hat{\sigma^2_y}\times\hat{\sigma^2_{\hat{y}}}} = \hat{\rho_{y, \hat{y}}}$

Заметим еще, что задачи $RSS \rightarrow min\; и\; R^2\rightarrow max$ эквивалентны.

Еще раз повторяю. Это справедливо только если есть константа в построенном прогнозе!

###Почему пропал $\epsilon$ ?

Действительно, а почему я так смело убрала случайную величину? Магия, жульничество, тайна за семью печатями? Нет, все проще. Пришло время знакомиться с OLS и теоремой Гаусса-Маркова, в рамках которой мы будем работать довольно долго.

***

####Теорема Гаусса-Маркова (ТГМ)

Работает для регрессионной модели, если выполняются следующие предпосылки

- Модель правильно специфицирована, то есть в нее включен только необходимый $x$, объясняющий $y$

- $\exists x_i, что\; x_i\neq x_j, i,j \in Z$

- $\forall \epsilon_i, E(\epsilon_i)=0$

- $\forall \epsilon_i \: Var(\epsilon_i)=\sigma_{\epsilon_i}^2$

- $\forall \epsilon_i \:\epsilon_i \sim N(0, \sigma^2_{\epsilon_i})$ - случайный член ~~гомоскен~~гомоскедастичен

- $cov(\epsilon_i, \epsilon_j) = 0$ - отсутствие автокорреляции

- Объясняющая переменная содержит некоторую вариацию. Это значит, что $x$ не является константой, иначе мы бы не смогли рассчитать коэффициенты регрессии, так как напомню, что 

$\hat{\beta_0} = \overline{y}-\hat{\beta_1}\times\overline{x}$

$\hat{\beta_1}= \frac{\sum\limits_{i=0}^N (x_i-\overline{x})\times(y_i-\overline{y})}{\sum\limits_{i=0}^N(x_i-\overline{x})^2}= \frac{\hat{cov(x_i,y_i)}}{\hat{var(x)}}$  

А это значит, что $\overline{x}=x$, а следовательно $\hat{\beta_1}$ была бы неопределена, так как числитель и знамеатель дроби были бы равны 0

Это гарантирует, что стахостический компонент содержится только в $\epsilon$.

Для такой модели оценки МНК:

- Best 

$\forall \hat{\beta_i},  MSE(\hat{\beta_i})\le MSE(\tilde{\beta_i}))$

- Linear 

Модель линейна по $y$

- Unbiased 

$E(\hat{\beta_i})=\beta_i$

- Estimator 

С английского - оценка параметра

***

На веру ничего и никогда принимать нельзя, поэтому докажем каждый из пунтов последовательно, но перед этим сделаем подготовительные упрощения:

***

$var(\hat{\beta_0}) = var(\overline{y}-\hat{\beta_1}\times \overline{x})=var(\hat{\beta_1}\times \overline{x})=$

$\overline{x}^2\times var(\frac{\sum\limits_{i=0}^N (x_i-\overline{x})\times (y_i-\overline{y})}{\sum\limits_{i=0}^N(x_i-\overline{x})^2})=$
$\sigma^2_{\epsilon}(\frac{\sum\limits_{i=1}^n x_i^2 +n\times \overline{x}^2}{n\times \sum\limits_{i=1}^n (x_i- \overline{x})^2})=$
$\sigma^2_{\epsilon}(\frac{1}{n}+\frac{\overline{x}^2}{\sum\limits_{i=1}^n (x_i- \overline{x})^2})$

***

$var(\beta_1)=E([\hat{\beta_1}-E(\hat{\beta_1})^2]^2)=E(\hat{\beta_1}-\beta_1)^2=$

$E([\sum\limits_{i=1}^n\frac{\epsilon_i\times (x_i-\overline{x})}{\sum\limits_{j=1}^n(x_j-\overline{x})^2}]\;^2)=E(\sum\limits_{i=1}^n\frac{\epsilon_i^2\times (x_i-\overline{x})^2}{\sum\limits_{j=1}^n(x_j-\overline{x})^4})+\sum\limits_{j=1}^n\sum\limits_{i=1}^n\frac{\epsilon_i\times (x_i-\overline{x})}{\sum\limits_{j=1}^n(x_j-\overline{x})^2}\times \frac{\epsilon_j\times (x_j-\overline{x})}{\sum\limits_{j=1}^n(x_j-\overline{x})^2})=$

$(\sum\limits_{i=1}^n\frac{E(\epsilon_i^2)\times (x_i-\overline{x})^2}{\sum\limits_{j=1}^n(x_j-\overline{x})^4})+\sum\limits_{j=1}^n\sum\limits_{i=1}^n\frac{E(\epsilon_i)\times (x_i-\overline{x})}{\sum\limits_{j=1}^n(x_j-\overline{x})^2}\times \frac{E(\epsilon_j)\times (x_j-\overline{x})}{\sum\limits_{j=1}^n(x_j-\overline{x})^2})$

С учетом, что $E(\epsilon_i)^2=\sigma^2_{\epsilon_i},\:\; E(\epsilon_i\times \epsilon_j)=0$ получаем, что

$var(\beta_1)=\frac{\sigma^2_{\epsilon}}{\sum\limits_{i=1}^n (x_i- \overline{x})^2}$

Ты герой, если дочитал до этого места, а не пропустил доказательство.

***

Давайте буде считать, что $w_i=\frac{x_i - \overline{x}}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}$, тогда будет справедливы следующие свойства:

1.$\sum\limits_{i=0}^n w_i = \frac{\sum\limits_{i=0}^n (x_i-\overline{x})}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}= 0$

2.$\sum\limits_{i=0}^n x_i\times w_i = 1$

3.$\sum\limits_{i=0}^n w_i^2 = \frac{1}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}$

Перепишем наши оценки параметров в новом виде

$\hat{\beta_0}= \sum\limits_{i=1}^n (\frac{1}{n}-w_i\times \overline{x})\times y_i$

$\hat{\beta_1}= \sum\limits_{i=1}^nw_i\times y_i$

***

#####Best

Рассмотрим $\tilde{\beta_1}=\sum\limits_{i=1}^n \tilde{w_i}\times y_i$, которая является несмещенной оценкой лля $\beta_1$, ф значит справедливо $\sum\limits_{i=1}^n \tilde{w_i}=0, \; \sum\limits_{i=1}^n \tilde{w_i}\times x_i = 1$.

$var(\tilde{\beta_1}) = var(\sum\limits_{i=1}^n\tilde{w_i}\times y_i)= \sigma_{\epsilon}^2\times \sum\limits_{i=1}^n\tilde{w_i}^2 \rightarrow min$

Рассмотрим поближе $\sum\limits_{i=1}^n\tilde{w_i}^2 = \sum\limits_{i=1}^n(\tilde{w_i} -w_i+w_i)^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}-w_i)w_i+ \sum\limits_{i=1}^nw_i^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}w_i-w_i^2)+ \sum\limits_{i=1}^n w_i^2=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2\sum\limits_{i=1}^n(\tilde{w_i}\times (\frac{(x_i-\overline{x})}{\sum\limits_{j=1}^n (x_j - \overline{x})^2})- \frac{1}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ 2(\frac{\sum\limits_{i=1}^n(\tilde w_i\times x_i) - \overline{x}\times \sum\limits_{i=1}^n(\tilde w_i)}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}) - \frac{1}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}=$

$\sum\limits_{i=1}^n(\tilde{w_i}-w_i)^2+ \frac{1}{\sum\limits_{j=1}^n (x_j - \overline{x})^2}$

В таком случае, только при $\tilde w_i = w_i, var(\hat{\beta_1})= \frac{\sigma^2_{\epsilon}}{\sum\limits_{i=1}^n x_i^2}$

***

#####Linear

Линейность оценки следует из детерминированности $x$  и линейности $\hat{cov}(x,\epsilon)$

***

#####Unbiased

$E(\hat{\beta_1})=E(\beta_1 + \frac{\hat{cov(\epsilon, x)}}{\hat{var(x)}})= \beta_1+\frac{E(\sum\limits_{i=1}^n(\epsilon- \hat{\epsilon})\times (x-\hat{x}))}{(n-1)\times \hat{var(x)}}= \beta_1$

$E(\hat{\beta_0})= E(\hat{y}-\hat{\beta_1}\times \hat{x})=\beta_0$

***

####Полезные результаты МНК без матриц

В этой главе мы закинем удочку в тему проверки гипотез,так как именно это задача любого анализа модели: доказать, что построенная модель -это не просто набор  из рандомно напиханных факторов, которые каким-то образом попались на глаза в процессе исследования, а обдуманные и очень тщательно выбранные факторы, адекватно отображающие интересующий процесс в жизни.

***

Важно помнить следующие выводы из парной регрессии в условиях ТГМ

1. $\overline{\hat{\epsilon}} = 0, \; \hat{\epsilon}= y-\hat{y}$

2. $\overline{y}=\overline{\hat{y}}$

3. $\sum\limits_{i=0}^n x_i\times \hat{\epsilon} =0$

4. $\sum\limits_{i=0}^n \hat{y}\times \hat{\epsilon} =0$

5. Несмещенная оценка дисперсии ошибки прогноза выражается $\hat{\sigma^2_{\epsilon}}=\frac{RSS}{n-2}$

***
Давай подумаем, какая из всех возможных прямых будет ближе всего ко всем точкам - выборке наблюдений?

Конечно, регрессионная, просто по ее определени,ведь разброс остатков для нее минимальный, а значит среднее квадратичное отклонение для разброса еще меньше. Зная это, выведем еще одну несмещенную оценку для $\sigma^2_{\epsilon_i}$.

$E(MSD(\epsilon_i))=E(\frac{1}{n}\sum\limits_{i=1}^n(\epsilon_i-\overline{\epsilon})^2)=\frac{1}{n}E(\sum\limits_{i=1}^n(\epsilon_i)^2)=\frac{n-2}{n}\times \sigma^2_{\epsilon}$

$s^2_{\epsilon}=\frac{n}{n-2}\times MSD(\epsilon)= \frac{n}{(n-2)\times n} \;\times \sum\limits_{i=1}^n(\epsilon_i)^2=\frac{RSS}{n-2}=\sigma^2_{\epsilon}$

***

6. $\frac{RSS}{\sigma^2_{\epsilon}}\sim \chi^2_{n-2}$

***

Тут надо вспомнить, из чего же складывается распределение $\chi^2_n$

Если $\forall z_i\sim N(0;1),\;z_j\sim N(0;1), \;i\in(1,n)\; и \;при\; условии, \; что \; cov(z_i,z_j)=0$, верно  $\sum\limits_{i=1}^nz_i^2\sim\chi^2_n$

Используем метод пристального взгляда и... очевидно!

***

7. Нормальность случайных возмущений
 
 Последнее неочевидно, поэтому требует доказательства.
 
 ***
 
 Из ТГМ нам известно, что $\epsilon_i \sim N(0, \sigma^2_{\epsilon_i}), \; cov(\epsilon_i, \epsilon_j) = 0$, это дает нам право считать, что 
 
 $\hat{\beta_1} =\frac{\hat{cov}(x_i,y_i)}{\hat{var}(x)} = \frac{\hat{cov}(x_i, \beta_0 +\beta_1\times x_i +\epsilon_i)}{\hat{var}(x)}= \frac{\hat{cov}(x_i, \beta_0}{\hat{var}(x)}+\beta_1 \times \frac{\hat{cov}(x_i, x_i}{\hat{var}(x)}+\frac{\hat{cov}(x_i, \epsilon_i)}{\hat{var}(x)}=$
 
 $\beta_1 + \frac{\hat{cov(x_i,\epsilon_i)}}{\hat{var(x)}}$,
 
 поэтому $\hat{\beta_1} \sim N(\beta_1, \sigma^2_{\beta_1})$. Аналочичный анализ справедлив и для $\hat{\beta_0}$
 
 ***
 
8.  $\forall i \in [1,2] \: оценки \:\hat{\beta_i} \; и\; \hat{\sigma^2_{\epsilon}}$ являются независимыми

***

Это доказывается через ковариацию

***

####Тестирование гипотез в парной регрессии

Ну вот наконец-то я добралась до второго модуля.

Гипотезы!

Для начала обозначим план работ :)

Всего нам нужно знать 2 гипотезы для случая парной регресии.

***

1. Гипотеза о конкретном значении коэффициентов

Рассмотрим только случай двусторонней гипотезы:

$H_0: \beta_1=\beta_1^0$

$H_a: \beta_1 \neq \beta_1^0$

Как мы уже знаем $\beta_1 \sim N(\beta_1, \sigma^2_{\hat{\beta_1}})$, тогда наша тестовая статистика принимает вид

$\frac{\hat{\beta_1}-\beta_1^0}{\sigma_{\hat{\beta_1}}}\sim N(0, 1)$, но вот же грусть-печаль, нам никак не узнать $\sigma^2_{\hat{\beta_1}}$, так как в нее входит неизвестный никому параметр $\sigma^2_{\epsilon_i}$. Тогда поступим классическим образом и используем оценку неизвестного параметра:  $\hat{\sigma^2_{\hat{\beta_1}}}$ 

В таком случае $\frac{\hat{\beta_1}-\beta_1^0}{\hat{\sigma_{\hat{\beta_1}}}}\sim t_{n-2}$, тогда доверительный интервал для проверки будет иметь вид:

$\hat{\beta_1}-t_{n-2, 1-\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_1}}} \le \beta_1^0 \le \hat{\beta_1}+t_{n-2,\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_1}}}$

Если заданное в нулевой гипотезе значение беты попадает в доверительный интервал, тогда гипотеза не отвергается. 

***

2. Гипотеза о значимости коэффициента

Рассмотрим только случай двусторонней гипотезы:

$H_0: \beta_1=0$

$H_a: \beta_1 \neq 0$

В таком случае $\frac{\hat{\beta_1}}{\hat{\sigma_{\hat{\beta_1}}}}\sim t_{n-2}$, тогда доверительный интервал для проверки будет иметь вид:

$\hat{\beta_1}-t_{n-2, 1-\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_1}}} \le \beta_1^0 \le \hat{\beta_1}+t_{n-2,\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_1}}}$

Если 0 попадает в доверительный интервал, тогда гипотеза не отвергается или иными словами, коэффициент НЕ значим. 

***

#### МНК в матрицах. 

Тут будет линал, поэтому ~~слабонервным не смотреть~~ нужно кое-что дополнительно ввести.

Да, если бы все в мире описывалось простыми парными регрессиями, то жизнь была бы сказкой, но ~~к сожалению~~ реальность многогранная и парные регрессии используются редко, так как плохо отображают действительность, вместо них используют многофакторные модели или множественные регрессии, поэтому визуализация становится сложной, а взаимосвязи между факторами и регрессантом - не очевидными. Что же нам делать? Вспоминать линейную алгебру конечно!

Не пугайся, мой друг, больно не будет. :)

Для начала определимся с обозначениями, следуя Конвенции:

1. $y_{n\times1}$ - это вектор регрессант

2. $X_{n\times(k+1)}$ - это матрица констант

3. $\beta_{(k+1)\times1}$ -это вектор из $k+1$ коэффициентов

4. $u_{n\times1}$ -это вектор случайных величин

Поэтому справедливо следущее

1. $y=X\beta+u$

2. $\hat{y}=X\hat{\beta}$

3. $\hat{\beta}=(X'X)^{-1}X'y$

4. $\hat{u}= y- \hat{y}$

5. $E(u)=0_{n \times1}$

6. $Var(u)=\sigma^2\times I_{n\times n}$

7. $RSS= u'u= (y-X\hat{\beta})'(y-X\hat{\beta})=y'y-2\hat{\beta}'X'y+\hat{\beta}'X'X\hat{\beta}$

***

Посмотрим теперь на показатели качества подгонки множественной регрессии

В общем-то коэффициент множественной детерминации будет определяться по той же самой формуле, как и раньше $R^2=\frac{ESS}{TSS}$,и почти все свойства сохраняются с небольшими изменениями.

1. $R^2=1-\frac{RSS}{TSS}$, но опять же оговоримся о надобности константы в модели для работы этого свойства.

2. $R^2=\frac{\hat{var}(\hat{y})}{\hat{var}(y)}$

3. $R^2= \frac{\sum\limits_{i=1}^N (\hat{y}- \overline{y})^2}{\sum\limits_{i=1}^N (y-\overline{y})^2} = \hat{\rho_{y_i, x_i}^2}$

4. $RSS \rightarrow min \Leftrightarrow R^2\rightarrow max$

5. При добавлении фактора $R^2$ не уменьшается

***

$R^2=1-\frac{\sum\limits_{i=1}^n \hat{u}(\hat{\beta_0}...\beta_{k})}{\sum\limits_{i=1}^n(y-\overline{y})^2}$

$R^2=1-\frac{\sum\limits_{i=1}^n \hat{u}(\hat{\beta_0}...\beta_{k+1})}{\sum\limits_{i=1}^n(y-\overline{y})^2}$

Если решить задачу максимизации для двух таких $R^2$, то окажется, что второй $R^2$ не уменьшится. Проверь это сам, а то вдруг я тебя обманываю.

***

Как мы уже знаем, чем больше $R^2$, тем по идее наша модель лучше описывает реальность, то есть напихать ~~доф~~ очень много факторов, то даже при единичном значении $R^2$ модель будет плохой, поэтому умные люди придумали корректировать коэффициент детерминации регрессии на число степеней свободы и назвали его $R_{adj}$.

$R_{adj}=\overline{R^2}=1- \frac{RSS/(n-k-1)}{TSS/(n-1)}$

Свойства:

1. Cвязь коэффициентов $R^2$ и $R^2_{adj}$

$R^2_{adj}= 1-(1-R^2)-\frac{n-1}{n-k-1}$, это следует из из простой фрифметики $R^2=1-\frac{RSS}{TSS} \rightarrow \frac{RSS}{TSS}= 1-R^2$

2.$R^2 \le R^2_{adj}$

Замечу еще, что последнее свойство справедливо только для регрессий с одинаковыми зависимыми переменными, но можно взять разные наборы НЕзависимых переменных.

3. Лучше модель с большим $R^2_{adj}$

Вот так незаметно закончидась глава, а ты боялся. :)

***

####Теорема Гаусса-Маркова в матрицах

В каком-то смысле использоваие упрощает нашу жизнь, например, теперь нужно писать только одну формулу для оценок коэффициентов.

Из-за того что когда-то давно мы предположили, что ошибки - это случайная величина, мы имеем право считать оценки коэффициентов случайными величинами. Это работает и в матрицах, так как матрица $X$ детерминирована, а вектор $u$ случаен.

$\hat{\beta}=(X'X)^{-1}X'y= (X'X)^{-1}X'(X\beta+u)= \beta+(X'X)^{-1}X'u$

#####Теорема Гаусса_макркова для множественной регрессии 

Если в $y=X\beta+u$ выполнены предпосылки

- Модель правильно специфицирована, то есть в нее включены все необходимые факторы, а лишних факторов нет и выбрана правильная функциональная форма модели

- Ранг матрицы $x$ максимален, то есть матрица невырожденная

- $E(u)=0$

- $Var(u)=\sigma_{u}^2\times I_{n\times n}$

- $\forall\epsilon_i,\epsilon_j \in u, \:\: cov(\epsilon_i, \epsilon_j) = 0$ 

тогда оценки МНК являются <span style="color:blue">BLUE</span>

Если ТГМ выполняется, то верно

$\hat{\sigma_{u}}^2= \frac{RSS}{n-k-1}$, где $k$ - это количество факторов в модели.

***

####Ура! Тестируем гипотезы в рамках множественной регрессии!

Я исренне верю, что ты умеешь считать односторонние доверительные интервалы, поэтому распишу только двусторонние. Анализ проверки гипотезы немного усложняется, так как мы работаем не с одной бетой, а с одной из большого множества бет.

Каков наш план, капитан? А вот, собственно:

***

1. Гипотеза о конкретном значении коэффициента

$H_0: \beta_i=\beta_i^0$

$H_a: \beta_i \neq \beta_i^0$

В случае множественной регрессии $\frac{\hat{\beta_i}-\beta_i^0}{\hat{\sigma_{\hat{\beta_i}}}}\sim t_{n-k-1}$, тогда доверительный интервал для проверки будет иметь вид:

$\hat{\beta_i}-t_{n-k-1, \frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_i}}} \le \beta_i^0 \le \hat{\beta_i}+t_{n-k-1,1-\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_i}}}$

Если заданное в нулевой гипотезе значение беты попадает в доверительный интервал, тогда гипотеза не отвергается. Похоже на что-то?

***

2. Гипотеза о значимости коэффициента

Рассмотрим только случай двусторонней гипотезы:

$H_0: \beta_i=0$

$H_a: \beta_i \neq 0$

В таком случае $\frac{\hat{\beta_1}}{\hat{\sigma_{\hat{\beta_1}}}}\sim t_{n-k-1}$, тогда доверительный интервал для проверки будет иметь вид:

$\hat{\beta_i}-t_{n-k-1, \frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_i}}} \le \beta_1^0 \le \hat{\beta_i}+t_{n-k-1,1-\frac{\alpha}{2}}\times \hat{\sigma_{\hat{\beta_i}}}$

Если 0 попадает в доверительный интервал, тогда гипотеза не отвергается или иными словами, коэффициент _НЕ_ значим. 

***

3. Проверка гипотез об адекватности регрессии

Эта гипотеза необходима для проверки наличия линейной зависимости между $x$ и $y$. Если раньше достаточно было проверить значимость $x$, то сейчас надо поступить хитрее. Так как в множественной регрессии существует совокупное влияние нескольких $x_{i,j}$ на $y$, то давай проверим гипотезу о том, что все коэффициенты равны 0.

$H_0: \beta_0=...= \beta_k=0$

$H_a: \exists \beta_i \neq 0, \; \forall i=1... k$

Иными словами нулевая гипотеза гласит, что выбранный набор независимых переменных _НЕ_ оказывает  линейного воздействия на $y$.

Teстовая статистика для проверки гипотезы имеет вид:

$\frac{ESS/k}{RSS/(n-k-1)}=\frac{R^2/k}{(1-R^2)/(n-k-1)} = F_{k, \; n-k-1}$

Грустно тут придумывать доверительный интервал, поэтому просто нужно сравнить полученное значение F-теста с критическим для обозначенного количества степеней свободы

Гипотеза *отвергается*, если $F_{k, \; n-k-1} > F_{\alpha (k, \; n-k-1)}^{cr}$

А теперь разовьем мысль: а что если в $y$ добавить скажем $m-k$ переменных, а чего мелочиться, то каков будет вклад новых факторов в регрессии?

Для этого уже есть гипотеза и статистика.

$H_0:\:\beta_{k+1}=...= \beta_m=0$

$H_a: \exists \beta_i \neq 0, \; \forall i=k+1... m$

$\frac{(RSS-RSS_m)/m-k}{RSS_m/(n-m)} = F_{m-k, \; n-m}$

Если гипотеза отвергается, то вклада новые факторы не приносят.

***

4. Проверка общей гипотезы о наличии линейных соотношений между коэффициетнов

Что такое линейные ограничения? Это когда несколько бет при разных факторах равны между собой или просто равны 0.

Строго говоря, есть три варианта, как проверить такую гипотезу: громоздким t-тестом, тестом Вальда и F-тестом, о последнем, как о наиболее удобном мы и поговорим ниже.

Пусть $Q$ это некоторая матрица размера $(q\times k+1)$, где q -это количество линейно независимых ограничений на коэффициенты (то есть количество не выражаемых друг из друга ограничений).

$H_0: Q\beta= q$

$H_a: Q\beta \neq q$

$F=\frac{(RSS_r-RSS_{ur})/q}{RSS_{ur}/(n-k-1)} = \frac{(R^2_{ur}-R^2_r)/q}{(1-R^2_{ur})/(n-k-1)}$, где $RSS)r$ -модель с учетом всех ограничений $RSS_{ur}$- модель без ограничений. 

Если гипотеза не отвергается, то лучше использовать короткую модель, потому что она удобнее для анализа. И знаешь, для объяснения такой статистики есть геометрическая интерпретация.

```{r}
#Тут будет график. Скоро. А вообще, если ты знаешь, как тут рисовать 3д модели, помоги мне, пожалуйста.
```

***

###Фиктивные переменные и зачем они нужны

Фиктивные переменные или их еще называют дамми - это такие классые переменные, которые принимают только два значения: 0 и 1.
Такие переменные нужны для "разветвления" регрессий на два или больше путей развития событий.

Продолжение следует 
