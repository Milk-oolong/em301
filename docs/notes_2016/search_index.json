[
["index.html", "Конспект семинаров по метрике-2016 О конспекте", " Конспект семинаров по метрике-2016 Студенты ИП и Борис Демешев 2017-03-03 О конспекте Метрика с R :) Напутствия: Идеальный конспект — интересный и без отклонений от здравомыслия. Примочки и пеночки нужны! Не будь занудой! Конспект одного семинара должен иметь одно заглавие уровня # и несколько, скажем от двух до пяти, подзаголовков уровня ##. После «решёточек» должен идти пробел. После заголовка должен стоять краткий английский уникальный идентификатор, например, {#04_matrix_algebra}. Помни об оформлении знаков препинания: после запятой есть пробел, а до запятой — нет. Существует длинное тире, —, которое отличается от просто дефиса -. Рисунки оформляй в открытом софте (tex + tikz, inkscape, graphviz, geogebra, draw.io и прочее) и прикладывай к работе. Есть ещё Ipe и tikzit. Рисунки клади в подпапку images Соблюдай конвенцию о названиях файлов: файлы относящиеся к третьему семинару должны начинаться с 03_, и сам конспект и рисунки. Имена файлов не должны содержать русских букв и пробелов. Никаких здесь и тут в ссылках. Текст, замещающий ссылку, должен быть осмысленным! Например, [Весёлый поисковик](http://www.yandex.ru) Про язык разметки маркдаун. Подробно рассказано как вставлять картинки, списки, цитаты, оформлять разделы и подразделы. Про язык латех от Воронцова. Подробно рассказано, как набирать дроби, суммы и другие формулы. Частые косяки: Функции оформляются со слэшем спереди, например, \\ln, \\cos Забудь про $$! Формулы на всю строчку пишутся в окружении \\[...\\], а именно в три строки: \\[ a^2 + b^2 = c^2 \\] Русский текст внутри формул пишется с помощью \\text{Привет!}. Посмотри, как сделали конспект другие, и сделай лучше! :) Обрати внимание на название .Rmd файлов, на структуру внутри .Rmd файлов. Каждый кусок кода должен иметь уникальное название, например, {r, &quot;plotting_histogram&quot;} Уважай букву ё – ставь над ней точки! :) library(&quot;tidyverse&quot;) # ggplot2 for plots, dplyr for data manipulation, broom and more library(&quot;sandwich&quot;) # оценка Var для гетероскедастичности library(&quot;lmtest&quot;) # тест Бройша-Пагана library(&quot;data.table&quot;) # манипуляции с данными library(&quot;reshape2&quot;) # преобразование длинных таблиц в широкие Данная версия конспекта скомпилирована для html. "],
["ols-intro.html", "1 Метод наименьших квадратов 1.1 Основная задача 1.2 Реализация в R: 1.3 Домашнее задание", " 1 Метод наименьших квадратов Конспект: Бердникович Алеся, Головина Мария дата: 05.09.2016 1.1 Основная задача Маша каждый день ловит покемонов и решает задачи по теории вероятностей. Пусть x и y - случайные величины, \\(x_i\\) - количество решённых в i-тый день задач, а \\(y_i\\) - количество пойманных в i-тый день покемонов. Результаты наблюдения за действиями Маши представлены в таблице: День \\(x_i\\) \\(y_i\\) 1 1 10 2 2 0 3 0 4 Необходимо определить, как количество пойманных за день покемонов зависит от количества решённых за день задач. Предположим, что регрессионная модель имеет линейный вид \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\), где коэффициенты \\(\\beta_1, \\beta_2\\) неизвестны и должны быть оценены, а \\(\\epsilon_i\\) - случайная величина. Тогда прогнозируемая зависимость имеет вид \\(\\hat{y}_i=\\hat{\\beta}_1+\\hat{\\beta}_2x_i\\). 1.1.1 Метод наименьших квадратов (OLS): \\(y_i-\\hat{y}_i\\) - ошибка прогноза, которую нужно минимизировать. Штрафная функция: \\[ Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + (y_3-\\hat{y}_3)^2 = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2 x_1))^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2} \\] 1.1.2 Метод наименьших модулей (LAD): Альтернативный метод минимизации ошибок прогноза. Отличие заключается в виде штрафной функции: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = |y_1-\\hat{y_1}|^2 + |y_2-\\hat{y}_2|^2 + |y_3-\\hat{y}_3|^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2}\\] Найдём \\(\\hat{\\beta}_1,\\hat{\\beta}_2\\) в нашей задаче методом наименьших квадратов: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2x_1)^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2\\] \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (10-(\\hat{\\beta}_1+\\hat{\\beta}_2)^2 + (0-(\\hat{\\beta}_1+2\\hat{\\beta}_2))^2 + (4-(\\hat{\\beta}_1))^2 \\to min\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_1} = -28 + 6\\hat{\\beta}_1 + 6\\hat{\\beta}_2 = 0\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_2} = -20 + 6\\hat{\\beta}_1 + 10\\hat{\\beta}_2 = 0\\] \\[\\hat{\\beta}_1 = \\frac{20}{3}, \\hat{\\beta}_2 = -2\\] Искомая оценка зависимости числа пойманных покемонов от числа решённых задач: \\[\\hat{y}_i = \\frac{20}{3} - 2x_i\\] 1.2 Реализация в R: x &lt;- c(1, 2, 0) y &lt;- c(10, 0, 4) md &lt;- data.frame(problem = x, pokemon = y) md ## problem pokemon ## 1 1 10 ## 2 2 0 ## 3 0 4 Восстановление линейной зависимости методом наименьших квадратов: model_1_ols &lt;- lm(data = md, pokemon~problem) summary(model_1_ols) ## ## Call: ## lm(formula = pokemon ~ problem, data = md) ## ## Residuals: ## 1 2 3 ## 5.333 -2.667 -2.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.667 5.963 1.118 0.465 ## problem -2.000 4.619 -0.433 0.740 ## ## Residual standard error: 6.532 on 1 degrees of freedom ## Multiple R-squared: 0.1579, Adjusted R-squared: -0.6842 ## F-statistic: 0.1875 on 1 and 1 DF, p-value: 0.7399 Подключаем нужный пакет: library(&quot;quantreg&quot;) Если пакета не установлен, то это исправляется командой install.packages(&quot;quantreg&quot;) Восстановление линейной зависимости методом наименьших модулей: model_1_lad &lt;- rq(data = md, pokemon~problem) summary(model_1_lad) ## ## Call: rq(formula = pokemon ~ problem, data = md) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 4.000000e+00 -1.797693e+308 1.797693e+308 ## problem -2.000000e+00 -1.797693e+308 1.797693e+308 Предположим теперь иную модель зависимости \\(y_i = \\hat{\\beta}x_i\\), ищем оценку единственного неизвестного коэффициента \\(\\hat{\\beta}\\) с помощью метода наименьших модулей. Штрафная функция примет вид \\[Q(\\hat{\\beta}) = |10-\\hat{\\beta}| + |0-\\hat{\\beta}| + |4-0| \\to min\\] Точки изломов функции находятся в нулях подмодульных выражений: \\(\\hat{\\beta}=0\\) и \\(\\hat{\\beta}=10\\). Функция принимает наименьшее значение при \\(\\hat{\\beta}=0\\) (см. график), что говорит об отсутствии зависимости числа пойманных покемонов от числа решённых задач. 1.2.1 График штрафной функции: x &lt;- seq(-10, 20, 0.001) fx &lt;- (x &lt;= 0) * (14 - 3 * x) + (x &gt; 0 &amp; x &lt; 10) * (14 + x) + (x &gt;= 10) * (3*x - 6) plot(x = x, y = fx, xlab = expression(hat(beta)), ylab = &#39;Q&#39;, pch = 20, col = &#39;orchid4&#39;) 1.3 Домашнее задание Вывести общие формулы для коэффициентов \\(\\hat{\\beta}, \\hat{\\beta}_1, \\hat{\\beta}_2\\), используя МНК-оценку, при условии, что: \\(y_i = \\beta x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta} x_i\\); \\(y_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta}_1+\\hat{\\beta}_2 x_i + \\epsilon_i\\). "],
["recall-all.html", "2 Вспомнить всё", " 2 Вспомнить всё Найдите длины векторов \\(a=(1,1,1)\\) и \\(b=(1,2,3)\\) и косинус угла между ними. Найдите один любой вектор, перпенидкулярный вектору \\(b\\). Сформулируйте теорему о трёх перпендикулярах и обратную к ней На плоскости \\(\\alpha\\) лежит прямая \\(\\ell\\). Вне плоскости \\(\\alpha\\) лежит точка \\(C\\). Ромео проецирует точку \\(C\\) на прямую \\(\\ell\\) и получает точку \\(R\\). Джульетта проецирует точку \\(C\\) сначала на плоскость \\(\\alpha\\), а затем проецирует полученную точку \\(A\\) на прямую \\(\\ell\\). После двух действий Джульетта получает точку \\(D\\). Обязательно ли \\(R\\) и \\(D\\) совпадают? Для матрицы \\[ A=\\begin{pmatrix} 5 &amp; 4 \\\\ 4 &amp; 5 \\\\ \\end{pmatrix} \\] Найдите собственные числа и собственные векторы матрицы Найдите \\(\\det (A)\\), \\({\\mathrm{tr}}(A)\\) Найдите собственные числа матрицы \\(A^{2016}\\), \\(\\det (A^{2016})\\) и \\({\\mathrm{tr}}(A^{2016})\\) Известно, что \\(X\\) — матрица размера \\(n \\times k\\) и \\(n&gt;k\\), известно, что \\(X&#39;X\\) обратима. Рассмотрим матрицу \\(H=X(X&#39;X)^{-1}X&#39;\\). Укажите размер матрицы \\(H\\), найдите \\(H^{2016}\\), \\({\\mathrm{tr}}(H)\\), \\(\\det(H)\\), собственные числа матрицы \\(H\\). Штрих означает транспонирование. Занудная халява: известно, что \\({\\mathbb{C}ov}(X, Y)=5\\), \\({\\mathbb{V}ar}(X)=10\\), \\({\\mathbb{V}ar}(Y)=20\\), \\({\\mathbb{E}}(X)=10\\), \\({\\mathbb{E}}(Y)=-10\\). Найдите \\({\\mathbb{C}ov}(X+2Y, Y-X)\\), \\({\\mathbb{V}ar}(X+2Y)\\), \\({\\mathbb{E}}(X+2Y)\\). За 100 дней Ромео посчитал все глубокие вздохи Джульетты. Настроение Джульетты столь спонтанно, что глубокие вздохи за разные дни можно считать независимыми. В сумме оказалось 890 вздохов. Сумма квадратов оказалась равна 8000. Постройте 95%-ый доверительный интервал для математического ожидания ежедневного количества глубоких вздохов Джульетты. На~уровне значимости 5%-ов проверьте гипотезу, что математическое ожидание равно~9. Ромео подкидывает монетку два раза. Если монетка выпадает орлом, то Ромео кладет в мешок черный шар, если решкой — белый. Джульетта не знает, как выпадала монетка, и достает шары из мешка наугад по очереди. Первый шар оказался черного цвета. Какова вероятность того, что второй шар Джульетты будет белым? Что-то с памятью моей стало… Линейная алгебра: Великолепный учебник! Strang, Introduction to linear algebra. Ознакомиться можно на gen.lib.rus.ec :) "],
["ols-geometry.html", "3 Геометрия МНК 3.1 Обозначения 3.2 Ныряем в \\(n\\)-мерное пространство 3.3 Больше проекций", " 3 Геометрия МНК конспект: Света Колесниченко дата: 19 сентября 2016 3.1 Обозначения Варианты представления регрессии: Скалярный вариант: \\(\\hat y_{i} = \\hat \\beta_1 + \\hat \\beta_2\\, x_{i} + \\hat \\beta_3\\, z_{i}\\) Векторный вариант: \\(\\hat y = \\hat \\beta_1\\, e + \\hat \\beta_2\\, x + \\hat \\beta_3\\, z\\) \\[ \\begin{matrix} &amp; e = \\vec 1 = \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$}, \\end{matrix} \\] \\[ \\begin{matrix} &amp; x = \\\\ \\end{matrix} \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} \\begin{matrix} &amp; y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\begin{matrix} &amp; z = \\\\ \\end{matrix} \\begin{pmatrix} z_{1} \\\\ \\vdots \\\\ z_{n} \\end{pmatrix} \\textit{ - векторы переменных} \\] Количество наблюдений = \\(n\\), количество коэффициентов \\(\\beta\\) = количество регрессоров = \\(k\\). Матричный вариант: \\(\\hat y = X\\, \\hat \\beta\\) \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\beta_{1} \\\\ \\vdots \\\\ \\hat \\beta_{k} \\end{pmatrix} \\begin{matrix} \\textit{ - вектор размера } k\\times 1, \\end{matrix} \\] \\[ \\begin{matrix} &amp; X = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; x_{1} &amp; z_{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - матрица размера } n \\times k \\end{matrix} \\] Конвенция об обозначениях: \\(y, \\beta, \\hat \\beta, x, z\\) - векторы \\(y_{i}, \\beta_{j}, \\hat \\beta_{7}, x_{45}, z_{37}\\) - числа (скаляры) \\(\\Omega, X, H\\) - матрицы 3.2 Ныряем в \\(n\\)-мерное пространство \\[ \\min_{i\\in I} \\sum_{i=1}^n (y_{i} - \\hat y_{i})^{2} = \\min_{i\\in I} \\sum_{i=1}^n |y_{i} - \\hat y_{i}|^{2} \\textit{ - минимизируем квадрат длины вектора} \\] \\[ \\begin{pmatrix} \\bar y \\\\ \\vdots \\\\ \\bar y \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\vec 1 \\end{matrix} \\] \\(\\bar y = \\hat y\\) - т.к. \\(\\bar y \\cdot \\vec 1 = \\hat y \\cdot \\vec 1\\) среднее значение = среднее значение прогнозов (\\(\\hat y_{i}\\)) Картиночка Лапы “Лапа” = \\(Lin (e, x, z) \\leftarrow\\) выбираем через e, x, z положение \\(\\hat y\\) \\(\\hat y\\) - проекция y на “лапу” y - линейная комбинация e, x, z \\(\\rightarrow\\) лежит в линейной оболочке этих векторов \\(\\hat \\varepsilon = y - \\hat y\\) - вектор “остатков”/ошибок прогнозов/resideals \\(\\hat \\varepsilon \\,\\bot\\, e, \\hat \\varepsilon \\,\\bot\\, x, \\hat \\varepsilon \\,\\bot\\, y\\) \\(\\hat \\varepsilon \\cdot \\vec 1 = 0\\), \\(\\hat \\varepsilon \\cdot x = 0\\), \\(\\hat \\varepsilon \\cdot z = 0\\) \\(\\leftarrow\\) скалярное произведение векторов (ссыль подробнее) перпендикулярных векторов равно 0. \\(\\hat \\varepsilon \\, \\bot \\, \\textit{Лапа} \\rightarrow \\hat \\varepsilon \\, \\bot \\, \\textit{любому вектору, лежащему в Лапе}\\) Великая Теорема о 3 перпендикулярах и аж в 2 формулировках и с чертёжиком. \\[ \\sum_{i=1}^n \\hat \\varepsilon_{i}=0 , \\sum_{i=1}^n \\hat \\varepsilon_{i} x_{i}=0 \\] \\[ \\begin{matrix} &amp; X&#39; = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; \\dots &amp; 1\\\\ x &amp; \\dots &amp; x_{n} \\\\ z &amp; \\dots &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\begin{matrix} &amp; \\hat \\varepsilon = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\varepsilon_{1} \\\\ \\vdots \\\\ \\hat \\varepsilon_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\] Условие ортогональности: \\(X&#39; \\cdot \\hat \\varepsilon = 0\\) - размерность этого нуля - \\(k \\times 1\\) \\(\\hat y = X \\cdot \\hat \\beta \\rightarrow \\hat \\beta = \\frac{\\hat y}{X} = \\frac{\\hat \\varepsilon - Y}{X}\\) 3.2.1 Упражнение 1 Выведите \\(\\hat \\beta\\) из \\(X&#39;\\cdot(y-X\\cdot\\hat \\beta) = 0\\) \\(\\hat \\varepsilon = y - \\hat y = y - X\\cdot\\hat \\beta\\) \\(X\\) - задает “лапу”. \\(\\hat \\beta\\) - отвечает за то, с каким весом в \\(\\hat y\\) входят базисные векторы «лапы». \\(\\hat \\beta = (\\sum_{i=1}^n x^2_{i})^{-1}\\, \\sum_{i=1}^n x_{i} y_{i}=0\\) - для \\(\\hat y_{i} = \\hat \\beta \\, x_{i}\\) \\(X&#39;\\cdot y = X&#39;\\cdot X\\cdot \\hat \\beta\\) \\((X&#39; \\cdot X)^{-1}\\cdot X&#39; \\cdot y = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot X \\cdot \\hat \\beta\\) \\(\\hat \\beta = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot y\\) 3.3 Больше проекций 3.3.1 Упражнение 2 Спроецируйте вектор \\(\\,\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}\\,\\) на прямую, порождённую вектором \\(\\,\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\,.\\) Визуализация задачи \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix} \\begin{matrix} = 4^{-1} \\cdot 10 \\cdot \\frac{1}{4} \\cdot 10 = 2.5 \\\\ \\end{matrix} \\] \\[ \\begin{matrix} &amp; \\hat y = X \\cdot \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 2.5 \\\\ 2.5 \\\\ 2.5 \\\\ 2.5 \\end{pmatrix} \\begin{matrix} = \\\\ \\end{matrix} \\begin{pmatrix} \\bar y \\\\ \\bar y \\\\ \\bar y \\\\ \\bar y \\end{pmatrix} \\] \\[ \\bar y = \\frac{1+2+3+4}{4} = 2.5 \\] Проекция вектора на прямую из единиц даёт вектор из средних. By the way, крутые читщиты по матрицам и основам линейной алгебры. 3.3.2 Упражнение 3 Сформулируйте все теоремы Пифагора \\(\\{\\hat \\varepsilon, \\hat y - \\bar y \\cdot \\vec 1, y - \\bar y \\cdot \\vec 1\\}\\,\\) Чертёжик одного из треугольников По Теореме Пифагора: \\[ |y - \\bar y \\cdot \\vec 1|^{2} = |\\hat \\varepsilon|^2 + |\\hat y - \\bar y \\cdot \\vec 1|^2 \\] \\[ \\sum_{i=1}^n (y_{i}- \\bar y)^2 = \\sum_{i=1}^n \\hat \\varepsilon^2_{i} + \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 \\] \\[ \\begin{matrix} &amp; y_{i}- \\bar y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} - \\bar y\\\\ \\vdots \\\\ y_{n} - \\bar y \\end{pmatrix} \\] Полное задание см. в Задачнике по координатам: 4.23, 4.24, 4.25 Коэффициент детерминации (\\(R^2\\)) - примитивный показатель качества прогнозов. \\[ R^2 = \\frac{ESS}{TSS} = \\frac{\\sum_{i=1}^n \\hat \\varepsilon^2_{i}}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\sum_{i=1}^n (y_{i}- \\hat y)^2}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\text{residial sum of squares (cумма квадратов остатков)}}{\\text{total sum of squares (полная сумма квадратов)}} \\] \\[ ESS = \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 = \\, \\text{explained sum of squares (&quot;объясненная&quot; сумма квадратов)} \\] В МНК работает (и точно только в нём!) соотношение: \\(RSS + ESS = TSS\\). В МНК решается задача минимизации RSS. Если прогнозы \\(\\hat y_i\\) идеально совпадают с \\(y_i\\), то \\(R^2 = 1 \\Rightarrow ESS = TSS\\). \\(R^2 \\in [0;1]\\, , R^2 = \\cos^2 \\rho\\) \\(\\hat y\\) ближе к \\(y\\) с ростом «лапы» \\(\\Rightarrow\\) \\(\\angle \\rho \\downarrow \\, \\rightarrow R^2\\,\\) т.к. \\(\\cos^2 \\rho \\uparrow\\) 3.3.2.1 ДЗ: 1.1, 1.2, 1.7, 1.12, 1.13, 4.13 (1-6), 4.23, 4.24, 4.25 из Задачника 3.3.2.1.1 Полезные ссылки: Репозиторий курса метрики и теории вероятностей "],
["matrix-fight.html", "4 Борьба с матрицами", " 4 Борьба с матрицами дата: 26 сентября 2016 конспект: Вика Шрамова, Эдуард Аюнц Семинар посвящен работе с матрицами - матричному дифференцированию и представлению многомерных случайных величин при помощи матриц. Перед тем как приступить к работе с матрицами, полезно повторить основные свойства операций над матрицами: \\(A(B+C) = AB+ AC\\) \\((A+B)^T=A^T + B^T\\) \\((AB)^T = B^T A^T\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A^{-1})^T = (A^T)^{-1}\\) Производные следа и определителя: \\(tr(AB)&#39;_A = B^T\\) \\(det(A)&#39;_A = det(A) (A^{-1})^T\\) \\((log det A(x) )&#39;_x = tr(A^{-1} A&#39;_x)\\) След и определитель: \\(det(AB) = det(A) det(B)\\) \\(det(A^{-1})= 1/det(A)\\) \\(det(A) = \\prod_j \\lambda_j\\) \\(tr(A) = \\sum_j A_{jj} = \\sum_j \\lambda_j\\) \\(tr(ABC) = tr(BCA) = tr(CAB)\\) Для начала напомним о разнице между одномерными и многомерными случайными величинами. Обозначим \\(y\\) как случайный вектор \\(\\left( \\begin{matrix} y_1 \\\\ \\vdots \\\\ y_n \\end{matrix} \\right)\\). Одномерную случайную величину будем обозначать маленькими латинскими буквами с индексами: \\(y_1\\). \\(Var(y)\\) = \\(\\left( \\begin{matrix} Var(y_1) &amp; Cov (y_1,_2) &amp; \\hdots Cov (y_1,_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_k,y_1) &amp; Var (y_k) &amp; \\hdots Cov (y_k,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_n,y_1) &amp; Cov (y_n,_2) &amp; \\hdots Var (y_n) \\end{matrix} \\right)\\) Из такой записи ковариции векторов очевидно, что если в формуле ковариации поменять местами векторы, то их матрица ковариации будет являться транспонированной матрицой ковариации векторов в исходной последовательности. \\(Cov(y,z) =Cov (z,y)ˆ{T}\\) Упражнение Дана матрица \\(A = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) и случайный вектор \\(y = \\left( \\begin{matrix} y_1 \\\\ y_2 \\end{matrix} \\right)\\) с матожиданием \\(E(y) = \\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) и дисперсией \\(Var(y) = \\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 1 \\end{matrix} \\right)\\) Требуется найти \\(E(z), Var(z), Cov(y,z).\\) Решение \\(E(z) = A \\cdot E(y) = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 4 \\\\ 23 \\end{matrix} \\right)\\) \\(Var(z)\\) = \\(A \\cdot Var(y) \\cdot Aˆ{T}\\) = \\(\\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 12 &amp; 12 \\\\ 12 &amp; 72\\\\ \\end{matrix} \\right)\\) \\(t = Ay\\) = \\(\\left( \\begin{matrix} 2y_1 \\\\ y_1 +3y_2 \\end{matrix} \\right)\\) \\(Cov(y,z)\\) = \\(\\left( \\begin{matrix} Cov (y_1, z_1) &amp; Cov (y_1,z_2) \\\\ Cov(y_2,z_1) &amp; Cov (y_1, z_2) \\end{matrix} \\right)\\) = \\(Cov(y, Ay) = Cov(y,y) Aˆ{T}\\) = \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) Упражнение Предположим, существует истинная зависимость \\(y = X\\beta +\\varepsilon\\) между оцениваемыми величиными. При оценивании параметров модели МНК будут фигурировать следующие величины: \\(y, \\hat{y}, \\varepsilon, \\hat{\\varepsilon}, \\beta , \\hat{\\beta}\\). Оценим все матожидания, дисперсии и ковариации указанных величин. Перед тем, как начать, необходимо сделать оговорку, что существуют две парадигмы исследования: 1) Предполагается, что матрица X является детерминированной. 2) Матрица X состоит из случайных величин. В ходе нашего курса мы будем работать со случайными X, однако пока будем считать, что матрица Х детерминирована. Решение Оценка \\(\\hat{\\beta} = (X&#39;X)^{T} Xy\\), \\(\\hat{\\varepsilon} = y- \\hat{y}\\), \\(\\hat{y} =X\\hat{\\beta}\\) Найдем матожидания: \\(E(\\beta) = beta,\\) так как \\(\\beta\\) – вектор неизвестных констант \\(E(\\epsilon) = 0\\) (по предпосылкам МНК) \\(E(y) = E(X\\beta + \\epsilon) = XE(\\beta) + E(\\epsilon) = X\\beta\\) \\(E(\\hat{y}) = E(X\\hat{\\beta}) = XE(\\hat{\\beta}) = XE((X^{&#39;}X)^{-1}X^{&#39;}y) = X(X^{&#39;}X)^{-1}X^{&#39;}E(y) = X(X^{&#39;}X)^{-1}X^{&#39;}X\\beta = X\\beta\\) \\(E(\\hat{\\epsilon}) = E(y - \\hat{y}) = E(y) - E(\\hat{y}) = X\\beta - X\\beta = 0\\) \\(E(\\hat{\\beta}) = (X^{&#39;}X)^{-1}X^{&#39;}E(y) = (X^{&#39;}X)^{-1}X^{&#39;}X\\beta = \\beta\\) Найдем, к примеру, \\(Cov(\\epsilon, \\hat{\\beta})\\): \\(Cov(\\epsilon,\\hat{\\beta}) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}y) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}(X\\beta+\\epsilon)) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}X\\beta+(X^{&#39;}X)^{-1}X^{&#39;}\\epsilon) = Cov(\\epsilon,\\epsilon)((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}I((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}X^{&#39;&#39;}((X^{&#39;}X)^{-1})^{&#39;} = \\sigma^{2}X((X^{&#39;}X)^{&#39;})^{-1} = \\sigma^{2}X(X^{&#39;}X)^{-1}\\) Существует 2 традиции матричного дифференцирования, суть различия которых заключается в представлении вектора (матрицы) производной — в виде столбца или в виде строки. Основные различия представлены в следующей таблице: \\[\\begin{align} \\frac{\\partial f}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{1}} \\\\ \\dfrac{\\partial f}{\\partial x_{2}} \\\\ \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial f}{\\partial X} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{11}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{1k}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n1}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{nk}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial g}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial g_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{1}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial g_{1}}{\\partial x_{n}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{n}} \\end{pmatrix} \\end{align}\\] Если \\(x\\) – вектор, \\(f(x) = Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = A^{&#39;}\\) Если \\(f(x) = x^{&#39;}Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = (A + A^{&#39;})x\\) Если \\(f(X) = det(X)\\), то \\(\\dfrac{\\partial f}{\\partial x} =det(X)(X^{-1})^{&#39;}\\) Доказательство второго свойства и не только можно почитать здесь "],
["-05-models-evaluation.html", "5 Доказательство свойств и оценка моделей. {#05_models_evaluation}", " 5 Доказательство свойств и оценка моделей. {#05_models_evaluation} конспект: Артём Калинин, Кирилл Улыбин дата: 9 сентября 2016 5.0.1 Пример из домашки Сначала разобрали пример из домашки прошлого семинара: Искали \\(cov(\\hat{y}, \\hat{\\epsilon})=?\\) \\(cov(\\hat{y}, \\hat{\\epsilon})= cov(X\\hat{\\beta}, y - \\hat{y})=cov(y-\\hat{y}, X\\hat{\\beta})&#39;=cov((I-\\underbrace{X(X&#39;X)^{-1}X&#39;}_H)y , X(X&#39;X)^{-1}X&#39;y))&#39;=\\) \\(=((I-H)cov(y, y)(H)&#39;)&#39;=(\\sigma^{2}(IH-H))&#39;=(\\sigma^{2}(H-H))&#39;=0\\) Интуитивное объяснение результата - предсказания не должны зависеть от ошибок. Например, если бы предсказания положительно зависели от ошибок, можно было бы сделать поправку в предсказании на известную величину ошибки. Вспомогательно: H - матрица-шляпница! Почему? H*(любой вектор)=(его проекция) \\(Hy=\\hat{y}\\) \\(H&#39;=H\\) \\(H^{2}=X(X&#39;X)^{-1}\\underbrace{X&#39;X(X&#39;X)^{-1}}_IX&#39;=H\\) 5.0.2 Упражнение 1 Даны модель А: \\(y_i=\\beta x_i + \\epsilon_i\\), модель B: \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\) \\(\\sum_{i=1}^n x_i=50\\);\\(\\sum_{i=1}^n x_iy_i=-50\\); \\(\\sum_{i=1}^n x_i^2=2000\\); \\(\\sum_{i=1}^n y_i=20\\); \\(\\sum_{i=1}^n y_i^2=500\\); \\(n=100\\) Найти для модели B (для модели A - дома): \\(1)X-?\\) \\(2)X&#39;X-?\\) \\(3)\\hat{\\beta}-?\\) \\(4)Var(\\hat{\\beta})-?\\) \\(5)X&#39;y-?\\) Решение 1)\\(X\\) = \\(\\left( \\begin{matrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\\\ \\end{matrix} \\right)\\) 2)\\(X&#39;\\) = \\(\\left( \\begin{matrix} 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\dots &amp; x_n \\\\ \\end{matrix} \\right)\\) \\((X&#39;X)\\) = \\(\\left( \\begin{matrix} 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\dots &amp; x_n \\\\ \\end{matrix} \\right)\\) \\(\\left( \\begin{matrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} n &amp; x_1+x_2+...x_n &amp;\\\\ x_1+x_2+...+x_n &amp; x_1^2+x_2^2+...+x_n^2 &amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} 100 &amp; 50 &amp;\\\\ 50 &amp; 2000 &amp;\\\\ \\end{matrix} \\right)\\) 3)\\(X&#39;y\\)=\\(\\left( \\begin{matrix} 1 &amp; \\dots &amp; 1 &amp;\\\\ x_1 &amp; \\dots &amp; x_n &amp;\\\\ \\end{matrix} \\right)\\) \\(\\left( \\begin{matrix} y_1&amp;\\\\ \\vdots&amp;\\\\ y_n&amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} \\sum_{i=1}^n y_i &amp;\\\\ \\sum_{i=1}^n x_iy_i &amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)\\) 4)\\(\\hat{\\beta}=(X&#39;X)^{-1}X&#39;y=\\left( \\begin{matrix} 100 &amp; 50\\\\ 50 &amp; 2000\\\\ \\end{matrix} \\right)^{-1}=\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)=\\frac{1}{197500}\\left( \\begin{matrix} 2000 &amp; -50\\\\ -50 &amp; 100\\\\ \\end{matrix} \\right)\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)=\\frac{1}{197500}\\left( \\begin{matrix} 42500 \\\\ -51000 \\\\ \\end{matrix} \\right)\\) 5)\\(Var(\\hat{\\beta})=\\sigma^2(X&#39;X)^{-1}=\\sigma^2*\\frac{1}{197500}\\left(\\begin{matrix} 2000 &amp; -50 \\\\ -50 &amp; 100\\\\ \\end{matrix} \\right)\\) 5.0.3 Упражнение 2 Доказать, что \\(Var(Ay)=AVar(y)A&#39;\\) Напомним, что \\(Var(z) = \\left(\\begin{matrix} var(z_1) &amp; cov(z1,z2) &amp; \\dots &amp; cov(z_1,z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ cov(z_n, z_1) &amp;\\dots &amp;\\dots &amp; var(z_n)\\\\ \\end{matrix} \\right)\\) Сначала докажем вспомогательные утверждения: а)\\(E(Ay)=AE(y)\\) б)\\(E(zB)=E(z)B\\) в)\\(Var(z)=E(zz&#39;) - E(z)E(z&#39;)\\) а)\\(E(Ay)=AE(y)\\) Левая часть: \\(E(Ay)=Left_{ij} = E(\\sum_{k=1}^s a_{ik}y_{kj})=\\sum_{k=1}^s a_{ik}E(y_{kj})\\) Правая часть: \\(AE(y) = Right_{ij}=\\sum_{k=1}^s a_{ik}E(y_{kj})\\) б)\\(E(zB)=E(z)B\\); Док-во такое же как в (а) в)\\(Var(z)=E(zz&#39;) - E(z)E(z&#39;)\\) Левая часть: \\(Var(z) = Left_{ij}=cov(z_i,z_j)\\) Правая часть: \\(Right_{ij}=E(zz&#39;)_{ij} - (E(z)E(z&#39;))_{ij}=E(z_iz_j)-E(z_i)E(z_j)=cov(z_i,z_j)\\) \\(E(zz&#39;)= \\left( \\begin{matrix} E(z_1^2) &amp; E(z_1z_2) &amp; \\dots &amp; E(z_1z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ E(z_nz_1) &amp; \\dots &amp; \\dots &amp; E(z_n^2)\\\\ \\end{matrix} \\right)\\) \\(E(z)E(z&#39;) = \\left( \\begin{matrix} E(z_1)E(z_1) &amp; \\dots &amp; E(z_1)E(z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ E(z_n)E(z_1) &amp; \\dots &amp; E(z_n)E(z_n)\\\\ \\end{matrix} \\right)\\) Теперь вернемся к доказательству \\(Var(Ay)=AVar(y)A&#39;\\). \\(Var(Ay) = E(Ay(Ay)&#39;)-E(Ay)E((Ay)&#39;)=E(Ayy&#39;A&#39;)-E(Ay)E(y&#39;A&#39;)=AE(yy&#39;)A&#39;-AE(y)E(y&#39;)A&#39;=A(\\underbrace{E(yy&#39;)-E(y)E(y&#39;))}_{Var(y)}A&#39;=AVar(y)A&#39;\\) 5.0.4 Упражнение 3 Дано \\(X_{n\\times1}\\), \\(A_{n\\times n}\\) \\(f(A)_{1\\times 1}=X&#39;_{1\\times n}A_{n\\times n}X_{n\\times 1}\\) Найти \\(\\frac{\\partial f}{\\partial A}\\), т.е. скаляр дифференцируем по каждому элементу матрицы А. Решение: \\(\\frac{\\partial f}{\\partial A}_{ij}=\\frac{\\partial f}{\\partial a_{ij}}\\) \\(X&#39;A= \\left( \\begin{matrix} x_1 &amp; x_2 &amp;\\dots &amp; x_n \\\\ \\end{matrix} \\right) \\left( \\begin{matrix} a_{11} &amp; a_{12}&amp;\\dots &amp; a_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ a_{n1} &amp; \\dots &amp;\\dots &amp; a_{nn} \\end{matrix} \\right) = \\left( \\begin{matrix} \\sum_{i=1}^n x_ia_{i1} &amp; \\sum_{i=1}^n x_ia_{i2}&amp; \\dots &amp; \\sum_{i=1}^n x_ia_{in} \\\\ \\end{matrix} \\right)_{1\\times n}\\) \\(X&#39;AX = \\left( \\begin{matrix} \\sum_{i=1}^n x_ia_{i1} &amp; \\sum_{i=1}^n x_ia_{i2}&amp; \\dots &amp; \\sum_{i=1}^n x_ia_{in} \\\\ \\end{matrix} \\right)\\left( \\begin{matrix} x_1 \\\\ x_2\\\\ \\vdots\\\\ x_n\\\\ \\end{matrix} \\right)= \\sum_{j=1}^n (x_j \\sum_{i=1}^n x_i a_{ij})\\) Тогда можно переписать в виде: \\(f(A)= \\sum_{j=1}^n (x_j \\sum_{i=1}^n x_i a_{ij})= \\sum_{i=1, j=1}^n x_ix_ja_{ij}\\) То есть \\(\\frac{\\partial f}{\\partial A_{ij}}=x_ix_j\\) \\(\\frac{\\partial f}{\\partial A}= \\left( \\begin{matrix} x_1^2 &amp; x_1x_2 &amp; \\dots &amp; x_1x_n\\\\ x_2x_1&amp; x_2^2 &amp; \\dots &amp; x_2x_n \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ x_nx_1&amp; \\dots &amp; \\dots &amp; x_n^2\\\\ \\end{matrix} \\right)=XX&#39;\\) 5.0.5 Упражнение 4. Оценим модель с помощью ML Пусть истинная зависимость \\(y = X\\beta +\\varepsilon\\), причем \\(\\varepsilon \\sim N(0;\\sigma^{2}I)\\) Всего n наблюдений и k регрессоров Найти: а) \\(\\hat{\\beta_{ML}}\\), \\(\\hat{\\sigma^{2}_{ML}}\\) б) \\(E(\\hat{\\beta_{ML}})\\), \\(E(\\hat{\\sigma}^{2}_{ML})\\) с) \\(Var(\\hat{\\beta_{ML}})\\), \\(Var(\\hat{\\sigma}^{2}_{ML})\\) Решение а) Найдем ML оценки X и y известны, оцениваем \\(\\beta = \\left( \\begin{matrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{matrix} \\right)\\) и \\(\\sigma^{2}\\) Из \\(\\varepsilon \\sim N(0;\\sigma^{2}I))\\) и \\(y = X\\beta +\\varepsilon\\) следует, что \\(y \\sim N(X\\beta;\\sigma^{2}I)\\) Запишем формулу плотности многомерного нормального распределения: \\(p(y)=\\frac{1}{\\sqrt{(2\\pi)^n} \\sqrt{det(\\sigma^{2}I)}}e^{-\\frac{1}{2}(y-X\\beta)^{&#39;}(\\sigma^{2}I)^{-1}(y-X\\beta)}\\) Для удобства логарифмируем и получим задачу максимизации фукции правдоподобия: \\(Q=ln(p(y))=-\\frac{n}{2}ln(2\\pi)-\\frac{1}{2}ln(det(\\sigma^{2}I))-\\frac{1}{2}(y-X\\beta)^{&#39;}(\\sigma^{2}I)^{-1}(y-X\\beta) \\rightarrow \\max\\limits_{\\beta, \\sigma^{2}}\\) Заметим, что первое слагаемое не влияет на решение задачи максимизации, а \\(det(\\sigma^{2}I)=\\sigma^{2n}\\) \\(\\frac{\\partial Q}{\\partial \\beta}\\): \\(Q=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}-X^{&#39;}\\beta^{&#39;})(y-X\\beta)=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}y-\\beta^{&#39;}X^{&#39;}y-y^{&#39;}X\\beta+\\beta^{&#39;}X^{&#39;}X\\beta)\\) Заметим: \\(y^{&#39;}X\\beta\\) - скаляр, причем \\((y^{&#39;}X\\beta)^{&#39;}=\\beta^{&#39;}X^{&#39;}y\\), тогда можно записать в виде: \\(Q=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}y-2y^{&#39;}X\\beta+\\beta^{&#39;}X^{&#39;}X\\beta)\\) \\(\\frac{\\partial Q}{\\partial \\beta}=-\\frac{1}{2\\sigma^{2}}((-2y^{&#39;}X)^{&#39;}+(X^{&#39;}X+(X^{&#39;}X)^{-1})\\hat{\\beta})=0\\) \\(X^{&#39;}X\\hat{\\beta}=X^{&#39;}y\\) \\(\\hat{\\beta}=(X^{&#39;}X)^{-1}X^{&#39;}y\\) \\(\\frac{\\partial Q}{\\partial \\sigma^{2}}\\): \\(Q=-\\frac{n}{2}ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{&#39;}(y-X\\beta)\\) \\(\\frac{\\partial Q}{\\partial \\sigma^{2}}=\\frac{n}{\\hat{\\sigma}^{2}}-\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{\\hat{\\sigma}^{2}}\\) \\(\\hat{\\sigma}^{2}=\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{n}\\) б) По свойству ML оценок: \\(E(\\hat{\\beta})=\\beta\\), \\(E(\\hat{\\sigma}^{2})=\\sigma^{2}\\) с) Чтобы найти \\(Var(\\hat{\\beta_{ML}})\\), \\(Var(\\hat{\\sigma}^{2}_{ML})\\) посчитаем вторые производные: \\(\\frac{\\partial^2 Q}{\\partial \\beta^2}=-\\frac{X^{&#39;}X}{\\sigma^{2}}\\) \\(\\frac{\\partial^2 Q}{\\partial (\\sigma^{2})^2}=\\frac{n}{2(\\sigma^{2})^2}-\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{(\\sigma^{2})^3}\\) \\(\\frac{\\partial^2 Q}{\\partial \\beta \\partial \\sigma^{2}}=-\\frac{X^{&#39;}(y-X\\hat{\\beta})}{(\\sigma^{2})^{2}}=-\\frac{X^{&#39;}(y-(X^{&#39;}X)^{-1}X^{&#39;}Xy)}{(\\sigma^{2})^{2}}=0\\) тогда Var \\(\\left( \\begin{matrix} \\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\\\ \\end{matrix} \\right) = \\left( \\begin{matrix} -\\frac{1}{n}\\frac{\\partial^2 Q}{\\partial \\beta^2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{n}\\frac{\\partial^2 Q}{\\partial (\\sigma^2)^2}\\\\ \\end{matrix} \\right)^{-1} = \\left( \\begin{matrix} \\frac{n\\sigma^2}{X&#39;X} &amp; 0 \\\\ 0 &amp; 2(\\sigma^2)^2\\\\ \\end{matrix} \\right)\\) 5.0.6 ДЗ В упражнении 1 сделать те же пункты для модели А Даны \\(A_{r\\times s}\\), \\(B_{s\\times r}\\) Записать через эти матрицы ( и их преобразования) сумму: \\(\\sum_{i=1, j=1}^n a_{ij}b_{ij}=?\\) 5.0.7 Ссылки В лекции 5 Бостонского университета подробнее изложено про матрицу-шляпницу и её свойста Да и в целом курс Бостонского университета хорош :) "],
["hints-and-hats.html", "6 Статистические свойства RSS 6.1 Упражнение 1 6.2 Теорема 6.3 Домашка", " 6 Статистические свойства RSS Датa 10/10/16 Конспект: Мария Такташева, Алексей Панков Главный спонсор этого семинара – Алеся Бердникович, которая все решила дома. объявление публикуется на правах рекламы 6.1 Упражнение 1 Найти матожидание и дисперсию оценки параметра \\(\\hat{\\beta}\\) методом максимального правдоподобия. Для этого построим функцию правдоподобия: \\[ Q = \\ln p(y) = - \\dfrac{n}{2} \\ln 2 \\pi - \\dfrac{1}{2} \\ln \\det\\left(\\sigma^2 I\\right) - \\dfrac{1}{2} (y - X\\beta)&#39; (\\sigma^2 I)^{-1} (y - X \\beta) \\rightarrow \\max_{\\hat{\\beta}, \\hat{\\sigma^2}} \\] 6.1.1 Матожидание оценки Продифференцируем функцию правдоподобия по \\(\\hat{\\beta}\\), чтобы найти оценку ML параметра \\(\\beta\\) \\[\\begin{align} \\dfrac{\\partial Q}{\\partial \\hat{\\beta}} = \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ -\\dfrac{1}{2} \\left(y - X\\hat{\\beta} \\right)&#39;\\left(\\sigma^2 I\\right)^{-1}\\left(y- X\\hat{\\beta} \\right) \\right] = -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[\\left(y&#39; - \\hat{\\beta}&#39;X&#39;\\right)\\left(y - X\\hat{\\beta}\\right)\\right] = \\\\ = -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ y&#39;y -\\hat{\\beta}&#39;X&#39;y - y&#39;X\\hat{\\beta} - \\hat{\\beta}X&#39;X\\hat{\\beta} \\right] = 0 \\end{align}\\] А теперь заметим, что это выражение состоит из скаляров. И действительно: \\(y&#39;y = y&#39;_{1 \\times n} \\times y_{n \\times 1} = y&#39;y_{1 \\times 1} \\) \\(\\hat{\\beta}&#39;X&#39;y = \\hat{\\beta}&#39;_{1\\times k} \\times X&#39;_{k \\times n} \\times y_{n \\times 1} = \\hat{\\beta}&#39;X&#39;y_{1 \\times 1} \\) \\(y&#39;X\\hat{\\beta} = y&#39;_{1 \\times n} \\times X_{n \\times k} \\times \\hat{\\beta}_{k \\times 1} = y&#39;X\\hat{\\beta}_{1 \\times 1} \\) ну и без лишних подробностей \\(\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}_{1 \\times 1} \\) При этом известно, что если матрица \\(A = A_{1 \\times 1}\\) — скаляр, то \\(A&#39;= A\\). Значит \\[ \\hat{\\beta}&#39;X&#39;y = \\left( y&#39;X\\hat{\\beta} \\right)&#39; = y&#39;X\\hat{\\beta} \\] и выражение выше принимает вид \\[ -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ y&#39;y -\\hat{\\beta}&#39;X&#39;y - 2\\hat{\\beta}X&#39;X\\hat{\\beta} \\right] = 0 \\] Вспомнив некоторые правила матричного дифференцирования, можно прийти к виду: \\[\\begin{align} -\\dfrac{1}{2\\sigma^2 I} \\left[ -2X&#39;y + X&#39;X\\hat{\\beta} + \\left(X&#39;X\\right)&#39;\\hat{\\beta} \\right] = \\nonumber \\\\ = -\\dfrac{1}{2\\sigma^2 I} \\left[ -2X&#39;y + 2X&#39;X \\hat{\\beta} \\right] = 0 \\end{align}\\] Для тех, кто не помнит, как работать с матрицами \\[ \\dfrac{\\partial}{\\partial x} x&#39;Ax = \\left(A&#39; + A \\right) \\] \\[ \\dfrac{\\partial}{\\partial x} x&#39;A = A \\] \\[ \\dfrac{\\partial}{\\partial x} Ax = A&#39; \\] В итоге мы получаем оценку \\(\\hat{\\beta}_{ML} = \\left(X&#39;X\\right)^{-1}X&#39;y\\), которая совпадает с оценкой \\(\\hat{\\beta}_{OLS}\\), построенной методом наименьших квадратов. 6.1.2 Дисперсия оценки До того как мы будем искать производную функции правдоподобия, неплохо бы заметить, что из-за свойств определителя диагональной матрицы \\[ \\det\\left(\\sigma^2 I\\right) = \\left(\\sigma^2\\right)^n \\] Зная этот хинт, можно дифференцировать функцию правдоподобия по \\(\\hat{\\sigma^2}\\): \\[\\begin{align} \\dfrac{\\partial Q}{\\partial \\hat{\\sigma^2}} = \\dfrac{1}{2} \\left(y -X\\hat{\\beta}\\right)&#39;\\left(y-X\\hat{\\beta}\\right)\\cdot \\dfrac{1}{(\\hat{\\sigma^2})^2} - \\dfrac{1}{2} \\cdot \\dfrac{n}{\\hat{\\sigma^2}} = 0 \\end{align}\\] \\[ \\left(y -X\\hat{\\beta}\\right)&#39;\\left(y-X\\hat{\\beta}\\right) = n\\hat{\\sigma^2} \\] \\[ \\hat{\\sigma^2} = \\dfrac{(y -X\\hat{\\beta})&#39;(y-X\\hat{\\beta})}{n} \\] Теперь вспоминаем, что \\(\\hat{y} = X\\hat{\\beta} \\), \\(\\hat{\\varepsilon} = y - \\hat{y} \\), а \\(\\hat{\\varepsilon}&#39;\\hat{\\varepsilon} = RSS\\), откуда: \\[ \\hat{\\sigma^2} = \\dfrac{RSS}{n} \\] 6.1.3 Тривиальщина, которая здорово упрощает жизнь Пусть у нас есть матрица \\(a_{1\\times 1}\\), тогда магически \\[ a&#39; = a \\quad \\det(a) = a \\quad tr(a) = a \\] Теперь в более общем виде c \\(Z_{n\\times m}\\). А правда ли, что \\[ \\mathbb{E} \\left(Z&#39;\\right) = \\left( \\mathbb{E} \\left( Z \\right)\\right)&#39; \\]? Да! Математическое ожидание матрицы — это матрица, в которой от каждого элемента взято математическое ожидание. Транспонирование просто переставляет элементы матрицы, не изменяя их. А это тоже верно? \\[ \\det \\left(\\mathbb{E} \\left(Z\\right)\\right) = \\mathbb{E}\\left(\\det\\left(Z\\right)\\right) \\]? Нет. Приведем простой контрпример: пусть \\(A\\) — матрица \\(2\\times 2\\), тогда её определитель легко посчитать по формуле \\[ \\det\\left(\\mathbb{E}(A)\\right) = \\det \\left( \\begin{matrix} \\mathbb{E}(a_{1,1}) &amp; \\mathbb{E}(a_{1,2}) \\\\ \\mathbb{E}(a_{2,1}) &amp; \\mathbb{E}(a_{2,2}) \\end{matrix} \\right) = \\mathbb{E}(a_{1,1})\\mathbb{E}(a_{2,2}) + ... \\ne \\mathbb{E}\\left(a_{1,1}a_{2,2} + ...\\right) = \\mathbb{E}(\\det(A)) \\] Математическое ожидание произведения не равно произведению математических ожиданий в общем случае. Однако, это может быть верно, когда элементы матрицы не зависят друг от друга. Может быть \\[ tr\\left(\\mathbb{E}\\left(Z\\right)\\right) = \\mathbb{E}\\left(tr\\left(Z\\right)\\right) \\]? Точно! Ведь след — это сумма диагональных элементов Вывод: математическое ожидание любит след и транспонирование Продолжаем. \\[ RSS = (y - X\\hat{\\beta})&#39; (y - X\\hat{\\beta}) = (y - X(X&#39;X)^{-1} X&#39; y)&#39;(...), \\] где \\(H = X(X&#39;X)^{-1} X&#39;\\) — “матрица-шляпница” или “hat matrix” 6.1.4 Магические свойства “матрицы-шляпницы” А матрица \\(X\\) у Себера называется «матрица плана» Это матрица проекции. \\[ Hy = \\hat{y}, \\quad H\\times \\text{любой вектор = проекция этого вектора на &quot;лапу&quot;} \\] Напоминаем, что “лапа” — линейная оболочка вектора \\(X\\). Два раза проецировать можно, но результат не изменится :) \\[ H^2 y = Hy = \\hat{y} \\] \\[ H&#39; = H \\] Возвращаемся к дисперсии оценки \\(\\hat{\\beta_{ML}}\\) \\[ RSS = ((I-H)y)&#39;((I-H)y) = y&#39;(I-H)&#39;(I-H)y = \\] \\((A-B)&#39; = A&#39; - B&#39; \\Rightarrow \\) \\[ = y&#39; (II - HI - IH +H^2) y = y&#39;(I-H) y \\] Поскольку \\(RSS\\) имеет размерность \\(1\\times 1\\), можно перейти к следу, для того, чтобы переставить местами множители \\[ \\mathbb{E}(RSS) = \\mathbb{E}(tr(RSS)) =\\mathbb{E}(tr(y&#39;(I-H) y) = \\] \\(tr(A\\cdot B) = tr(B \\cdot A), \\quad tr(A+B) = tr(A) + tr(B) \\Rightarrow \\) \\[ = \\mathbb{E}(tr((I-H)y&#39;y) = \\mathbb{E}(tr(yy&#39;) - tr(Hyy&#39;)) = tr(\\mathbb{E}(yy&#39;)) - tr(\\mathbb{E}(Hyy&#39;)) = tr(\\mathbb{E}(yy&#39;)) - tr(H \\mathbb{E}(yy&#39;)) = \\] \\(\\mathbb{E}(yy&#39;)_{n\\times n} =\\) ?, можно найти из \\(Var(y) = \\mathbb{E}(yy&#39;) - \\mathbb{E}(y)\\mathbb{E}(y&#39;)\\) \\(Var(y) = \\sigma^2 I\\ \\) \\(\\mathbb{E}(y)\\mathbb{E}(y&#39;) = X\\beta \\beta&#39;X&#39; \\), т.к. \\(y = X\\beta + \\varepsilon, \\mathbb{E}(y) = \\mathbb{E}(X\\beta) + \\mathbb{E}(\\varepsilon) = X\\beta \\) \\(\\Rightarrow \\mathbb{E}(yy&#39;) = \\sigma^2 I + (X\\beta \\beta&#39; X&#39;)_{n \\times n}\\) \\[ = tr((I-H)(\\sigma^2 I + X\\beta \\beta&#39; X&#39;) = tr(\\sigma^2(I-H)) = \\] \\((I-H)(X\\beta \\beta&#39;X&#39;) = X\\beta \\beta&#39;X&#39; - X\\beta \\beta&#39;X&#39; = 0\\) Хорошее свойство следа: \\(tr(X) = \\sum_i \\lambda_i \\) — равен сумме собственных чисел матрицы \\[ = \\sigma^2 (\\lambda_1 + ... + \\lambda_n) = \\] 6.1.5 Собственные числа Для матрицы \\(I\\) — \\(\\underbrace{1 ... 1}_{\\text{n штук}}\\). Для матрицы \\(H\\) — \\(\\underbrace{1 ... 1}_{\\text{k штук}}\\underbrace{0 ... 0}_{\\text{n-k штук}}\\). Множество собственных чисел устроено так, поскольку \\[ (I-H) v = \\begin{cases} 1 \\cdot v \\text{ для перпендикуляров лапе}\\\\ 0 \\cdot v \\text{ для лежащих в лапе} \\end{cases} \\] \\[ = (n-k)\\sigma^2 = E(RSS) \\] Наконец-то мы посчитали \\(E(RSS)\\). Теперь ясно, что оценка дисперсии случайной ошибки \\(\\varepsilon_i\\) методом максимального правдоподобия — смещенная! \\[ \\mathbb{E} (\\hat{\\sigma}^2_{ML}) = \\dfrac{n-k}{n} \\sigma^2 \\ne \\sigma^2 \\] Зато можно построить несмещенную \\[ \\hat{\\sigma}^2_{\\text{скорр}} = \\dfrac{RSS}{n-k} \\] 6.2 Теорема Если \\(M\\) — проектор — выполняет проецирование (\\(M&#39; = M\\), \\(M^2 = M\\)) и вектор \\(u \\sim \\mathbb{N}(0, I)\\), то \\(u&#39;Mu \\sim \\chi^2_{rk (M)}\\). Для проектора \\(rk(M)= tr(M)\\) — количество линейно независимых собственных векторов (или просто столбцов в матрице \\(M\\)) 6.3 Домашка Найти как распределено \\(\\dfrac{RSS}{\\sigma^2}\\) Посчитать руками \\(H\\), \\(I-H\\), \\(rk(H)\\), \\(rk(I-H)\\), \\(tr(H)\\), \\(tr(I-H)\\), найти закон распределения \\(\\dfrac{\\varepsilon&#39;H\\varepsilon}{\\sigma^2}\\), \\(\\dfrac{\\varepsilon&#39;(I-H)\\varepsilon}{\\sigma^2}\\), \\(\\dfrac{y&#39;(I-H)y}{\\sigma^2}\\) и \\(\\dfrac{RSS}{\\sigma^2}\\) если известно, что \\[ X = \\left( \\begin{matrix} 1 &amp; 0.1 \\\\ 1 &amp; 0.2 \\\\ 1 &amp; 0.3 \\end{matrix} \\right), \\quad y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathbb{N}(0, \\sigma^2 I) \\] "],
["holyday2-preparation.html", "7 Подготовка к празднику №2 7.1 Упражнение 1 7.2 Упражнение 2 7.3 Подготовка к контрольной! 7.4 Упражнение 3 7.5 ДЗ", " 7 Подготовка к празднику №2 Дата: 31.11.2016 Авторы: Герман Никита, Ишмаева Бэлла 7.1 Упражнение 1 Найти как распределена случайная величина: \\[ \\frac{RSS}{\\sigma^2} \\sim {?} \\] Для этого надо вспомнить теорему: Если одновременно выполнено: \\(H\\) — проектор: \\(H^2 = H\\), \\(H^T = H\\) \\(u\\sim N(0; I)\\) То: \\(u^{T}\\cdot H \\cdot u \\sim \\chi^2_{k}\\), где \\(k\\) — это размерность пространства, куда \\(H\\) проецирует. Вспомним, как распределена случайная ошибка в модели регрессии: \\[ \\varepsilon \\sim N(0; \\sigma^2{I}) \\] При этом: \\[ \\frac{RSS}{\\sigma^2} = \\frac{\\hat{\\varepsilon}^{T}\\cdot\\hat{\\varepsilon}}{\\sigma^2} \\] Теперь вспомним, что такое наблюдаемая ошибка: \\[ \\hat{\\varepsilon} = y - \\hat{y} = y - Hy = (I - H)y = (I - H)(X\\beta + \\varepsilon) = X\\beta - HX\\beta + (I - H)\\varepsilon \\] Вспомним, что \\(H=X(X^{T}X)^{-1}X^{T}\\) Также вспомним, что матрица H проецирует любой вектор на линейную оболочку, порожденную X. Тогда проекция X будет тоже X. То есть \\(HX=X\\). Тогда \\(\\hat{\\varepsilon} = (I - H)\\varepsilon\\). Отсюда: \\[ \\frac{{RSS}}{\\sigma^2} = \\frac{ \\varepsilon^{T}(I - H)^{T}\\cdot(I - H)\\varepsilon }{\\sigma^2} \\] Вспомним, что \\((I - H)^{T} = (I - H)\\) и \\((I - H)^2 = (I - H)\\). Тогда: \\[ \\frac{{RSS}}{\\sigma^2} = \\frac{ \\varepsilon^{T}(I - H)\\varepsilon}{\\sigma^2} = u^T(I-H)u, \\] где \\(u = \\frac{\\varepsilon}{\\sigma^2}\\), \\(u \\sim N(0;I)\\). Вспоминая теорему и, используя свойство, что \\({\\mathrm{rank}}(I - H) = tr(I - H) = n - k\\), получаем: \\[ \\frac{RSS}{\\sigma^2} \\sim \\chi^2_{n - k} \\] 7.2 Упражнение 2 Необходимо посчитать: \\[ E\\left(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y}\\right) \\] Если известно, что y задается регрессионной моделью: \\[ \\begin{aligned} y &amp;= {X}\\beta + \\varepsilon \\\\ \\varepsilon &amp; \\sim N(0;\\sigma^2 I) \\end{aligned} \\] S — матрица строевого леса: \\[ S = \\begin{pmatrix} 1 &amp; \\cdots &amp; 1\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Заметим, что \\(({y}^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\) имеет размерностнь 1 x 1. Возьмем след, потому что след матрицы 1 x 1 равняется самой матрице: \\[ tr\\left(E({y}^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = E\\left(tr(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = E\\left(\\frac{1}{n}S\\cdot tr(y^{T}y)\\right) = tr\\left(\\frac{1}{n}{S}\\cdot{E}(y^{T}y)\\right) \\] Вспомним, что: \\[ E(yy^{T}) = Var(y) + E(y)E(y^{T}) \\] Тогда: \\[ tr\\left(E(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = \\frac{1}{n}tr\\left[ S(\\sigma^2 I + X\\beta\\beta^{T}X^{T})\\right] = \\frac{1}{n}{tr}(S\\sigma^2 I) + \\frac{1}{n} tr(SX\\beta\\beta^{T}X^{T}) \\] Заметим, что след матрицы \\(S\\sigma^2 I\\) равен \\(n\\sigma^2\\). Также заметим, что \\(SX\\beta\\beta^{T}X^{T}\\) - скаляр. Отсюда: \\[ tr\\left(E(y^{T}\\cdot\\frac{1}{n}\\cdot S\\cdot{y})\\right) = \\sigma^2 + \\frac{1}{n}SX\\beta\\beta^{T}X^{T} \\] 7.3 Подготовка к контрольной! 7.3.0.1 Сначала вспомним модель парной регрессии Общая формула нахождения оценок \\(\\beta\\) для множественной регрессии: \\[ \\hat{\\beta} = (X^{T}X)^{-1}X^{T}y \\] А для парной: \\[ \\begin{aligned} {y}_{i} &amp;= \\beta_1 + \\beta_2 x_{i} + \\varepsilon_{i}, \\, \\text{где} \\\\ X &amp;= \\begin{pmatrix} 1 &amp; x_1\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; x_n \\end{pmatrix}\\\\ X^{T}X &amp;= \\begin{pmatrix} n &amp; \\sum_{i}x_{i}\\\\ \\sum_{i}x_{i} &amp; \\sum_{i}x_{i^2} \\end{pmatrix}\\\\ X^{T}y &amp;= \\begin{pmatrix} \\sum_{i} y_{i}\\\\ \\sum_{i} x_{i}y_{i}\\\\ \\end{pmatrix} \\end{aligned} \\] В этом случае можно воспользоваться шаманским способом обращения матриц: \\[ A^{-1} = \\begin{pmatrix} a &amp; b\\\\ c &amp; d \\end{pmatrix}^{-1} =\\frac{1}{detA} \\begin{pmatrix} d &amp; -b\\\\ -c &amp; a \\end{pmatrix} \\] 7.3.0.2 А теперь перейдем к задачам про доверительный интервал Когда мы решали подобного рода задачи в прошлом году, мы обычно находили статистику и сравнивали ее с критическим значением нормального распределения, распределения Стьюдента или Фишера, в зависимости от того, как распределена была сама статистика. В нашем случае нам нужно проверять гипотезы и строить доверительные интервалы для \\(\\beta\\), а значит нужно понять, какое распределение имеет статистика: \\[ \\frac{\\hat\\beta - \\beta}{{se}\\left(\\hat\\beta\\right)} \\] Если с числителем еще более или менее понятно: \\(\\hat\\beta\\) имеет нормальное распределение, то вот со знаменателем стоит разобраться, что мы и будем делать дальше. В домашнем задании мы уже выводили, что: \\[ \\begin{cases} Cov\\left(\\hat\\varepsilon,\\hat\\beta\\right) = 0\\\\ Cov\\left(\\hat\\beta\\right) = \\sigma^2 \\left(X^{T}X\\right)^{-1} \\end{cases} \\] А скорректированная \\(ML\\) оценка для сигмы-квадрат : \\[ \\hat\\sigma^2 = \\left(\\frac{RSS}{n-k}\\right) \\] Стоит отметить, что в начале семинара мы уже показали, что, если отмасштабировать, \\(RSS\\) будет иметь распределение \\(\\chi^2_{n - k}\\), которое здесь делится на количество степеней свободы \\(n - k\\,\\)(должно напоминать нам \\(t-\\) или \\(F-\\) распределение) Теперь вернемся к условию \\(Cov(\\hat\\varepsilon,\\hat\\beta) = 0\\): вообще говоря, равенство нулю ковариации не говорит нам о независимости случайных величин, но в нашем случае \\(\\hat\\epsilon\\) и \\(\\hat\\beta\\) имеют совместное нормальное распределение, а значит мы можем сказать, что они независимы. Имеем: \\[ Var\\left(\\hat\\beta\\right) = \\sigma^2 \\left(X^{T}X\\right)^{-1} \\hat{Var}\\left(\\hat\\beta\\right) = \\hat{\\sigma^2} \\left(X^{T}X\\right)^{-1} \\hat{Var}\\left(\\hat\\beta\\right) = \\left(\\frac{RSS}{n-k}\\right) \\left(X^{T}X\\right)^{-1} \\] Матрица \\(\\hat{Var}\\left(\\hat\\beta\\right)\\) имеет вид: \\[ \\begin{aligned} \\begin{pmatrix} \\hat{Var}\\left(\\hat\\beta_1\\right) &amp; \\cdots &amp; \\hat{Cov}\\left(\\hat\\beta_n,\\hat\\beta_1\\right)\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\hat{Cov}\\left(\\hat\\beta_1,\\hat\\beta_n\\right) &amp; \\cdots &amp; \\hat{Var}\\left(\\hat\\beta_{n}\\right) \\end{pmatrix}\\\\ \\end{aligned} \\] А каждый элемент матрицы мы можем посчитать исходя из данных. Вспомним, как выглядит распределение Стьюдента: \\[ \\begin{aligned} \\frac{N\\left({0,1}\\right)}{\\sqrt{\\frac{\\chi^2_{r}}{r}}} \\sim t_{r} \\end{aligned} \\] Тогда мы можем получить следующую теорему: Если: \\(Y = X\\beta +\\varepsilon\\) \\(\\varepsilon \\sim N\\left(0, \\sigma^2 I\\right)\\) То: \\[ \\frac{\\hat\\beta_{j} - \\beta_{j}}{se\\left(\\hat\\beta_{j}\\right)} \\sim {t}_{n-k}, \\] где \\(se = \\sqrt{\\hat{Var}\\left(\\hat\\beta_{j}\\right)}\\) — это стандартная ошибка. 7.4 Упражнение 3 Дано: \\(\\sum_{i} x_{i} = 5\\), \\(\\sum_{i} y_{i} = 2\\), \\(\\sum_{i} x_{i}y_{i} = 20\\), \\(\\sum_{i}x_{i}^2 = 10\\), \\(\\sum_{i} y_{i}^2 = 80\\), \\(n=5\\) \\(y_{i}=\\beta{x}_{i}+\\varepsilon_{i}\\) \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^2\\right)\\) Найти: \\(X^{T}X\\), \\(X^{T}y\\) \\(\\hat\\beta\\), \\(RSS\\), \\(\\hat\\epsilon^2\\), \\(se\\left(\\hat\\beta\\right)\\) Построить 95% доверительный интервал для \\(\\hat\\beta\\) при \\(\\alpha=0.05\\) и проверить гипотезу: \\[ \\begin{aligned} H_{0}:\\, \\beta &amp;= 0\\\\ H_{a}:\\, \\beta &amp;\\neq 0 \\end{aligned} \\] Решение: a) \\(X^{T}X = \\sum_{i}x_{i}^2 = 10\\), \\(X^{T}y = \\sum_{i}x_{i}y_{i} = 20\\) b) \\(\\hat\\beta = (X^{T}X)^{-1}{X}^{{T}}{y} = \\left(10^{-1}20\\right) = 2\\). Для нахождения \\(RSS\\), нужно вспомнить, что: \\(H={X}({X}^{{T}}{X})^{-1}{X}^{{T}}\\) \\[ RSS = y^{T}\\left(I-H\\right)y = y^{T}Iy - y^{T}Hy =y^{T}y - y^{T}X\\left(X^{T}X\\right)^{-1}X^{T}y = \\sum_{i} y_{i}^2 - \\sum_{i}x_{i}y_{i}\\left(\\sum_{i} x_{i}^2\\right)^{-1}\\sum_{i}x_{i}y_{i} \\] \\[ \\begin{aligned} RSS &amp;= 80 - 20\\frac{1}{10}20 = 40\\\\ \\hat\\sigma^2 &amp;= \\frac{RSS}{n-k} = \\frac{40}{4} = 10\\\\ \\hat{Var}\\left(\\hat\\beta\\right) &amp;= \\hat\\sigma^2\\left(X^{T}X\\right)^{-1} = 10\\frac{1}{10} = 1\\\\ se\\left(\\hat\\beta\\right) &amp;= \\sqrt{\\hat{Var}\\left(\\hat\\beta\\right)} = 1 \\end{aligned} \\] c) В общем случае доверительный интервал выглядит так: \\[ \\beta\\in\\left[\\hat\\beta - t_{crit}\\cdot{se}\\left(\\hat\\beta\\right);\\hat\\beta + t_{crit}\\cdot{se}\\left(\\hat\\beta\\right)\\right] \\] Находим в таблице критическое значение для t-распределения с \\(\\left({n-1}\\right) = 4\\) степенями свободы и 5% уровнем значимости: \\({t}_{crit} = 2,77\\). Тогда наш интервал выглядит следующим образом: \\[ \\beta\\in[2 - 2,77;2 + 2,77] \\] Чтобы проверить гипотезу, найдем \\({t}\\) наблюдаемое: \\[ \\frac{\\hat\\beta - \\beta}{se\\left(\\hat\\beta\\right)} = \\frac{2-0}{1} = 2 \\] Получаем, что \\({H}_{{0}}\\) не отвергается. 7.5 ДЗ Демоверсия и разделы 3-4 "],
["many-restrictions.html", "8 Гипотезы о нескольких ограничениях в регрессии 8.1 Примеры: 8.2 Проверка гипотез по шагам: 8.3 Замечание: 8.4 Теорема: 8.5 Упражнение: 8.6 Упражнение: 8.7 Упражнение:", " 8 Гипотезы о нескольких ограничениях в регрессии Датa ??/??/16 Конспект: Арсений Лысенко 8.1 Примеры: \\(H_{0}: \\beta_{2} = 0\\) \\(H_{0}: \\beta_{2} = \\beta_{7}\\) \\[ H_{0}: \\begin{cases} \\beta_{2} = \\beta_{7}\\\\ \\beta_{2} = \\beta_{3}\\\\ \\end{cases} \\] \\[ H_{0}: \\begin{cases} \\beta_{1} = 1\\\\ \\beta_{2} = 1\\\\ \\beta_{3} = 2\\\\ \\end{cases} \\] 8.2 Проверка гипотез по шагам: Строим регрессию, забыв про ограничения; Получим \\(RSS_{UR} - RSS_{\\text{Unrestricted}}\\) Строим регрессию с учетом ограничений. Получим \\(RSS_{R} - RSS_{\\text{Restricted}}\\) 8.3 Замечание: МНК минимизирует RSS, поэтому безусловный RSS(\\(RSS_{\\text{UR}}\\)) будет меньше или равен условного RSS(\\(RSS_{\\text{R}}\\)). 8.4 Теорема: Если выполнены предпоссылки теоремы Гаусса-Маркова, \\(H_{0}\\) верна ии \\(\\epsilon \\sim N(0; \\sigma * I)\\), тогда: F = \\(\\dfrac{(RSS_{R} - RSS{UR})/(\\text{кол-во ограничений})}{RSS_{UR}/(n - k_{UR})}\\) \\(F \\sim F_{\\text{кол-во ограничений}; n - k_{UR}}\\) где \\(k_{UR}\\) - колличество коэффициентов в неограниченной модели. 8.5 Упражнение: Харис пытается понять, что лучше помогает решать задачи по эконометрике: -поедание пирожков(штуки) -посещение лекций(академические часы) \\(problems_{t} = \\beta_{1} + \\beta_{2}lecture_{t} + \\beta_{3}pie_{t} + u_{t}\\) \\(H_{0}: \\beta_{2} = \\beta_{3}\\) Какую регрессию нужно оценить, чтобы найти \\(RSS_{R}\\)? 8.5.1 Решение Согласно \\(H_{0}\\) должно выполняться: \\(\\beta_{2} = \\beta_{3}\\). Тогда: \\(problem_{i} = \\beta_{1} + \\beta_{2}(lecture_{i} + pie_{i}) + u_{i} = \\beta_{1} + \\beta_{2}lp_{i} + u_{i}\\) где \\(lp_{i} = lecture_{i} + pie_{i}\\) А что, если Харис захочет проверить гипотезу о постоянной отдаче от масштабов? Как тогда будут выглядеть \\(H_{0}\\) и ограниченная регрессия? UR: \\(ln problem_{i} = \\gamma_{1} + \\gamma_{2}\\ln lecture_{i} + \\gamma_{3}\\ln pie_{i} + u_{i}\\) \\(problem_{i} = e^{\\gamma_{1}}*lecture_{i}^{\\gamma_{2}}*pie_{i}^{\\gamma_{3}}*e^{u_{i}}\\) \\(H_{0} = \\gamma_{1} + \\gamma_{3} = 1\\) R: \\(ln problem_{i} = \\gamma_{1} + \\gamma_{2}\\ln lecture_{i} + (1 - \\gamma_{2})\\ln pie_{i} + u_{i}\\) После преобразований получим: \\((\\ln problem_{i} - \\ln pie_{i}) = \\gamma_{1} + \\gamma_{2} (\\ln lecture_{i} - \\ln pie_{i}) + u_{i}\\) Введём новые переменные: \\(\\tilde{y_{i}} = \\ln problem_{i} - \\ln pie_{i}\\) \\(\\tilde{x_{i}} = \\ln lecture_{i} - \\ln pie_{i}\\) Получим: \\(\\tilde{y_{i}} = \\gamma_{1} + \\gamma_{2}\\tilde{x_{i}} + u_{i}\\) 8.6 Упражнение: 10 наблюдений \\(UR: RSS = 50 R^2 = 0,3\\) \\(problem_{i} = \\beta_{1} + \\beta_{2}lecture_{i} + \\beta_{3}pie_{i} + u_{i}\\) \\[ H_{0}: \\begin{cases} \\beta_{2} = 0\\\\ \\beta_{3} = 0\\\\ \\end{cases} \\] \\(H_{A}: \\text{ хотя бы одна из } \\beta_{2} \\text{ и } \\beta_{3} \\ne 0\\) а) Как выглядит ограниченная регрессия? Чему равен \\(RSS_{R}\\)? б) Как выглядит \\(F\\)? Проверить \\(H_{0}\\) на 5% уровне значимости. 8.6.1 Решение: а) \\(problem_{i} = \\beta_{1} + \\mu_{i}\\) \\(ESS = 0\\) \\(TSS = RSS\\) \\(R^2 = \\dfrac{ESS}{TSS}\\) \\[ R_{UR}^2 = 0,3 = \\dfrac{TSS_{UR} - 50}{TSS_{UR}} TSS_{UR} = 500/7 = 71 \\] \\(problem_{i} = \\beta_{1} + \\mu_{i}\\) Следовательно, \\(TSS_{R} = TSS_{UR} = RsS_{R} = 71\\) б) \\(F = \\dfrac{(71-50)/2}{50/(10-3)} = \\dfrac{10,5}{7} = 1,4\\) \\(F_{cr} = 4,7\\). Получается, что основная гипотеза не отвергается. 8.7 Упражнение: Пусть Харис решил заново оценить модель после второго модуля чтобы понять, изменилась ли зависимость. В первом модуле было 10 наблюдений. Во втором модуле было 8 наблюдейний. Кроме того, известно: по двум модулям: \\(RSS = 150\\) по первому модулю: \\(RSS = 50\\) по второму модулю: \\(RSS = 70\\) \\(H_{0}\\): зависимость не и зменилась \\(H_{A}\\): зависимость изменилась, но осталась линейной а) Как выглядят ограниченная и неограниченная регрессии? б) \\(RSS_{UR} - ?\\) \\(RSS_{R} - ?\\) в) Проверить гипотезу \\(H_{0}\\). 8.7.1 Решение: I модуль: \\(\\beta_{1}, \\beta_{2}, \\beta_{3}\\) II модуль: \\(\\gamma_{1}, \\gamma_{2}, \\gamma_{3}\\) \\[ H_{0} = \\begin{cases} \\beta_{1} = \\gamma_{1}\\\\ \\beta_{2} = \\gamma_{2}\\\\ \\beta_{3} = \\gamma_{3}\\\\ \\end{cases} \\] Ограниченная модель строится по всем наблюдениям(по 18), то есть \\(RSS_{R} = 150\\). Теперь рассмотрим неограниченную модель. \\[ \\begin{matrix} &amp; X = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; lecture_{1} &amp; pie_{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; lecture_n &amp; pie_{n} \\end{pmatrix} \\] \\[y_{I} = X_{I}\\cdot\\beta + u_{I} \\] \\[y_{II} = X_{II}\\cdot\\beta + u_{I} \\] \\[ \\begin{pmatrix} y_{I} \\\\ y_{II} \\end{pmatrix} \\begin{matrix} = \\begin{pmatrix} X_{I} &amp; ... &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; ... &amp; X_{II} \\end{pmatrix} \\cdot \\begin{pmatrix} \\beta_{1} \\\\ \\beta_{2}\\\\ \\beta_{2}\\\\ \\gamma_{1} \\\\ \\gamma_{2}\\\\ \\gamma_{2} \\end{pmatrix} + \\begin{pmatrix} u_{I} \\\\ u_{II} \\end{pmatrix} \\end{matrix} \\] \\[ y_{i} = \\beta_{1} \\cdot m^1_{i} + \\beta_{2} \\cdot lect_{i} \\cdot m^1_{i} + \\beta_{3} \\cdot pie_{i}\\cdot m^1_{i} + \\gamma_{1} \\cdot m^2_{i} + \\gamma_{2} \\cdot lect_{i} \\cdot m^2_{i} + \\gamma_{3} \\cdot pie_{i}\\cdot m^2_{i} \\] \\[ m^2 = 1 - m^1 \\] \\[=&gt; RSS_{UR} = RSS_{1} + RSS_{} = 50 + 70 = 120\\] Датa 21/11/16 Конспект: Козловский Евгений, Ермакова Мария "],
["frish-and-co.html", "9 Фриш и Ко 9.1 Упражнение 1 9.2 Упражнение 2 9.3 Домашка", " 9 Фриш и Ко Вспомним теорему Фриша-Вау-Ловелла. Допустим, имеется следующая модель: \\[ y_{i}=\\beta_1{x}_{i}+\\beta_2{z}_{i}+u_{i} \\] Заметим сначала, что вышеупомянутую теорему можно применять вне зависимости от того, есть ли в модели константа или нет. Мы хотим оценить коэффициент \\(\\beta_1\\). Стандартный способ — построить регрессию вида: \\[ \\hat{y_{i}}=\\hat{\\beta_1}{x}_{i}+\\hat{\\beta_2}{z}_{i}+u_{i} \\] исходя из которой мы легко с помощью МНК можем вычислить \\(\\hat{\\beta_1}\\). Теорема имени Фриша-Вау-Ловелла гласит, что есть и другой путь, состоящий из нескольких шагов: строим регрессию \\(\\hat{y_i}=\\hat{\\lambda}z_i\\), вычисляем остатки \\(\\tilde{y_i}=y_i-\\hat{y_i}\\) строим регрессию \\(\\hat{x_i}=\\hat{\\delta}z_i\\), вычисляем остатки \\(\\tilde{x_i}=x_i-\\hat{x_i}\\) теперь нам остается оценить модель \\(\\tilde{y_i}=\\alpha\\tilde{x_i}+\\varepsilon_i\\), т.е. нужно построить регрессию остатков \\(\\tilde{y_i}\\) на остатки \\(\\tilde{x_i}\\): \\(\\hat{\\tilde{y_i}}=\\hat{\\alpha}\\tilde{x_i}\\). Полученная оценка \\(\\hat{\\tilde{\\alpha}}\\) будет в точности совпадать с \\(\\hat{\\beta_1}\\) из исходной регрессии! Проведем аналогию \\[ y_i = \\beta x_i + u_i\\] \\[ \\hat{\\beta} = \\frac{\\sum{x_i y_i}}{\\sum x_i^{2}}\\] \\[ y_i = \\beta_1x_i+\\beta_2z_i + u_i \\] \\[ \\hat{\\beta} = \\frac{\\sum{\\tilde{x_i} \\tilde{y_i}}}{\\sum \\tilde{x_i^{2}}}\\] ,где \\(\\tilde{y_i}\\) и \\(\\tilde{x_i}\\) остатки от регрессий \\(y_{i}\\) на \\(z_{i}\\) и \\(x_{i}\\) на \\(z_{i}\\) 9.1 Упражнение 1 В качестве примера рассмотрим весьма популярный массив mtcars, встроенный в R, из которого возьмем следующие характеристики автомобилей: \\(mpg\\) — количество миль на галлон бензина \\(wt\\) — масса автомобиля \\(hp\\) — мощность (в л.с.) Пусть имеются следующие модели с соответствующими остатками: \\[ mpg_i = \\beta_1 + \\beta_2 hp_i + u_i; \\widetilde{mpg}_i=mpg_i-\\widehat{mpg_i} \\] \\[wt_i = \\gamma_1 + \\gamma_2 hp_i + u_i; \\widetilde{wt}_i=wt_i-\\widehat{wt_i}\\] Пусть также дана ковариационная матрица остатков: \\[ \\begin{aligned} \\widehat{Var}\\left( \\begin{matrix} \\widetilde{mpg}_i \\\\ \\widetilde{wt}_i \\end{matrix} \\right) &amp;= \\begin{pmatrix} 14.4 &amp; -2.10\\\\ -2.10 &amp; 0.542 \\end{pmatrix}\\\\ \\end{aligned} \\] Найдем всевозможные коэффициенты в следующих регрессиях: \\[ mpg_i = \\alpha_1+ \\alpha_2 wt_i + \\alpha_3 hp_i + \\upsilon_i\\] \\[ wt_i = \\delta_1 + \\delta_2 mpg_i + \\delta_3 hp_i + \\varepsilon_i \\] Чтобы отыскать оценку \\(\\alpha_2\\), нужно построить регрессию \\(\\widetilde{mpg}_i = \\alpha_2 \\widetilde{wt}_i\\) Можем воспользоваться ковариационной матрицей для нахождения оценки коэффициента \\(\\alpha_2\\): \\[ \\widehat{\\alpha_2} = \\frac{{\\sum{\\widetilde{mpg}_i \\widetilde{wt}_i}/(n-1)}}{{\\sum \\widetilde{wt_i^{2}}/(n-1)}}=\\frac{-2.10}{0.542}=-3.87454 \\] По такой же схеме можем найти \\(\\hat{\\delta_2}\\): \\[ \\widehat{\\delta_2} = \\frac{{\\sum{\\widetilde{mpg}_i \\widetilde{wt}_i}/(n-1)}}{{\\sum \\widetilde{mpg_i^{2}}/(n-1)}}=\\frac{-2.10}{14.4}=-0,14583 \\] 9.2 Упражнение 2 В глубокий тыл противника заброшен майор Пейн. Оказалось, что ему под силу в уме оценивать \\(\\widehat{\\beta}\\) для регрессий типа \\(y_i= \\beta x_i + u_i\\), а также он что-то слышал про теорему Фриша-Вау-Ловелла. Что мы хотим? Узнать, сколько нужно построить регрессий, чтобы найти все коэффициенты для: \\(y_i=\\gamma_1 z_i + \\gamma_2 x_i + u_i\\) б) \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 z_i + \\beta_4 w_i + u_i\\) Для начала разберемся с пунктом а) \\(y_i=\\gamma_1 z_i + \\gamma_2 x_i + u_i\\) Оценим \\(\\gamma_2\\) (найдем \\(y_i\\) на \\(x_i\\)) : Очистим y от z Очистим x от z Строим регрессию очищенного y на очищенный x Оценим \\(\\gamma_1\\) Очистим y от x Очистим z от x Строим регрессию очищенного y на очищенный z Итого необходимо \\(3+3 = 6\\) регрессий. Ну а теперь пункт б: Для оценки \\(\\beta_1\\): (строим тройные регрессии) \\(y_i\\) на \\(x_i,z_i,w_i\\) (k штук) 1 на \\(x_i,z_i,w_i\\) (k штук) \\(\\tilde{y_i}\\) на \\(\\tilde{1}\\) Итого : \\(2k+1\\) регрессий для оценивания одного коэффициента, следовательно для оценивания всех коэффициентов понадобится \\(4(2k+1)\\) однофакторных регрессий, где \\(k\\) — это количество регрессий на одну переменную, необходимых для того, чтобы узнать оценки всех коэффициентов трехфакторной модели. Заметим, что если за \\(m\\) обозначить количество однофакторных регрессий, которые требуется построить, чтобы рассчитать оценки всех коэффиентов трехфакторной модели, то будет выполняться равенство: \\(k=3(2m+1)=3(2\\times6+1)=39\\). Следовательно \\(4(2k+1)=4(2\\times39+1)=316\\). На самом деле Фриш и Ко окружают нас повсюду! Попробуем узреть их теорему в стандартной формуле для \\(\\hat{\\beta_2}\\) в модели \\(y_i=\\beta_1+\\beta_2x_i+u_i\\): \\[ \\hat{\\beta_2} = \\frac{\\sum{(x_i-\\overline{x})(y_i-\\overline{y})}}{\\sum{(x_i-\\overline{x})^2}} \\] Но, как известно, \\(\\overline{x}\\) и \\(\\overline{y}\\) — это \\(\\hat{\\alpha_1}\\) и \\(\\hat{\\alpha_2}\\) в регрессиях \\(x_i=\\alpha_1\\) и \\(y_i=\\alpha_2\\) соответственно, а \\(x_i-\\overline{x}\\) и \\(y_i-\\overline{y}\\) — это остатки. Таким образом, оценку \\(\\hat{\\beta_2}\\) можно также получить в результате регрессии остатков \\(\\tilde{y_i}\\) на остатки \\(\\tilde{x_i}\\) 9.3 Домашка 4.51, 4.49, 4.47, 4.40, 4.45, 4.46, 4.43, 3.61, 3.60, 3.48, 3.49, 3.47 "],
["-12-everything-about-beavers.html", "10 Стохастические бобры и прочие условности {#12_everything_about_beavers} 10.1 Упражнение № 0 10.2 Условные свойства 10.3 Упражнение № 1 10.4 Случайность? Не думаю! Или история о том, как отличить стохастические '{и}ксы 10.5 Теорема (как на все это смотрели Гаусс и Марков) 10.6 А что если гетероскедастичность? 10.7 В чем прелесть гетероскедастичности?", " 10 Стохастические бобры и прочие условности {#12_everything_about_beavers} Дата: 05.12.2016 Авторы: Уманец Екатерина, Купцова Анастасия 10.1 Упражнение № 0 Дано \\(u\\), \\(x\\) - скалярные случайные величины Хотим найти \\({\\mathbb{E}}(u|x)\\) \\({\\mathbb{V}ar}(u|x)\\) Решение \\[ E(u|x=2)=0,5*(-1)+0,5*1=0 \\] \\[ E(u|x=3)=-1*0,25 +1*0,75 \\] \\[ E(u|x) = \\begin{cases} 0, &amp; \\text{если } x=2 \\\\ 0,5, &amp; \\text{если } x=3 \\\\ \\end{cases} = 0.5(x-2)\\] \\(Var(u|x)=E(u^2|x)-E^2(u|x)\\) \\(E(u^2|x)=1\\Rightarrow\\) \\[ Var(u|x) = \\begin{cases} 1/3, x=2 \\\\ 3/4, x=3 \\\\ \\end{cases} =1-0,25(x-2)^2\\] 10.2 Условные свойства \\(E(f(x)|x)=f(x)\\) \\(Var(f(x)|x)=0\\) \\(E(f(x)y|x)=f(x)E(y|x)\\) \\(E(E(y|x))=E(y)\\) \\(Var(y)=Var(E(y|x))+E(Var(y|x))\\) 10.3 Упражнение № 1 Дано: \\(x_1,x_2...\\) \\(x_i\\sim N(10,9)\\) - независимы \\(\\underbrace{ \\overbrace{(x_1,u_1)}^{can \\ be \\ dependent} \\rm (x_2,u_2)}_{independent}...\\) - независимы и одинаково распеделены \\[ X=\\begin{pmatrix} x_1 \\\\ : \\\\ x_n\\end{pmatrix} ,u=\\begin{pmatrix} u_1 \\\\ : \\\\ u_n\\end{pmatrix} \\] Хотим найти \\(plim(\\frac {1}{n}X&#39;X)^{-1}\\) \\(plim(\\frac {1}{n}X&#39;u)\\) \\(plim(X&#39;X)^{-1}X&#39;u\\) Решение ЗБЧ: \\(Y_1,..,Y_n\\) - независимы и одинаково распределены \\(\\bar{Y_n}\\longrightarrow E(Y_1)\\) \\(plim(\\frac{1}{n}\\sum_{1}^{n} x_i^2)^{-1}= (plim(\\frac{1}{n} \\sum_{1}^{n}x_i^2))^{-1} =E(x_1^2)^{-1}=(9+100)^{-1}=\\frac{1}{109}\\) \\(plim(\\frac {1}{n}X&#39;u) = [\\frac{X&#39;u}{n}=\\frac{\\sum(x_iu_i)}{n}]=E(x_i,u_i)=E(E(x_i,u_i|x)) = E(x_i\\underbrace{E(u_i|x_i)}_{=0})=0\\) \\(plim(X&#39;X)^{-1}X&#39;u=plim(\\frac{1}{n}X&#39;X)^{-1}\\frac{1}{n}X&#39;u=plim(\\frac{1}{n}X&#39;X)^{-1}*plim(\\frac{1}{n}X&#39;u)=109^{-1}*0=0\\) 10.4 Случайность? Не думаю! Или история о том, как отличить стохастические '{и}ксы Будем исследовать такой воспрос: как количество выпитого кофе влияет на производительность Бориса. Эксперимент №1 (неслучайные иксы): пригласим 100 рандомных Борисов, попросим Бориса номер один в первый день выпить одну кружку кофе, Бориса номер два во второй - две, и так далее, скажем, сто дней; соответственно, на сотый день сотый Борис будет пить сто кружек кофе; и посмотрим, сколько брутальных задачек по эконометрике каждый Борис сможет решить в каждый из этих дней. Внимание: В данном экмперименте ни один Борис не пострадал!!! Эксперимент №2 (стохастические иксы): поймаем 100 рандомных Борисов на улице и спросим, сколько кружек кофе каждый из них пьет и сколько брутальных задач решает за день. Главное отличие между этими двумя экспериментами заключается в том, что в первом случае мы сами выбирали количество кружек кофе, а во втором эта величина получалась случайно. 10.5 Теорема (как на все это смотрели Гаусс и Марков) Если: \\(y = X\\beta + u\\) - наша регрессионная модель со стохастическими иксами \\((\\chi_{i|\\dots}, {y}_{i})\\) - независимы и одинаково распределены (то есть между парами зависимостей нет, а вот внутри пары - угадайте что:) ), где \\(\\chi_{i|\\dots}\\) - \\(i\\)-ая строка матрицы \\(X\\) \\(E({u}_{i}|\\chi_{i|\\dots}) = 0\\) \\(Var({u}_{i}|\\chi_{i|\\dots}) = \\sigma^2\\) - условие гомоскедастичности \\(P(\\text{столбцы матрицы } X \\text{ линейно независимы}) = 1\\) Тогда: \\(E(\\hat{\\beta}|X) = \\beta\\) и \\(E(\\hat{\\beta}) = \\beta\\) \\(Var(\\hat{\\beta}|X) = {\\sigma^2}(X^{T}X)^{-1}\\) \\(\\hat{\\beta}\\) линейна по \\(y\\) \\(\\hat{\\beta}\\) эффективна среди линейных по \\(y\\) и несмещенных оценок \\(plim(\\hat{\\beta}) = \\beta\\) 10.6 А что если гетероскедастичность? Пусть выполняются все предположения предыдущей теоремы, кроме одного - гомоскедастичности. То есть теперь \\(Var({u}_{i}|\\chi_{i|\\dots}) = f(\\chi_{i|\\dots})\\) - условие гетероскедастичности. Тогда хотим найти: \\(\\Omega = Var(u|X)\\) \\(E(\\hat{\\beta}|X)\\) \\(Var(\\hat{\\beta}|X)\\) \\(plim(\\hat{\\beta})\\) Решение: \\(\\Omega = Var(u|X) = \\begin{pmatrix} f(\\chi_{1|\\dots}) &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ldots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; f(\\chi_{n|\\dots}) \\end{pmatrix}\\) Теперь поймем, откуда это взялось: во-первых, \\(Var({u}_{i}|X) = Var({u}_{i}|\\chi_{i|\\dots}) = f(\\chi_{i|\\dots})\\) - элементы на диагонали матрицы \\(\\Omega\\) во-вторых, \\(Cov({u}_{i}, {u}_{j}|X) = 0\\) - элементы вне диагонали матрицы \\(\\Omega\\) \\(E(\\hat{\\beta}|X) = \\beta\\) Действительно: \\(E(\\hat{\\beta}|X) = E({(X^{T}X)^{-1}}X^{T}y|X) = {(X^{T}X)^{-1}}X^{T}E(y|X)\\) Так как \\(E(y|X) = E(X\\beta + u|X) = X\\beta + E(u|X) = X\\beta + 0\\) (Note: \\(E(u|X) = 0\\) следует из предпосылок теоремы, а именно \\(E({u}_{i}|\\chi_{i|\\dots}) = 0\\)), то \\(E(\\hat{\\beta}|X) = {(X^{T}X)^{-1}}X^{T}E(y|X) = {(X^{T}X)^{-1}}X^{T}X\\beta = 1*\\beta = \\beta\\) \\(Var(\\hat{\\beta}|X) = {(X^{T}X)^{-1}}X^{T}{\\Omega}{X}{(X^{T}X)^{-1}}\\) Докажем, что на самом деле получается именно такая сэндвич-формула: \\(Var(\\hat{\\beta}|X) = Var({(X^{T}X)^{-1}}X^{T}y|X) = {(X^{T}X)^{-1}}X^{T}Var(y|X)({(X^{T}X)^{-1}}X^{T})^{T} = {(X^{T}X)^{-1}}X^{T}Var(y|X){X}{(X^{T}X)^{-1}}\\) Так как \\(Var(y|X) = Var(X\\beta + u|X) = Var(u|X) = \\Omega\\), то \\(Var(\\hat{\\beta}|X) = {(X^{T}X)^{-1}}X^{T}Var(y|X){X}{(X^{T}X)^{-1}} = {(X^{T}X)^{-1}}X^{T}{\\Omega}{X}{(X^{T}X)^{-1}}\\) \\(plim(\\hat{\\beta}) = \\beta\\) Докажем это: \\[\\begin{multline} plim(\\hat{\\beta}) = plim({(X^{T}X)^{-1}}X^{T}y) = plim({(X^{T}X)^{-1}}X^{T}(X\\beta + u)) = plim({(X^{T}X)^{-1}}X^{T}X\\beta) + plim({(X^{T}X)^{-1}}X^{T}u) = \\\\ =\\beta + plim({(X^{T}X/n)^{-1}}X^{T}u/n) = \\beta + plim{(X^{T}X/n)^{-1}} \\cdot plim(X^{T}u/n) = \\beta + const \\cdot 0 = \\beta \\end{multline}\\] Эти пределы мы уже искали в упражнении 1 :) Как Уайт предлагает оценивать дисперсию оценки \\(\\beta\\) : \\(\\hat{Var}(\\hat{\\beta}|X) = {(X^{T}X)^{-1}}X^{T}{\\hat{\\Omega}}{X}{(X^{T}X)^{-1}}\\) где \\({\\hat{\\Omega}} = \\begin{pmatrix} {\\hat{u}_{1}}^{2} &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ldots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; {\\hat{u}_{n}}^{2} \\\\ \\end{pmatrix}\\) 10.7 В чем прелесть гетероскедастичности? А вот в чем - зная о гетероскедастичности, мы знаем распределение: \\(\\frac{\\hat{{\\beta}_{j}} - {\\beta}_{j}}{{SE}_{HC}(\\hat{{\\beta}_{j}})} \\sim N(0,1)\\) - асимптотически, конечно! "],
["hetero-exercises.html", "11 Упражнения на гетероскедастичость 11.1 Упражнение 0 11.2 Упражнение 1 11.3 Стохастические регрессоры 11.4 Теорема Гаусса Маркова для стохастических регрессоров 11.5 Упражнение 2", " 11 Упражнения на гетероскедастичость 11.1 Упражнение 0 Даны две случайные величины: \\(u, x\\) - случайные величины (матрица) \\(E(u|x)=?\\) Посчитаем условные математические ожидания для каждой случайной величины: \\(E(u|x=2)=-1*0.5+1*0.5=0\\) \\(E(u|x=3)=-1*\\frac{0.2}{0.8}+1*\\frac{0.6}{0.8}=0.5\\) Совместим их: \\[ E(u|x) = \\begin{cases} 0 &amp;\\text{если x=2}\\\\ 0.5 &amp;\\text{если x=3} \\end{cases} \\] Также можем представить это условное математическое ожидание в виде функции \\(E(u|x)=0.5(x-2)\\) Значение условной дисперсии \\(Var(u|x)\\): Способ №1 Считаем отдельные дисперсии: \\(Var(u|x=2)\\) и \\(Var(u|x=3)\\). Совмещаем их, по аналогии с математическим ожиданием. Способ №2 \\(Var(u|x)=E(u^2|x)-[E(u|x)]^2\\) \\(E(u^2|x)=1\\) \\(Var(u|x)=1-0.25(x-2)^2\\) Так как x принимает всего два значения (\\(x=2, x=3\\)) условную дисперсию можно представить в виде системы: \\[ Var(u|x) = \\begin{cases} 1 &amp;\\text{если x=2}\\\\ 0.75 &amp;\\text{если x=3}\\end{cases} \\] или в качестве функции: \\(Var(u|x)=0.25x+1.5\\) Свойства условного математического ожидания: \\(E(f(x)|x)=f(x)\\) \\(Var(f(x)|x)=0\\) \\(E(f(x)y|x)=f(x)E(y|x)\\) \\(E[E(y|x)]=E(y)\\) \\(Var(y)=Var(E(y|x))+E(Var(y|x))\\) по теореме Пифагора 11.2 Упражнение 1 Даны случайные величины \\(X=(x_1, x_2, ..., x_n)\\) и \\(u=(u_1, u_2, ..., u_n)\\), где \\(x_i\\)~\\(N(10;9)\\), при этом векторы \\((x_1, u_1); (x_2, u_2) ..., (x_n, u_n)\\) независимы, то есть может существовать зависимость между \\(x_i, u_i\\), однако нет зависимости между парами. *\\(E(u_i|x_i)=0\\) \\(plim(\\frac{1}{n}X&#39;X)^{-1}= ?\\) \\(plim(\\frac{1}{n}X&#39;u)= ?\\) \\(plim(X&#39;X)^{-1}X&#39;u= ?\\) Решение: \\(X&#39;X = x_1^2 + x_2^2 + ... + x_n^2 = \\sum_{i=1}^n x_i^2\\) \\(plim(\\frac{1}{n}\\sum_{i=1}^n x_i^2)^{-1}=[E(x_i^2)]^{-1}=(109)^{-1}\\) (по закону больших чисел) Напоминание: \\(lim(f(a_n))=f(lim(a_n))\\) ЗБЧ: \\(Y_1, Y_2, ..., Y_n ~ i.i.d.\\), следовательно, \\(\\bar(Y_n) -&gt; E(Y_1)\\) \\(E(x_i^2)=Var(x_i)+(E(x_i))^2\\) \\(\\frac{1}{n}X&#39;u = \\frac{1}{n}\\sum_{i=1}^n x_iu_i^2\\) По ЗБЧ \\(plim(frac{1}{n}\\sum_{i=1}^n x_iu_i^2) = E(x_iu_i)\\). Исходя из свойств условного математического ожидания \\(E[E(y|x)]=E(y)\\) представим \\(E(x_iu_i)\\) как \\(E(E(x_iu_i|x_i))\\). Так как \\(x_i\\) известно, то по свойству \\(E(f(x)y|x)=f(x)E(y|x)\\) можем вынести известную случайную величину за знак математического ожидания: \\(E(x_iE(u_i|x_i)) = 0\\) Следовательно, \\(plim(\\frac{1}{n}X&#39;u) = 0\\) \\(X&#39;X\\) и \\(X&#39;u\\) случайные величины домножим \\(X&#39;X\\) и \\(X&#39;u\\) на \\(\\frac{1}{n}\\) \\(plim(\\frac{1}{n}X&#39;X)^{-1}\\frac{1}{n}X&#39;u\\) = \\(plim(\\frac{1}{n}X&#39;X)^{-1}* plim(\\frac{1}{n}X&#39;u)=(109)^{-1}*0=0\\) Следовательно, \\(plim(X&#39;X)^{-1}X&#39;u=0\\) Напоминание: \\(lim(a_nb_n)=lim(a_n)lim(b_n)\\) 11.3 Стохастические регрессоры Если: \\(y = X\\beta+u\\) \\(\\beta\\) - константы В матрице Х: \\(x_{i.}\\) - i-ая строка; векторы \\((x_{i.},y_i)\\) независимы и одинаково распределены. Наблюдения - это случайная выборка \\(E(u_i|x_i)=0\\) Гомоскедастичность \\(E(u_i^2|x_i)=Var(u_i|x_i)=\\sigma^2\\) \\(P(столбцы X линейнонезависимы)=1\\) \\(\\hat{\\beta}=(X&#39;X)^{-1}X&#39;y\\) 11.4 Теорема Гаусса Маркова для стохастических регрессоров Несмещенность: \\(E(\\hat{\\beta}|X)=\\beta\\) \\(Var(\\hat{\\beta}|X)=\\sigma^2(X&#39;X)^{-1}\\) \\(\\hat{\\beta}\\) линейна по y \\(\\hat{\\beta}\\) эффективная и несмещенная оценка среди линейных по y \\(plim(\\hat{\\beta})=\\beta\\) 11.5 Упражнение 2 Нарушена предпосылка теоремы Гаусса Маркова об отсутствие гетероскедастичности: \\(Var(u_i|x_i)=f(x_{i.})\\) Найти: Какой вид имеет матрица \\(\\Omega=Var(u|X)\\) \\(E(\\hat{\\beta}|X)=?\\) \\(Var(\\hat{\\beta}|X)=?\\) \\(plim(\\hat{\\beta})=?\\) Решение: \\(\\Omega=\\left( \\begin{matrix} f(x_{1.}) &amp; cov(u_1,u_2|X)&amp;\\dots &amp; cov(u_1,u_n|X) \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\ cov(u_1,u_n|X) &amp; \\dots &amp;\\dots &amp; f(x_{n.}) \\end{matrix} \\right)\\) Так как выборка случайна, то \\(cov(u_i,u_j|X)=0\\), при \\(i\\neq j\\) \\(\\Omega=\\left( \\begin{matrix} f(x_{1.}) &amp; 0 &amp;\\dots &amp; 0 \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\ 0 &amp; \\dots &amp;\\dots &amp; f(x_{n.}) \\end{matrix} \\right)\\) \\(E(\\hat{\\beta}|X)=\\beta\\) \\(Var(\\hat{\\beta}|X)=Var((X&#39;X)^{-1}X&#39;y|X)=(X&#39;X)^{-1}X&#39;\\times Var(y|X)\\times ((X&#39;X)^{-1}X&#39;)&#39;= (X&#39;X)^{-1}X&#39;\\times Var(y|X)\\times X(X&#39;X)^{-1}=(X&#39;X)^{-1}X&#39;\\times Var(X\\beta+u|X)\\times X(X&#39;X)^{-1}=(X&#39;X)^{-1}X&#39;\\times Var(u|x)\\times X(X&#39;X)^{-1}=(X&#39;X)^{-1}X&#39;\\times \\Omega\\times X(X&#39;X)^{-1}\\) В действительности возникает проблема при оценке матрицы \\(\\Omega\\), так как она имеет размер \\(n\\times n\\), то есть нам нужно оценить n чисел на диагонали по n наблюдениям, что является достаточно нетривиальной задачей. Оказывается, что нельзя получить состоятельную оценку матрицы \\(\\Omega\\), но можно получить такую оценку этой матрицы, чтобы левая часть, то есть \\(Var(\\hat{\\beta}|X)\\), была состоятельна. Например, состоятельной оценкой бцдет следующая: \\(\\hat{Var}_{HC}(\\hat{\\beta}|X)=(X&#39;X)^{-1}X&#39;\\times \\hat{\\Omega}\\times X(X&#39;X)^{-1}\\) где \\(\\hat{\\Omega}\\) можно представить в виде:(White) \\(\\hat{\\Omega}_{HC0}=\\left( \\begin{matrix} \\hat{u_1^2} &amp; 0 &amp;\\dots &amp; 0 \\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\ 0 &amp; \\dots &amp;\\dots &amp; \\hat{u_n^2} \\end{matrix} \\right)\\) абревиатура НС - heteroscedasticity-consistent При гетероскедастичности оценки несмещенные и состоятельные, но проблема будет заключаться в t-статистике, так как мы не знаем ее распределение, следовательно, нам необходимо использовать стандартные отклонения из устойчивой к гетерокедастичности матрицы, чтобы получить надежные оценки: \\(t_{HC}=\\frac{\\hat{\\beta_j}-\\beta_j}{se_{HC}(\\hat{\\beta_j})}\\) ~ \\(N(0;1)\\) где \\(se_{HC}(\\hat{\\beta_j})\\) находятся из матрицы \\(\\hat{Var}_{HC}(\\hat{\\beta}|X)\\) и теперь \\(t_{HC}\\)~\\(N(0;1)\\) (асимтотически нормальное) \\(plim(\\hat{\\beta})=plim(X&#39;X)^{-1}X&#39;y=\\beta+plim(X&#39;X)^{-1}X&#39;u=\\beta\\) "],
["-14-hetero-or-homo.html", "12 Конспект семинара 14. Гетеро или гомо? {hetero_or_homo}", " 12 Конспект семинара 14. Гетеро или гомо? {hetero_or_homo} Для начала стоит определить, что такое Гетероскедастичность!!! Определение, украденное с Википедии: Гетероскедастичность (англ. heteroscedasticity) — понятие, используемое в прикладной статистике (чаще всего — в эконометрике), означающее неоднородность наблюдений, выражающуюся в неодинаковой (непостоянной) дисперсии случайной ошибки регрессионной (эконометрической) модели. Гетероскедастичность противоположна гомоскедастичности, означающей однородность наблюдений, то есть постоянство дисперсии случайных ошибок модели. Другими словами: \\[Var(u_i|X) \\neq {const} = f(x_i)\\] 12.0.1 Какие последствия отсюда вытекают? Хорошие последствия: \\(\\widehat{\\beta}\\) - несмещенно \\(\\widehat{\\beta}\\) - состоятельно Плохие последствия: \\(\\widehat{\\beta}\\) - неэффективно \\[\\left(\\frac{\\widehat{\\beta}_i-\\beta_i}{se(\\widehat{\\beta}_i)}\\right) \\nrightarrow N(0, 1)\\] (!) Для борьбы с гетероскедастичностью нужно знать структуру самой гетероскедастичности! Например, на прошлом семинаре была доказано, что: \\[\\left(\\frac{\\widehat{\\beta}_i-\\beta_i}{se_{hco} (\\widehat{\\beta}_i)}\\right) \\rightarrow N(0, 1)\\] В данном случае индекс hco означает heteroscedasticity consistent, т.е. гетероскедастично устойчиво. Говоря языком программистов и статистов, берутся робастные стандартные ошибки. \\(se_{hco} (\\widehat{\\beta}_i)\\) добывается с диоганали матрицы \\(\\widehat{var}(\\widehat{\\beta}|X)=(X&#39;X)^{-1}\\widehat{\\Omega}X(X&#39;X)\\) \\[ \\widehat{\\Omega}_{hco}=\\begin{pmatrix} \\widehat{u}^2_1 &amp; \\ldots &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\widehat{u}^2_2 &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; \\ldots &amp; \\ldots &amp; \\widehat{u}^2_n \\\\ \\end{pmatrix} \\] 12.0.2 Обнаружение гетероскедастичности Графическое Для наглядного примера возьмём датасет про зависимость стоимости квартир от площади. График с явной гетероскедастичностью На данном графике красными линиями обозначены примерные отклонения. Отчетливо видно, что в зависимости от значения площади наблюдается разный разброс в ценах квартир. Как правило, причиной возникновения гетероскедастичности является размер объекта. Например: размер семьи влияет на диспресию расходов и доходов размер компании влияет на разброс в доходах сотрудников можно придумать ещё много примеров Более утончённое графическое Если \\(Var(u_i|X) = \\sigma^2\\), то \\(var(\\widehat{u}|X) = (I-H)\\sigma^2\\). Можно заметить, что если u - гомоскедастично, тогда дисперсии всё равно будут разными (\\(H=X(X&#39;X)^{-1}X&#39;\\)). То есть \\(Var(\\widehat{u}_i|X)=g_i(X)\\) Что же делать? Можно отнормировать остатки! \\[ \\widehat{u}^{*}_i=\\frac{\\widehat{u}^*_i}{\\sqrt{1-h_{ii}}} \\] В данном случае \\(h_{ii}\\) это диагональные элементы матрицы H \\[\\Rightarrow Var(\\widehat{u}^*_i)=\\sigma^2\\] Следовательно, в случае в случае гомоскедастичности дисперсия будет постоянной величиной. В случае же гетероскедастичности график будет выглядить следующим образом (возьмём тот же пример с квартирами): В даном случае строится график зависимости \\(\\widehat{u}^{*2}_i\\) или \\(|{\\widehat{u}^*_i}|\\) от переменной, преположительно виновной в гетероскедастичности. Тесты Как делаются большинство тестов: Шаг №1 Оценим \\(y_i = \\beta_i+\\beta_2x_i+u_i\\) Отсюда получим \\(\\widehat{u}_i\\) Шаг №1.5 \\[\\widehat{u}^*_i=\\frac{\\widehat{u}_i}{\\sqrt{1-h_{ii}}}\\] Шаг 2: вспомогательная регрессия Строим регрессию «размера остатка», это может быть \\(\\widehat{u}^2_i\\), \\(|\\widehat{u}_i|\\) или \\(\\log\\widehat{u}_i\\) на объясняющие переменные: \\[ \\text{размер остатка}_i = \\gamma_1+\\gamma_2z_i+\\gamma_3w_i + \\ldots+ \\gamma_h r_i + \\nu_i \\] В случае \\(\\hat u^2_i\\) используется тест Бройша — Пагана (про него можно подробно почитать на википедии) Далее проверем гипотезу о гетероскедастичности: \\[ H_0:\\gamma_1=\\gamma_2=...=\\gamma_h=0, \\text{гомоскедастичность} \\] \\(H_a\\): хотя бы один из коэффициентов \\(\\gamma_1\\), \\(\\gamma_2\\), , \\(\\gamma_h\\) отличен от нуля. Далее для проверки можно взять одну из тестовых статистик: F-test \\[ F_{test}=\\frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/n-k_{UR}} \\] где \\(RSS_{R}\\) - это сумма квадратов остатков модели с ограничениями \\(RSS_{UR}\\) - это сумма квадратов остатков модели без ограничений \\(q\\) - это количество ограничений \\(n\\) - это объём выборки \\(k_{UR}\\) - это количество параметров модели без ограничений Тест множителей Лагранжа \\[ LM=nR^2_{step2} \\sim \\chi^2_{n-1} \\] где \\(R\\) - коэффициент детерминации \\(n\\) - количество ограниченй Тест Голдфельда-Квандта (Goldfeld-Quandt) \\[ \\begin{cases} H_0:Var(u_i|x_i)=\\sigma^2\\\\ H_a:Var(u_i|x_i)=f(x_i) \\\\ \\end{cases} \\] Далее наблюдения сортируются по подозреваемому росту (по дисперсии). Из сортированной выборки выкидываются наблюдения с средней дисперсией (как правило, избавляются примерно от 20%). Далее берётся F-статистика. Выходит примерно следующая картина: \\[ F=\\frac{RSS2/(n_2-k)}{RSS_1/(n_1-k)} \\sim F(n_2-k, n_1-k) \\] где \\(RSS_1\\) - сумма квадратов остатков части выборки с низкой дисперсией \\(RSS_2\\) - сумма квадратов остатков части выборки с высокой дисперсией \\(n_1\\) - количество наблюдений с низкой дисперсией \\(n_2\\) - количество наблюдений с высокой дисперсией \\(k\\) - количество ограничений Примечание: если \\(n_1=n_2\\), тогда статистика равна: \\[ F=\\frac{RSS_2}{RSS_1} \\] Если F большое \\(\\Rightarrow H_0\\) отвергается (т.е. наблюдается гетероскедастичность). 12.0.3 Так делать нехорошо: Протестировать наличие гетероскедастичности Если гипотеза о гомоскедаксичности отвергнута, использовать робастные стандартные ошибки(\\(se_{hco} (\\widehat{\\beta}_i)\\)) Если гипотеза о гомоскедастичности отвергнута, использовать обычные стандартные ошибки (\\(se(\\widehat{\\beta}_i)\\)) 12.0.4 Примечание Как делают нормальные люди: RSS - Residiual Sum of sq = \\(\\widehat{u}^2_i\\) ESS - Explained Sum of sq = \\(\\sum (\\widehat{y}^2_i-\\bar{y}_i)^2\\) Как делают гопники: ESS - Error Sum of sq = \\(\\widehat{u}^2_i\\) RSS - Regression Sum of sq = \\(\\sum (\\widehat{y}^2_i-\\bar{y}_i)^2\\) 12.0.5 Пакеты для R: tidyvers lmtest memisc sandwich 12.0.6 Немного R glimpse(diamonds) ## Observations: 53,940 ## Variables: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ... ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very G... ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, ... ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI... ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ... ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54... ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,... ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ... ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ... ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ... Посмотрим на график зависимости цены и карата бриллианта qplot(data = diamonds, x = carat, y = price) Видна гетероскедастичность Поробуем прологарифмировать. qplot(data = diamonds, x = log(carat), y = log(price)) Гетероскедастичность осталась. Построим модель регрессии, где карат - объясняющая переменнаяб цена - зависимая переменная. model &lt;- lm(data = diamonds, log(price) ~ log(carat)) coeftest(model) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4486607 0.0013647 6190.90 &lt; 2.2e-16 *** ## log(carat) 1.6758167 0.0019338 866.59 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 coeftest(model, vcov. = vcovHC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4486607 0.0014750 5728.1 &lt; 2.2e-16 *** ## log(carat) 1.6758167 0.0020278 826.4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 тест Breush-Pagan (White) bptest(model, varformula = ~ log(table) + log(carat), data = diamonds) ## ## studentized Breusch-Pagan test ## ## data: model ## BP = 449.18, df = 2, p-value &lt; 2.2e-16 тест Goldfeld-Quandt Чтобы отловить какая величина виновата в гетероскедастичности gqtest(model, fraction = 0.2, order.by = diamonds$table) ## ## Goldfeld-Quandt test ## ## data: model ## GQ = 1.004, df1 = 21574, df2 = 21574, p-value = 0.3834 gqtest(model, fraction = 0.2, order.by = diamonds$carat) ## ## Goldfeld-Quandt test ## ## data: model ## GQ = 1.3634, df1 = 21574, df2 = 21574, p-value &lt; 2.2e-16 12.0.7 Полезные ссылочки Ссылки на одну свободную энциклопедию: Собственно гетероскедастичность Википедия F-тест Википедия Тест множителей Лагранжа Википедия Тест Бройша — Пагана Википедия Тест Голдфелда — Куандта Википедия Прочие ссылки: Компьютерная гетероскедастичность от занимательного исследователя Гетероскедастичность в R Еще один рример определения гетероскедастичности в R R-bloggers "],
["bayesian-approach.html", "13 Байесовский подход 13.1 Хинты к брутальной части (16 января 2017) 13.2 Отличие классического подхода от байесовского", " 13 Байесовский подход конспект: Мария Салунина, Полина Лапшова дата: 23 января 2017 13.1 Хинты к брутальной части (16 января 2017) 13.1.1 Первая задача Дано: \\(\\hat{y}=3+7x\\) \\(R^2=0.6\\) \\(sVar(y)=1000\\) Задание 1: найти \\(sCov(y, x)\\). Мы знаем, что \\(sVar(y)=\\frac{\\sum\\limits_{i=1}^n(y_i-\\overline{y})^2}{n-1}\\). Замечание: чем отличается \\(Var(y)\\) от \\(sVar(y)\\)? \\(Var(y)\\) - матрица из констант размера \\(n \\times n\\). \\(sVar(y)\\) - вектор размера \\(1 \\times 1\\), случайная величина. Что такое \\(R^2\\)? Первая интерпретация: доля объяснённой дисперсии. \\(R^2=\\frac{sVar(\\hat{y})}{sVar(y)}=0.6\\), следовательно, \\(sVar(\\hat{y})=600\\). \\(\\hat{y}=3+7x\\), т. е. \\(\\hat{y}\\) в семь раз больше \\(x\\), значит: \\(sVar(x)=\\frac{600}{49}\\) Вторая интерпретация: \\(R^2=sCorr^2(y, \\hat{y})\\) \\(sCorr(y, \\hat{y})=\\sqrt{0.6}\\) \\(sCorr(y, x)=+\\sqrt{0.6}\\) Замечание: знак \\(&quot;+&quot;\\), так как в регрессии перед \\(x\\) стоит плюс. \\(sCorr(y, x)=\\frac{sCov(y, x)}{\\sqrt{sVar(y)sVar(x)}}=+\\sqrt{0.6}\\) \\(sVar(y)=1000\\) \\(sVar(x)=\\frac{600}{49}\\) Следовательно, \\(sCov(y, x)=85.7143\\) Итог: индивидуальные регрессии позволяют найти любую выборочную корреляцию, любую выборочную ковариацию, любую выборочную дисперсию. Задание 2: найти регрессию \\(y_i=\\beta_1 +\\beta_2 x_i +\\beta_3 z_i + u_i\\) Как найти вектор \\(\\hat{\\beta}\\)? \\(\\begin{pmatrix} \\hat{\\beta_1}\\\\ \\hat{\\beta_2}\\\\ \\hat{\\beta_3}\\\\ \\end{pmatrix} = (X&#39;X)^{-1}X&#39;y\\) Фриша-Вау: проведём регрессию в два шага. Отрегрессируем \\(x\\), \\(z\\), \\(y\\) на \\(1\\) (чтобы избавиться от единичного столбца) Для \\(x\\): \\(x_i=c+u_i\\) тогда: \\(\\hat{c}=\\overline{x}\\), \\(\\widetilde{x_i}=x_i-\\overline{x}\\) - остаток от регрессии \\(x\\). \\(\\dots\\) Получим остатки от регрессий \\(\\widetilde{x}, \\widetilde{z}, \\widetilde{y}\\). Чтобы оценить \\(\\hat{\\beta_2}\\) и \\(\\hat{\\beta_3}\\), необходимо построить следующую регрессию. \\(\\widetilde{y}\\) на \\(\\widetilde{x}\\), \\(\\widetilde{z}\\) \\(\\hat{\\widetilde{y_i}}=\\hat{\\beta_2} \\widetilde{x_i} + \\hat{\\beta_3} \\widetilde{z_i}\\) Следовательно: \\(\\begin{pmatrix} \\hat{\\beta_2}\\\\ \\hat{\\beta_3}\\\\ \\end{pmatrix} = (\\widetilde{X}&#39;\\widetilde{X})^{-1}\\widetilde{X}&#39;\\widetilde{y}\\) \\(\\widetilde{y} = \\left( \\begin{matrix} y_1 - \\overline{y}\\\\ \\vdots\\\\ y_n - \\overline{y}\\\\ \\end{matrix} \\right)\\) \\(\\widetilde{X} = \\left( \\begin{matrix} x_1 - \\overline{x} &amp; z_1 - \\overline{z}\\\\ \\vdots&amp; \\vdots\\\\ x_n - \\overline{x} &amp; z_n - \\overline{z}\\\\ \\end{matrix} \\right)\\) \\(\\widetilde{X}^{&#39;}\\widetilde{X} = \\left( \\begin{matrix} \\sum\\limits_{i=1}^n(x_i-\\overline{x})^2 &amp; \\sum\\limits_{i=1}^n(x_i-\\overline{x})(z_i-\\overline{z})\\\\ \\sum\\limits_{i=1}^n(x_i-\\overline{x})(z_i-\\overline{z}) &amp; \\sum\\limits_{i=1}^n(z_i-\\overline{z})^2\\\\ \\end{matrix} \\right)\\) \\(\\widetilde{X}^{&#39;}\\widetilde{y} = \\left( \\begin{matrix} \\sum\\limits_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})\\\\ \\sum\\limits_{i=1}^n(z_i-\\overline{z})(y_i-\\overline{y})\\\\ \\end{matrix} \\right)\\) \\(\\left( \\begin{matrix} \\hat{\\beta_2}\\\\ \\hat{\\beta_3}\\\\ \\end{matrix} \\right) = \\left(\\frac{\\widetilde{X}^{&#39;}\\widetilde{X}}{n-1}\\right)^{-1}\\frac{\\widetilde{X}^{&#39;}\\widetilde{y}}{n-1}\\) Средние были даны, любая регрессия проходит через среднюю линию, можно восстановить \\(\\hat{\\beta_1}\\). 13.1.2 Третья задача Дано две модели: \\(\\hat{y_i}=\\hat{\\beta_1}+\\underset{(3.2)}{\\hat{\\beta_2}}x_i\\) \\(\\hat{y_i}=\\hat{\\gamma_1}+\\hat{\\gamma_2} x_i+\\underset{(7.8)}{10}z_i\\) \\(R^2=0.8\\) (для второй модели) Задание: проверить гипотезу \\(H_0: \\beta_2=0\\). Проверим сначала \\(\\gamma_3\\): \\(t = \\frac{10}{7.8}\\) Если знаем \\(R^2\\), то можем проверить значимость регрессии в целом. \\(t^2=F=\\left(\\frac{10}{7.8}\\right)^2\\) Одинаковы ли регрессии. \\(F = \\frac{(R_{UR}^2-R_R^2)/1}{R_{UR}^2/(n-k_{UR})}\\) Пояснение: единичка в знаменателе числителя — одно ограничение на вторую регрессию (\\(\\gamma_3=0\\)). Следовательно, получаем \\(R_R^2\\), можем проверить гипотезу о значимости. Почему \\(t^2=F\\)? \\(y_i=\\beta_1+\\beta_2 x_i+u_i\\) \\(t = \\frac{\\hat{\\beta_2}}{se(\\hat{\\beta_2})} = \\frac{sCov(x,y)}{sVar(x)}\\frac{1}{\\sqrt{\\frac{RSS}{n-2}\\frac{1}{\\sum(x_i-\\overline{x})^2}}}\\) \\(RSS^2 = (1-R^2)*TSS = (1-sCorr^2(x,y))sVar(y)(n-1)\\) \\(t^2=\\frac{sCov^2(x,y)}{sVar(x)}\\frac{(n-2)(n-1)}{(1-sCorr^2(x,y))sVar(y)(n-1)}=\\frac{sCorr^2(x,y)/1}{(1-sCorr^2(x,y))/(n-2)}\\) \\(H_0: \\beta_2=0\\) — одно ограничение. Проверяем одну и ту же гипотезу только с разных сторон. \\(F=\\frac{R^2}{(1-R^2)/(n-2)}\\) — вуаля! то же самое! Ч.Т.Д. 13.2 Отличие классического подхода от байесовского 13.2.1 Классический подход \\(\\beta\\) vs \\(\\hat{\\beta}\\) \\(\\beta\\) — неизвестный параметр \\(const\\) (не знаю и никогда не узнаю). \\(P(\\beta&gt;0)\\) — бессмысленный вопрос (данная вероятность равна либо \\(0\\), либо \\(1\\)). Модель: как данные \\(y_i\\) зависят от \\(\\beta\\). Применяется метод (метод наименьших квадратов, метод моментов, метод максимального правдоподобия и т.д.); на основании модели получаем \\(\\hat{\\beta}=f(y_1, y_2, \\dots, y_n)\\). ЦПТ, ЗБЧ, \\(\\Delta\\)-метод при \\(n \\to \\infty\\) \\(\\hat{\\beta} \\sim \\mathbb{N}(., .)\\). Итог: пункт 3 — точечная оценка \\(\\hat{\\beta}\\); пункт 4 — доверительный интервал \\(\\beta \\in [\\hat{\\beta} - \\dots; \\hat{\\beta} + \\dots]\\). 13.2.2 Байесовский подход Есть только \\(\\beta\\) — всё наше знание о неизвестном параметре сформулировано в виде закона распределения. Априорное мнение о \\(\\beta\\) до получения наблюдения. Существует \\(P(\\beta&gt;0)\\). Модель: как данные \\(y_i\\) зависят от \\(\\beta\\). Используется формула условной вероятности. По формуле \\(f(\\beta\\mid y_1, y_2, \\dots, y_n)\\) получается апостериорное распределение \\(\\beta\\). \\(P(\\beta&gt;0\\mid y_1, y_2, \\dots, y_n)\\) — можно найти. Можно получить точечную и интервальную оценку. 13.2.3 Упражнение №1 (про шапку с монетками) Условие: в шапке есть серебряные и золотые монетки; \\(\\beta\\) — доля золотых монеток. \\(0 \\leqslant \\beta \\leqslant 1\\) Предположим, \\(\\beta\\) распределена равномерно: \\(\\beta \\sim \\mathbb{U}[0, 1]\\) \\(y_i\\) — наблюдения. \\[ y_i = \\begin{cases} 0, &amp; \\text{если монетка серебряная;} \\\\ 1, &amp; \\text{если монетка золотая.} \\end{cases} \\] Достаём, смотрим, какая монетка, отмечаем наблюдение, кидаем монетку обратно в шапку, перемешиваем. $y_i \\mid \\beta$ — независимы |$y_i \\mid \\beta$|0|1| |-----------|---------|---------| |*вероятность*|$1-\\beta$| $\\beta$ | ![*Функция плотности &quot;до&quot;*](images/16_1.png) $y_1=1$ $y_2=1$ $f(\\beta\\mid y_1=1, y_2=1)=\\frac{f(\\beta, y_1=1, y_2=1)}{P(y_1=1,y_2=1)} \\propto f(\\beta, y_1=1, y_2=1) = P(y_1=1, y_2=1 \\mid \\beta)f(\\beta)$ \\[ P(y_1=1, y_2=1 \\mid\\mid \\beta)f(\\beta) = \\begin{cases} \\beta^2, &amp; \\beta \\in [0;1], \\\\ 0, &amp; \\text{иначе.} \\\\ \\end{cases} \\] \\(\\int\\limits_0^1 \\beta^2\\,d\\beta = \\left. \\frac{\\beta^3}{3}\\right|_0^1 = \\frac{1}{3}\\) Следовательно, необходимо домножить на 3. \\[ f(\\beta\\mid y_1=1, y_2=1) = \\begin{cases} 3\\beta^2, &amp; \\beta \\in [0;1], \\\\ 0, &amp; \\text{иначе.} \\end{cases} \\] ![*Функция плотности &quot;после&quot;*](images/16_2.png) До наблюдений: $P(\\beta&gt;\\frac{1}{2})=\\frac{1}{2}$. После наблюдений: $P(\\beta&gt;\\frac{1}{2}\\mid y_1=1, y_2=1)=\\int\\limits_\\frac{1}{2}^1 3\\beta^2\\,d\\beta = \\frac{7}{8}$. Точечная оценка: апостериорное мат. ожидание или медиана. $\\mathbb{E}(\\beta\\mid y_1=1, y_2=1) = \\int\\limits_0^1 \\beta \\cdot f(\\beta\\mid y_1=1, y_2=1)\\,d\\beta$ 13.2.4 Упражнение №2 (про Машу и мышек) Задача: где-то на числовой прямой, в точке \\(m\\), Маша прячет от мышек сыр. При этом известно, что её любимое число — 13. \\(m \\sim \\mathbb{N}(13, 100)\\) Прибежали две мышки и пропищали, где, по их мнению, наиболее сильно пахнет сыром: \\(y_1=9\\), \\(y_2=18\\). \\(y_i\\mid m\\) — независимы; \\(y_i=m+u_i\\); \\(y_i\\mid m \\sim \\mathbb{N}(m, 25)\\). \\(f(m)\\propto e^{-(m-13)^2/(2*100)}\\) — априорная функция плотности. \\(f(m\\mid y_1=9,y_2=18) \\propto f(m,y_1=9,y_2=18)=f(y_1=9,y_2=18\\mid m)f(m) \\propto e^{-(9-m)^2/(2*25)}e^{-(18-m)^2/(2*25)}e^{-(m-13)^2/(2*100)} \\propto e^{-\\frac{1}{2} * \\frac{1}{100} * (9m^2-2m(13+9*4+18*4))} \\propto e^{-\\frac{1}{2} * \\frac{1}{100/9} * (m-\\frac{121}{9})^2}\\) — функция плотности. \\(m \\sim \\mathbb{N}(\\frac{121}{9}, \\frac{100}{9})\\) Магия: \\(\\frac{121}{9}=\\frac{4}{9}*9+\\frac{4}{9}*18+\\frac{4}{9}*13\\) 13.2.4.1 Домашнее задание Установить STAN bayesian. "],
["multinomial.html", "14 Мультиномиальное распределение 14.1 Задание 1 14.2 Задание 2 14.3 Определение симплекса (n-мерного) 14.4 Переходим в STAN", " 14 Мультиномиальное распределение Конспект: Молдованов Александр, Умрихин Александр Дата: 08.02.2017 Можно почитать о мультиномиальном распределении на википедии 14.1 Задание 1 Дано: Вы решили сходить в лес 100 раз. Вероятность встретить там волка составляет 0,3, Бабу-Ягу - 0,1, а Хариса - 0,6. Пусть \\[ \\begin{pmatrix} х_{1} \\\\ х_{2} \\\\ х_{3} \\end{pmatrix} , \\] где \\(х_{1}\\) - количество раз, когда вы встретили волка, \\(х_{2}\\) - Бабу-Ягу, \\(х_{3}\\) - Хариса. Этот вектор имеет мультиномиальное распределение с параметрами (n=100, p(0,3; 0,1; 0,6)), т. е. \\[ \\begin{pmatrix} х_{1} \\\\ х_{2} \\\\ х_{3} \\end{pmatrix} \\sim multi (n=100, p(0,3; 0,1; 0,6)) \\] Вопрос: Какова вероятность встретить волка 10 раз, Бабу-Ягу - 20, а Хариса - 70? p(\\(х_{1}\\)=10, \\(х_{2}\\)=20, \\(х_{3}\\)=70)-? Решение: \\[ \\left. \\begin{array} [c]{ll}0,3^{10} - \\text{вероятность встретить 10 волков}\\\\ 0,1^{20} - \\text{вероятность встретить 20 раз Бабу-Ягу}\\\\ 0,6^{70} - \\text{вероятность встретить 70 раз заблудившегося Хариса} \\end{array} \\right\\} =&gt; \\] вероятность встретить их всех за 100 раз = \\(0,3^{10}\\times0,1^{20}\\times0,6^{70}\\) Важен коэффициент перед этим всем: !!!!!! обосновать !!!!! \\(C^{10}_{100} \\times C^{20}_{90}\\) = \\(\\frac{100!}{10!\\times90!} \\times \\frac{90!}{20!\\times70!} = \\frac{100!}{10!\\times20!\\times70!}\\)\\ Ответ: \\(\\frac{100!}{10!\\times20!\\times70!}\\times0,3^{10}\\times0,1^{20}\\times0,6^{70}\\) 14.2 Задание 2 Дано: \\[ p = \\begin{pmatrix} p_{1} \\\\ p_{2} \\\\ p_{3} \\end{pmatrix} , \\] \\[ x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{pmatrix} \\sim mult(n, (p_{1}, p_{2}, p_{3})) \\] Вопрос: \\(E(X), Var(X)\\) - ? Решение: \\[ E(X)=\\begin{pmatrix} np_{1} \\\\ np_{2} \\\\ np_{3} \\end{pmatrix} \\] \\[\\begin{multline} Cov(x_{1}; x_{2}) = Cov(I_{1,1} + I_{1,2} + ... + I_{1,100}; I_{2,1} + I_{2,2} + ... + I_{2, 100})=\\\\ = \\sum\\ Cov(I_{1,i}; I_{2,i}) = nCov(I_{1,1};I_{2,1})=n(E(I_{1,1}I_{2,1})-E(I_{1,1})E(I_{2,1}))=-np_{1}p_{2} \\Rightarrow\\\\ Var(X)=\\begin{pmatrix} np_{1}(1 - p_{1}) &amp; -np_{1}p_{2} &amp; -np_{1}p_{3} \\\\ -np_{1}p_{2} &amp; np_{2}(1 - p_{2}) &amp; -np_{2}p_{3}\\\\ -np_{1}p_{3} &amp; -np_{2}p_{3} &amp; np_{2}(1 - p_{2}) \\end{pmatrix} = n\\Bigg(\\begin{pmatrix} p_{1} &amp; 0 &amp; 0 \\\\ 0 &amp; p_{2} &amp; 0\\\\ 0 &amp; 0 &amp; p_{3} \\end{pmatrix}-pp^{T}\\Bigg) \\end{multline}\\] 14.3 Определение симплекса (n-мерного) Опять же можно почитать на википедии \\(\\Delta^n = \\{x | x_1 , ..., x_n \\geq 0 , x_1 + ... + x_n =1 \\}\\) Рисунок для n = 3 14.4 Переходим в STAN Для Байевского подхода нужна модель, которая описывает данные: \\[ \\begin{pmatrix} х_{1} \\\\ х_{2} \\\\ х_{3} \\end{pmatrix} \\sim multi (n=100, p(0,3; 0,1; 0,6))\\] Давайте уточним вводные данные: Мы были в гостях у бабушки. Она нам сказала, что волки водятся в лесу также часто, как и Баб-Яжки. Следовательно у вероятность встретить волка равна \\(\\alpha\\), Бабу-Ягу тоже \\(\\alpha\\), а Хариса: \\(1-2\\alpha\\) Зададим априорное распределение на \\(\\alpha\\): \\(\\alpha\\sim U[0,0.5]\\), где \\(U\\) - равномерное распределение Следовательно, наша задача сводится к тому, чтобы получить апостериорное распределение \\(\\alpha\\): f(\\(\\alpha\\)(x)) library(rstan) library(bayesplot) data &lt;- list(y = c(15, 5, 80)) model &lt;- stan_model(file = &#39;multinomial.stan&#39;) fit &lt;- sampling(model, data = data, seed = 42) fit fit_array &lt;- as.array(fit) mcmc_hist(fit_array) mcmc_trace(fit_array, pars = &#39;a&#39;) mcmc_violin(fit_array, pars = &#39;a&#39;) mcmc_hist(fit_array, pars = &#39;y_new[3]&#39;) fit_array[ , 2, 1] "],
["handmade-bayes.html", "15 Ручной Байес 15.1 Упражнение №1 15.2 Упражнение №2 15.3 Дополнительные материалы", " 15 Ручной Байес Конспект: Соколов Харис Дата семинара: 20.02.2017 15.1 Упражнение №1 У нас есть модель для данных: \\[ y_1, \\ldots, y_n \\ \\Big| \\ \\mu, \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\mu, \\sigma^2) \\] И априорное мнение: \\[\\begin{align} &amp; \\sigma^2 \\ \\sim \\ InvGamma(\\underline{s}, \\underline{r}) \\\\ &amp; \\mu \\ \\Big| \\ \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\underline{m}, \\underline{a}\\cdot h(\\sigma^2)) \\end{align}\\] Необходимо подобрать такую функцию \\(h(\\sigma^2)\\), чтобы апостериорные рапределения имели такой же вид: \\[\\begin{align} &amp; \\sigma^2 \\ \\Big| \\ \\vec{y} \\ \\sim \\ InvGamma(\\overline{s}, \\overline{r}) \\\\ &amp; \\mu \\ \\Big| \\ \\sigma^2, \\vec{y} \\ \\sim \\ \\mathcal{N}(\\overline{m}, \\overline{a} \\cdot h(\\sigma^2)) \\end{align}\\] Итого: у нас есть модель для данных и есть априорные распределения, применяя формулу условной вероятности (или используя алгоритм Монте-Карло на компьютере), мы получаем апостериорные распределения. И в данном упражнении мы хотим, чтобы апостериорные распределения были из того же класса распределений, что и априорные: априорно у нас были обратное гамма-распределение и нормальное распределение, значит, и апостериорно должны остаться обратное гамма-распределение и нормальное распределение. Необходимо понимать, где здесь случайные величины, а где константы. Константы: \\(\\underline{s}, \\underline{r}, \\underline{a}, \\underline{m}\\) (так как это наше мнение). Случайные величины: \\(\\mu, \\sigma^2\\) (так как мы в Байесовском подходе), \\(y_1, \\ldots, y_n\\), (так как все наблюдения случайны), \\(\\overline{s}, \\overline{r}, \\overline{a}, \\overline{m}\\) (так как параметры апостериорных распределений зависят от наблюдений, которые являются случайными величинами). Начинаем решать! Нам нужна функция апостериорной плотности: \\[\\begin{align} f(\\mu, \\sigma^2 \\ \\Big| \\ y_1, \\ldots, y_n) &amp; = \\frac{f(\\mu, \\sigma^2, \\ y_1, \\ldots, y_n)}{f(y_1, \\ldots, y_n)} \\propto \\underbrace{f(y_1, \\ldots, y_n \\ \\Big| \\ \\mu, \\sigma^2)}_{\\text{функция правдоподобия}} \\cdot \\underbrace{f(\\mu, \\sigma^2)}_{\\substack{\\text{априорная} \\\\ \\text{плотность}}} = \\\\ &amp; = f(y_1, \\ldots, y_n \\ \\Big| \\ \\mu, \\sigma^2) \\cdot f(\\mu \\ \\Big| \\ \\sigma^2) \\cdot f(\\sigma^2) \\end{align}\\] \\[ \\Rightarrow \\ \\ f(\\mu, \\sigma^2 \\ \\Big| \\ y_1, \\ldots, y_n) \\propto \\prod\\limits_{i=1}^{n} \\frac{1}{\\sqrt{\\sigma^2}}\\cdot e^{-\\frac{1}{2}\\frac{(y_i-\\mu)^2}{\\sigma^2}} \\times \\frac{1}{\\sqrt{\\underline{a}\\cdot h(\\sigma^2)}}\\cdot e^{-\\frac{1}{2}\\frac{(\\mu - \\underline{m})^2}{\\underline{a}\\cdot h(\\sigma^2)}} \\times \\left(\\sigma^2\\right)^{-\\underline{s}-1}\\cdot e^{-\\frac{\\underline{r}}{\\sigma^2}} \\equiv A \\] Хотим получить: \\[ \\frac{1}{\\sqrt{\\overline{a}\\cdot h(\\sigma^2)}}\\cdot e^{-\\frac{1}{2}\\frac{(\\mu - \\overline{m})^2}{\\overline{a}\\cdot h(\\sigma^2)}} \\times \\left(\\sigma^2\\right)^{-\\overline{s}-1}\\cdot e^{-\\frac{\\overline{r}}{\\sigma^2}} \\equiv B \\] Необходимо подобрать такие параметры \\(h(\\sigma^2), \\overline{s}, \\overline{r}, \\overline{a}, \\overline{m}\\), чтобы \\(A\\) было равно \\(B\\). Сначала посмотрим на то, что находится снаружи экспоненты. \\[ \\underbrace{\\left(\\frac{1}{\\sigma^2}\\right)^{\\frac{n}{2}} \\cdot \\left(\\frac{1}{h(\\sigma^2)}\\right)^{\\frac{1}{2}} \\cdot \\left(\\sigma^2\\right)^{-\\underline{s}-1}}_{\\text{Есть}} = \\underbrace{\\left(\\frac{1}{h(\\sigma^2)}\\right)^{\\frac{1}{2}} \\cdot \\left(\\sigma^2\\right)^{-\\overline{s}-1}}_{\\text{Хотим}} \\qquad \\text{(про $\\overline{a}$ и $\\underline{a}$ пока можно забыть}) \\] Отсюда видно, что \\(h(\\sigma^2)\\) может быть любой, так дробь как сокращается, а \\(\\ \\overline{s}=\\underline{s}+\\frac{n}{2}\\), \\(\\overline{a}\\) получим позже. Теперь посмотрим на то, что находится внутри экспоненты. \\[ \\underbrace{-\\frac{1}{2}\\cdot \\frac{\\sum_{i=1}^{n}(y_i-\\mu)^2}{\\sigma^2} - \\frac{1}{2}\\cdot \\frac{(\\mu-\\underline{m})^2}{\\underline{a}h(\\sigma^2)} - \\frac{\\underline{r}}{\\sigma^2}}_{\\text{Есть}} = \\underbrace{- \\frac{1}{2}\\cdot \\frac{(\\mu-\\overline{m})^2}{\\overline{a}h(\\sigma^2)} - \\frac{\\overline{r}}{\\sigma^2}}_{\\text{Хотим}} \\] Для упрощения жизни можно взять \\(h(\\sigma^2) = \\sigma^2\\). Тогда получим: \\[ \\dfrac{-\\frac{1}{2}\\cdot \\sum_{i=1}^{n}(y_i - \\mu)^2 - \\frac{1}{2\\underline{a}}(\\mu - \\underline{m})^2-\\underline{r}}{\\sigma^2}=\\dfrac{- \\frac{1}{2\\overline{a}}(\\mu - \\overline{m})^2-\\overline{r}}{\\sigma^2} \\] Нам нужно, чтобы обе эти функции (слева и справа от знака равно) совпадали как функции от \\(\\sigma^2\\) и \\(\\mu\\). Как функции от \\(\\sigma^2\\) они уже совпадают, так как \\(\\sigma^2\\) в знаменателе и слева, и справа. Значит, нам осталось сделать так, чтобы они совпадали как функции от \\(\\mu\\). Заметим, что в числителе обеих дробей стоят квадратичные по \\(\\mu\\) функции. Следовательно, чтобы они были одинаковыми, нам нужно просто приравнять коэффициенты перед \\(\\mu^2, \\ \\mu\\) и свободным членом. Сделаем это! \\[\\begin{align} &amp; \\text{Коэффициенты перед $\\mu^2$: } \\ \\frac{n}{2} - \\frac{1}{2\\underline{a}} = -\\frac{1}{2\\overline{a}} \\\\ &amp; \\text{Коэффициенты перед $\\mu$: } \\ -\\frac{1}{2}\\cdot 2\\cdot \\sum_{i=1}^{n}y_i \\cdot \\frac{1}{2\\underline{a}}+2\\underline{m} = 2\\overline{m}\\cdot \\left(-\\frac{1}{2\\overline{a}}\\right) \\\\ &amp; \\text{Коэффициенты перед свободным членом: } \\ -\\frac{1}{2}\\sum_{i=1}^{n}y_i^2 - \\frac{1}{2\\underline{a}}\\underline{m}^2-\\underline{r} = -\\overline{r}- \\frac{1}{2\\overline{a}}\\cdot \\overline{m}^2 \\end{align}\\] Чуть-чуть упростим: \\[ \\begin{cases} \\frac{1}{\\overline{a}} = \\frac{1}{\\underline{a}}+n \\\\ -\\frac{\\overline{m}}{\\overline{a}} = -\\sum_{i=1}^{n}y_i - \\frac{\\underline{m}}{\\underline{a}} \\\\ -\\frac{1}{2}\\sum_{i=1}^{n}y_i^2 - \\frac{1}{2\\underline{a}}\\underline{m}^2-\\underline{r} = -\\overline{r}- \\frac{1}{2\\overline{a}}\\cdot \\overline{m}^2 \\end{cases} \\] У нас есть система из трех уравнений с тремя неизвестными, из которой легко получить \\(\\overline{a}, \\ \\overline{m}\\) и \\(\\overline{r}\\). Итог Если: Модель имеет вид: \\[ y_1, \\ldots, y_n \\ \\Big| \\ \\mu, \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\mu, \\sigma^2) \\] Априорно мы считаем: \\[\\begin{align} &amp; \\sigma^2 \\ \\sim \\ InvGamma(\\underline{s}, \\underline{r}) \\\\ &amp; \\mu \\ \\Big| \\ \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\underline{m}, \\underline{a}\\cdot \\sigma) \\end{align}\\] То: Апостериорные распределния имеют следующий вид: \\[\\begin{align} &amp; \\sigma^2 \\ \\Big| \\ \\vec{y} \\ \\sim \\ InvGamma(\\overline{s}=\\underline{s}+\\frac{n}{2}, \\overline{r}) \\\\ &amp; \\mu \\ \\Big| \\ \\sigma^2, \\vec{y} \\ \\sim \\ \\mathcal{N}(\\overline{m}, \\overline{a} \\cdot \\sigma^2) \\end{align}\\] где \\(\\overline{a}, \\ \\overline{m}\\) и \\(\\overline{r}\\) находятся из следующей системы уравнений: \\[ \\begin{cases} \\frac{1}{\\overline{a}} = \\frac{1}{\\underline{a}}+n \\\\ -\\frac{\\overline{m}}{\\overline{a}} = -\\sum_{i=1}^{n}y_i - \\frac{\\underline{m}}{\\underline{a}} \\\\ -\\frac{1}{2}\\sum_{i=1}^{n}y_i^2 - \\frac{1}{2\\underline{a}}\\underline{m}^2-\\underline{r} = -\\overline{r}- \\frac{1}{2\\overline{a}}\\cdot \\overline{m}^2 \\end{cases} \\] 15.2 Упражнение №2 У нас есть модель: \\[ y_i = \\beta \\cdot x_i +u_i \\qquad \\qquad u_i \\ \\sim \\ \\mathcal{N}(0, \\sigma^2) \\] Каким должно быть априорное мнение о \\(\\beta\\) и \\(\\sigma^2\\), чтобы выполнялось \\(\\hat{\\beta}_{\\text{MAP}} = \\hat{\\beta}_{Ridge(\\lambda)}\\), где MAP — это maximum of posterior density. Напомним, что \\(\\hat{\\beta}_{Ridge(\\lambda)}\\) ищется из следующей задачи оптимизации: \\[ \\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i})^2 + \\lambda\\cdot \\beta^2 \\longrightarrow \\min\\limits_{\\beta} \\] А смысл всего этого в том, чтобы показать, что Байесовский подход содержит в себе Ridge-регрессию как частный случай. Теперь сформулируем априорное мнение (если мы считаем иксы детерминированными): \\[\\begin{align} &amp; \\beta \\ \\Big| \\ \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\underline{\\beta}, \\underline{a}\\cdot \\sigma^2) \\\\ &amp; \\sigma^2 \\ \\sim \\ InvGamma(\\underline{s}, \\underline{r}) \\\\ &amp; y_i \\ \\Big| \\ \\beta, \\ \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\beta \\cdot x_i, \\sigma^2) \\end{align}\\] Если же мы считаем иксы стохастическими, то априорное мнение будет выглядеть так: \\[\\begin{align} &amp; \\beta \\ \\Big| \\ \\sigma^2, \\ X \\ \\sim \\ \\mathcal{N}(\\underline{\\beta}, \\underline{a}\\cdot \\sigma^2) \\\\ &amp; \\sigma^2 \\ \\Big| \\ X \\ \\sim \\ InvGamma(\\underline{s}, \\underline{r}) \\\\ &amp; y_i \\ \\Big| \\ X, \\ \\beta, \\ \\sigma^2 \\ \\sim \\ \\mathcal{N}(\\beta \\cdot x_i, \\sigma^2) \\end{align}\\] На следующем шаге мы найдем апостериорное мнение, далее мы получим \\(\\hat{\\beta}_{\\text{MAP}}\\), которое максимизирует апостериорное мнение, и сделаем его таким, чтобы выполнялось \\(\\hat{\\beta}_{\\text{MAP}} = \\hat{\\beta}_{Ridge(\\lambda)}\\). Напомним, что в матричном виде \\(\\hat{\\beta}_{Ridge(\\lambda)} = \\left((X&#39;X)+\\lambda \\cdot I\\right)^{-1}X&#39;y\\), а в скалярном \\(\\hat{\\beta}_{Ridge(\\lambda)} = \\frac{\\sum_{i=1}^{n}x_i \\cdot y_i}{\\sum_{i=1}^{n}x_i^2 + \\lambda}\\). В качестве точечной Байесовской оценки берем моду апостериорного распределения (то есть максимум функции апостериорной плотности). В результате получим \\(\\hat{\\beta}_{\\text{MAP}} = g(y_i, x_i, \\underline{\\beta}, \\underline{s}, \\underline{a}, \\underline{r})\\), где \\(g\\) — функция. Подсказки: \\(\\underline{\\beta}=0, \\ \\underline{a}=\\frac{1}{\\lambda}\\), а от \\(\\underline{s}\\) и \\(\\underline{r}\\) ответ не зависит. (но это неточно (с)) 15.3 Дополнительные материалы Нано-задачник по байесовскому подходу Особо интересующиеся Байесовским подходом могут почитать следующие две книжки: Gelman A. et al. Bayesian data analysis. – Boca Raton, FL, USA : Chapman &amp; Hall/CRC, 2014. – Т. 2. McElreath R. Statistical rethinking: A Bayesian course with examples in R and Stan. – CRC Press, 2016. – Т. 122. А скачать их можно в ознакомительных целях :) "]
]
